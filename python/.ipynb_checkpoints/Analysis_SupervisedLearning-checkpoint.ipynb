{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-cd983f85b09e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# run json-to-pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Part-of-speech Tagging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfT' is not defined"
     ]
    }
   ],
   "source": [
    "# https://textblob.readthedocs.io/en/dev/\n",
    "\n",
    "# How to install TextBlob\n",
    "#     1. pip install -U textblob\n",
    "#     2. python -m textblob.download_corpora\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(dfT['text'][0]) # run json-to-pandas\n",
    "\n",
    "# Part-of-speech Tagging\n",
    "print blob.tags\n",
    "print\n",
    "\n",
    "# Noun Phrase Extraction¶\n",
    "print blob.noun_phrases\n",
    "print \n",
    "\n",
    "# Tokenization\n",
    "print blob.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0166666666667\n",
      "\n",
      "One---One\n",
      "of---of\n",
      "China---China\n",
      "'s---'s\n",
      "first---first\n",
      "female---female\n",
      "fighter---fighter\n",
      "pilots---pilot\n",
      "was---be\n",
      "killed---kill\n",
      "in---in\n",
      "a---a\n",
      "training---train\n",
      "accident---accident\n",
      "according---accord\n",
      "to---to\n",
      "state-run---state-run\n",
      "media---media\n",
      "reports…---reports…\n",
      "https---https\n",
      "t.co/DoEZLme8Cq---t.co/DoEZLme8Cq\n"
     ]
    }
   ],
   "source": [
    "# The subjectivity is a float within the range [0.0, 1.0] \n",
    "# where 0.0 is very objective and 1.0 is very subjective\n",
    "for sentence in blob.sentences:\n",
    "    print sentence.sentiment.polarity\n",
    "print\n",
    "\n",
    "# Lemmatize each word\n",
    "for sentence in blob.sentences:    \n",
    "    for word in sentence.words:\n",
    "        print \"%s---%s\" % (word, word.lemmatize('v')) # 'v' for 'verb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.nltk.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b645383ea184>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtreebank\u001b[0m \u001b[0;31m# to draw a parse tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# run json-to-pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfT' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank # to draw a parse tree\n",
    "\n",
    "sentence = dfT['text'][0] # run json-to-pandas\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "# Identify named entities - Make parse tree?\n",
    "# You might need to call nltk.download() and down load some packages\n",
    "entities = nltk.chunk.ne_chunk(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.728\n",
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.7 : 1.0\n",
      "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
      "              astounding = True              pos : neg    =     10.3 : 1.0\n",
      "             fascination = True              pos : neg    =     10.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Example from http://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    " \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    " \n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    " \n",
    "negcutoff = len(negfeats)*3/4\n",
    "poscutoff = len(posfeats)*3/4\n",
    " \n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 665 instances, test on 56 instances\n",
      "accuracy: 0.767857142857\n",
      "Most Informative Features\n",
      "                Carolina = True              pos : neg    =     10.3 : 1.0\n",
      "                 project = True              pos : neg    =      9.1 : 1.0\n",
      "                    love = True              pos : neg    =      8.7 : 1.0\n",
      "                  attack = True              neg : pos    =      8.5 : 1.0\n",
      "                 history = True              pos : neg    =      7.2 : 1.0\n",
      "               community = True              pos : neg    =      7.2 : 1.0\n",
      "                   Kaine = True              pos : neg    =      7.2 : 1.0\n",
      "                     NBC = True              pos : neg    =      7.2 : 1.0\n",
      "         President-elect = True              pos : neg    =      7.2 : 1.0\n",
      "                     Tim = True              pos : neg    =      7.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Example from http://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    " \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "negfeats = [(word_feats(text.split()), 'neg') for text in sample.loc[sample['labels'] == -1]['texts']]\n",
    "#neufeats = [(word_feats(text.split()), 'neu') for text in sample.loc[sample['labels'] == 0]['texts']]\n",
    "posfeats = [(word_feats(text.split()), 'pos') for text in sample.loc[sample['labels'] == 1]['texts']]\n",
    "\n",
    "negcutoff = int(math.ceil(len(negfeats)*0.92)) # 92%\n",
    "#neucutoff = int(math.ceil(len(neufeats)*0.92)) # 92%\n",
    "poscutoff = int(math.ceil(len(posfeats)*0.92)) # 92%\n",
    " \n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]# + neufeats[:neucutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]# + neufeats[neucutoff:]\n",
    "print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 5)\n",
      "(1402, 4)\n",
      "train on 665 instances, test on 56 instances\n",
      "accuracy: 0.767857142857\n",
      "Most Informative Features\n",
      "                Carolina = True              pos : neg    =     11.9 : 1.0\n",
      "                 project = True              pos : neg    =      9.1 : 1.0\n",
      "                     NBC = True              pos : neg    =      8.7 : 1.0\n",
      "                    love = True              pos : neg    =      8.7 : 1.0\n",
      "                  attack = True              neg : pos    =      8.5 : 1.0\n",
      "                 history = True              pos : neg    =      7.2 : 1.0\n",
      "               community = True              pos : neg    =      7.2 : 1.0\n",
      "                    beat = True              pos : neg    =      7.2 : 1.0\n",
      "                   Kaine = True              pos : neg    =      7.2 : 1.0\n",
      "                       7 = True              pos : neg    =      7.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# FROM FINAL NOTEBOOK\n",
    "\n",
    "import os            # Get current working directory to locate Turk files\n",
    "import pandas as pd  # Pandas to store and manipulate data\n",
    "from datetime import datetime # to import date info for posts\n",
    "import time\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "import math\n",
    "\n",
    "# Import the labeled data\n",
    "sample = pd.read_csv(os.getcwd() + \"/../Turk/6source_results_filtered.csv\")\n",
    "\n",
    "# Each post has three answers. Combine them into one (pos/neu/neg)\n",
    "# If a post has two or more same answers, then we will follow the answer\n",
    "# If a post has all different answer, then we will throw it away\n",
    "\n",
    "new_index = range(0, len(sample.index)/3)\n",
    "new_sample = pd.DataFrame(index=new_index, columns=['texts', 'sources', 'labels', 'statusID'])\n",
    "\n",
    "text = \"\"     # Keeping the text of the post we are working on\n",
    "source = \"\"   # Keeping the source of the post we are working on\n",
    "statusID = \"\" # Keeping the status ID of the post we are working on\n",
    "\n",
    "num_pos = 0   # number of 'positive' answers of the post we are working on\n",
    "num_neu = 0   # number of 'neutral' answers of the post we are working on\n",
    "num_neg = 0   # number of 'negative' answers of the post we are working on\n",
    "pd_index = 0  # Index for the post we are working on to be placed in new_sample\n",
    "\n",
    "for index, row in sample.iterrows():\n",
    "    # If this new post is different post, then append what we worked so far\n",
    "    if text != row['Input.content']:\n",
    "        if text != \"\":\n",
    "            if num_pos >= 2:\n",
    "                new_sample.loc[[pd_index], ['labels']] = 1\n",
    "            elif num_neu >= 2: # could be commented out for pos/neg\n",
    "                new_sample.loc[[pd_index], ['labels']] = 0\n",
    "            elif num_neg >= 2:\n",
    "                new_sample.loc[[pd_index], ['labels']] = -1\n",
    "            else: # 1:1:1\n",
    "                pd_index -= 1 # to keep the same index\n",
    "\n",
    "            new_sample.loc[[pd_index], ['sources']] = source\n",
    "            new_sample.loc[[pd_index], ['texts']] = text\n",
    "            new_sample.loc[[pd_index], ['statusID']] = statusID\n",
    "            pd_index += 1\n",
    "            \n",
    "        # Assign a new post\n",
    "        text = row['Input.content']\n",
    "        source = row['Input.source']\n",
    "        statusID = row['Input.statis_id']\n",
    "        num_pos = 0\n",
    "        num_neu = 0\n",
    "        num_neg = 0\n",
    "        \n",
    "    if row['Answer.sentiment'] == \"Positive\":\n",
    "        num_pos += 1\n",
    "    elif row['Answer.sentiment'] == \"Neutral\":\n",
    "        num_neu += 1\n",
    "    elif row['Answer.sentiment'] == \"Negative\":\n",
    "        num_neg += 1\n",
    "\n",
    "# may not be perfect, so delete not assigned rows\n",
    "new_sample = new_sample.dropna()\n",
    "        \n",
    "new_sample['labels'] = new_sample['labels'].astype('int')\n",
    "    \n",
    "print sample.shape\n",
    "print new_sample.shape # This should have 1/3 number of rows than \"sample\"\n",
    "\n",
    "sample = new_sample\n",
    "\n",
    "# Lemmanize words (e.g. is, are, am ->> be)\n",
    "# Might be better to skip it\n",
    "\n",
    "new_index = range(0, len(sample.index))\n",
    "new_sample = pd.DataFrame(index=new_index, columns=(\"texts\", \"sources\", \"labels\"))\n",
    "\n",
    "for index, row in sample.iterrows():\n",
    "    text = re.sub(r'\\W+' , ' ', row['texts']) # Getting rid of punctuation\n",
    "    blob = TextBlob(text)\n",
    "    newtexts = \"\"\n",
    "\n",
    "    for sentence in blob.sentences:\n",
    "        newtext = \"\"\n",
    "\n",
    "        for word in sentence.words:\n",
    "            newtext += \" \" + word.lemmatize('v') # 'v' for 'verb'\n",
    "\n",
    "        newtexts += newtext\n",
    "    new_sample['texts'].loc[index] = newtexts\n",
    "    \n",
    "sample['texts'] = new_sample['texts']\n",
    "\n",
    "# Getting rid of too common words (e.g. the, an)\n",
    "\n",
    "new_index = range(0, len(sample.index))\n",
    "new_sample = pd.DataFrame(index=new_index, columns=(\"texts\", \"sources\", \"labels\"))\n",
    "\n",
    "for index, row in sample.iterrows():\n",
    "    # If this new post is different post, then append what we worked so far\n",
    "    words = [w for w in row['texts'].split() if not w in stopwords.words(\"english\")]\n",
    "    new_sample['texts'].loc[index] = ' '.join(words)\n",
    "\n",
    "sample['texts'] = new_sample['texts']\n",
    "\n",
    "# Using Naive Bayes Classifier in NLTK \n",
    "\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "# We got a better score when we only considered pos/neg without neutral,\n",
    "# so we commented them off.\n",
    "\n",
    "# This one grabs texts from the labeled dataframe \"sample\"\n",
    "#     for an argument when labels match to either 1 (posfeats) or -1 (negfeats).\n",
    "# Then the function returns the array of dictionary [word:True] for each \n",
    "negfeats = [(word_feats(text.split()), 'neg') for text in sample.loc[sample['labels'] == -1]['texts']]\n",
    "posfeats = [(word_feats(text.split()), 'pos') for text in sample.loc[sample['labels'] == 1]['texts']]\n",
    "#neufeats = [(word_feats(text.split()), 'neu') for text in sample.loc[sample['labels'] == 0]['texts']]\n",
    "\n",
    "# Cutting off at 92% line. Since we have small labeled set,\n",
    "# we decided that having bigger training set would be better\n",
    "negcutoff = int(math.ceil(len(negfeats)*0.92)) # 92%\n",
    "poscutoff = int(math.ceil(len(posfeats)*0.92)) # 92%\n",
    "#neucutoff = int(math.ceil(len(neufeats)*0.92)) # 92%\n",
    " \n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff] #+ neufeats[:neucutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:] #+ neufeats[neucutoff:]\n",
    "print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out timeline graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               texts labels  \\\n",
      "0  EXCLUSIVE: Megyn Kelly opens up about her back...    neg   \n",
      "1  EXCLUSIVE: Megyn Kelly shares her thoughts on ...    neg   \n",
      "2  \"I do feel a responsibility as president of th...    neg   \n",
      "\n",
      "              datetime  \n",
      "0  2016-11-15 20:15:00  \n",
      "1  2016-11-15 20:00:00  \n",
      "2  2016-11-15 15:45:01  \n",
      "                                               texts labels  \\\n",
      "0  Democrats’ poor showing in last week’s electio...    neg   \n",
      "1  In a letter to the president-elect, Sen. Eliza...    neg   \n",
      "2  House Republicans sent a letter on Tuesday to ...    neg   \n",
      "\n",
      "              datetime  \n",
      "0  2016-11-15 23:45:10  \n",
      "1  2016-11-15 22:20:10  \n",
      "2  2016-11-15 20:30:07  \n",
      "                                               texts labels  \\\n",
      "0  \"We, your fellow residents of Century Hall, re...    pos   \n",
      "1  \"These mayors - all uber-liberals - believe th...    neg   \n",
      "2  Tampa Bay Buccaneers wide receiver Mike Evans ...    pos   \n",
      "\n",
      "              datetime  \n",
      "0  2016-11-15 21:30:03  \n",
      "1  2016-11-15 20:17:07  \n",
      "2  2016-11-15 15:35:25  \n",
      "                                               texts labels  \\\n",
      "0  Donald J. Trump's ex-wife Ivana wants to be am...    neg   \n",
      "1  \"Rudy Giuliani lacks any substantive diplomati...    neg   \n",
      "2  Megyn Kelly sees her story as a cautionary tal...    neg   \n",
      "\n",
      "              datetime  \n",
      "0  2016-11-15 23:25:00  \n",
      "1  2016-11-15 22:55:00  \n",
      "2  2016-11-15 21:57:12  \n",
      "                                               texts labels  \\\n",
      "0  Management of the complex on the Upper West Si...    neg   \n",
      "1  Her company says they are “still making adjust...    neg   \n",
      "2  In the days following the election, the ACLU N...    neg   \n",
      "\n",
      "              datetime  \n",
      "0  2016-11-15 22:18:03  \n",
      "1  2016-11-15 21:33:03  \n",
      "2  2016-11-15 20:37:47  \n",
      "                                               texts labels  \\\n",
      "0  This professor has accurately predicted the ou...    pos   \n",
      "1  Rudy Giuliani's speeches and legal work around...    neg   \n",
      "2  It could be one of the most complicated post-e...    pos   \n",
      "\n",
      "              datetime  \n",
      "0  2016-11-15 23:40:50  \n",
      "1  2016-11-15 20:45:03  \n",
      "2  2016-11-15 20:20:01  \n",
      "                                               texts labels  \\\n",
      "0  \"For Donald J. Trump, the reckoning is at hand...    neg   \n",
      "1  State representative-elect Ilhan Omar, the fir...    pos   \n",
      "2  Senate Democrats have demanded the removal of ...    neg   \n",
      "\n",
      "              datetime  \n",
      "0  2016-11-15 23:00:00  \n",
      "1  2016-11-15 22:18:20  \n",
      "2  2016-11-15 21:43:54  \n"
     ]
    }
   ],
   "source": [
    "# Importing machine_labeled data\n",
    "machine_labeled = {}\n",
    "machine_labeled['cbs'] = pd.read_csv(os.getcwd() + '/../Machine_labeled_Haru_/machine_labeled_cbs.csv')[['texts', 'labels', 'datetime']]\n",
    "machine_labeled['cnn'] = pd.read_csv(os.getcwd() + '/../Machine_labeled_Haru_/machine_labeled_cnn.csv')[['texts', 'labels', 'datetime']]\n",
    "machine_labeled['fox'] = pd.read_csv(os.getcwd() + '/../Machine_labeled_Haru_/machine_labeled_fox.csv')[['texts', 'labels', 'datetime']]\n",
    "machine_labeled['msnbc'] = pd.read_csv(os.getcwd() + '/../Machine_labeled_Haru_/machine_labeled_msnbc.csv')[['texts', 'labels', 'datetime']]\n",
    "machine_labeled['nyt'] = pd.read_csv(os.getcwd() + '/../Machine_labeled_Haru_/machine_labeled_nyt.csv')[['texts', 'labels', 'datetime']]\n",
    "machine_labeled['usatoday'] = pd.read_csv(os.getcwd() + '/../Machine_labeled_Haru_/machine_labeled_usatoday.csv')[['texts', 'labels', 'datetime']]\n",
    "machine_labeled['wsj'] = pd.read_csv(os.getcwd() + '/../Machine_labeled_Haru_/machine_labeled_wsj.csv')[['texts', 'labels', 'datetime']]\n",
    "\n",
    "for key, value in raw.iteritems():\n",
    "    print machine_labeled[key].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               texts labels  \\\n",
      "0  EXCLUSIVE: Megyn Kelly opens up about her back...    neg   \n",
      "1  EXCLUSIVE: Megyn Kelly shares her thoughts on ...    neg   \n",
      "2  \"I do feel a responsibility as president of th...    neg   \n",
      "\n",
      "              datetime     date  \n",
      "0  2016-11-15 20:15:00  2016-11  \n",
      "1  2016-11-15 20:00:00  2016-11  \n",
      "2  2016-11-15 15:45:01  2016-11  \n",
      "                                               texts labels  \\\n",
      "0  Democrats’ poor showing in last week’s electio...    neg   \n",
      "1  In a letter to the president-elect, Sen. Eliza...    neg   \n",
      "2  House Republicans sent a letter on Tuesday to ...    neg   \n",
      "\n",
      "              datetime     date  \n",
      "0  2016-11-15 23:45:10  2016-11  \n",
      "1  2016-11-15 22:20:10  2016-11  \n",
      "2  2016-11-15 20:30:07  2016-11  \n",
      "                                               texts labels  \\\n",
      "0  \"We, your fellow residents of Century Hall, re...    pos   \n",
      "1  \"These mayors - all uber-liberals - believe th...    neg   \n",
      "2  Tampa Bay Buccaneers wide receiver Mike Evans ...    pos   \n",
      "\n",
      "              datetime     date  \n",
      "0  2016-11-15 21:30:03  2016-11  \n",
      "1  2016-11-15 20:17:07  2016-11  \n",
      "2  2016-11-15 15:35:25  2016-11  \n",
      "                                               texts labels  \\\n",
      "0  Donald J. Trump's ex-wife Ivana wants to be am...    neg   \n",
      "1  \"Rudy Giuliani lacks any substantive diplomati...    neg   \n",
      "2  Megyn Kelly sees her story as a cautionary tal...    neg   \n",
      "\n",
      "              datetime     date  \n",
      "0  2016-11-15 23:25:00  2016-11  \n",
      "1  2016-11-15 22:55:00  2016-11  \n",
      "2  2016-11-15 21:57:12  2016-11  \n",
      "                                               texts labels  \\\n",
      "0  Management of the complex on the Upper West Si...    neg   \n",
      "1  Her company says they are “still making adjust...    neg   \n",
      "2  In the days following the election, the ACLU N...    neg   \n",
      "\n",
      "              datetime     date  \n",
      "0  2016-11-15 22:18:03  2016-11  \n",
      "1  2016-11-15 21:33:03  2016-11  \n",
      "2  2016-11-15 20:37:47  2016-11  \n",
      "                                               texts labels  \\\n",
      "0  This professor has accurately predicted the ou...    pos   \n",
      "1  Rudy Giuliani's speeches and legal work around...    neg   \n",
      "2  It could be one of the most complicated post-e...    pos   \n",
      "\n",
      "              datetime     date  \n",
      "0  2016-11-15 23:40:50  2016-11  \n",
      "1  2016-11-15 20:45:03  2016-11  \n",
      "2  2016-11-15 20:20:01  2016-11  \n",
      "                                               texts labels  \\\n",
      "0  \"For Donald J. Trump, the reckoning is at hand...    neg   \n",
      "1  State representative-elect Ilhan Omar, the fir...    pos   \n",
      "2  Senate Democrats have demanded the removal of ...    neg   \n",
      "\n",
      "              datetime     date  \n",
      "0  2016-11-15 23:00:00  2016-11  \n",
      "1  2016-11-15 22:18:20  2016-11  \n",
      "2  2016-11-15 21:43:54  2016-11  \n"
     ]
    }
   ],
   "source": [
    "# Extracting necessary date and time information\n",
    "for key, value in raw.iteritems():\n",
    "    machine_labeled[key]['date'] = \"\"\n",
    "    \n",
    "    for index, row in machine_labeled[key].iterrows():\n",
    "        date_obj = datetime.strptime(row['datetime'], \"%Y-%m-%d %H:%M:%S\")\n",
    "        machine_labeled[key]['date'][index] = date_obj.strftime(\"%Y-%m\")\n",
    "    \n",
    "    print machine_labeled[key].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usatoday\n",
      "     date  neg  pos\n",
      "0   15-04    6    6\n",
      "1   15-05   12    1\n",
      "2   15-06   14    6\n",
      "3   15-07   20    7\n",
      "4   15-08   30   12\n",
      "5   15-09   28   17\n",
      "6   15-10   29   12\n",
      "7   15-11   25    6\n",
      "8   15-12   30   15\n",
      "9   16-01   33   16\n",
      "10  16-02   31   42\n",
      "11  16-03   72   44\n",
      "12  16-04   42   18\n",
      "13  16-05   49   30\n",
      "14  16-06   54   23\n",
      "15  16-07  117   74\n",
      "16  16-08   56   33\n",
      "17  16-09  106   26\n",
      "18  16-10  262   82\n",
      "19  16-11  170  106\n",
      "wsj\n",
      "     date  neg  pos\n",
      "0   15-04   10    7\n",
      "1   15-05   14   14\n",
      "2   15-06   15   20\n",
      "3   15-07   16   14\n",
      "4   15-08   38   18\n",
      "5   15-09   23   22\n",
      "6   15-10   26   27\n",
      "7   15-11   28   21\n",
      "8   15-12   42   20\n",
      "9   16-01   41   35\n",
      "10  16-02   61   70\n",
      "11  16-03   77  102\n",
      "12  16-04   71   79\n",
      "13  16-05  112   91\n",
      "14  16-06   96   56\n",
      "15  16-07  130  104\n",
      "16  16-08  109   74\n",
      "17  16-09  131   76\n",
      "18  16-10  201  122\n",
      "19  16-11  193  133\n",
      "fox\n",
      "     date  neg  pos\n",
      "0   15-04   21   13\n",
      "1   15-05   21    8\n",
      "2   15-06   24    9\n",
      "3   15-07   52   11\n",
      "4   15-08   72   29\n",
      "5   15-09   59   20\n",
      "6   15-10   64   35\n",
      "7   15-11   45   24\n",
      "8   15-12   73   25\n",
      "9   16-01  108   45\n",
      "10  16-02  159   62\n",
      "11  16-03  218  118\n",
      "12  16-04  160  101\n",
      "13  16-05  194   91\n",
      "14  16-06  170   68\n",
      "15  16-07  226  102\n",
      "16  16-08  254   96\n",
      "17  16-09  258   86\n",
      "18  16-10  386  132\n",
      "19  16-11  196  144\n",
      "nyt\n",
      "     date  neg  pos\n",
      "0   15-04   19   10\n",
      "1   15-05   31   11\n",
      "2   15-06   28   12\n",
      "3   15-07   61   18\n",
      "4   15-08   83   26\n",
      "5   15-09   51   23\n",
      "6   15-10   68   24\n",
      "7   15-11   36   24\n",
      "8   15-12   63   17\n",
      "9   16-01   82   30\n",
      "10  16-02  101   65\n",
      "11  16-03  169   66\n",
      "12  16-04  119   45\n",
      "13  16-05  161   52\n",
      "14  16-06  124   49\n",
      "15  16-07  213  110\n",
      "16  16-08  181   54\n",
      "17  16-09  198   69\n",
      "18  16-10  303   95\n",
      "19  16-11  215  100\n",
      "cbs\n",
      "     date  neg  pos\n",
      "0   15-04    9    8\n",
      "1   15-05    7    3\n",
      "2   15-06    6    7\n",
      "3   15-07   26    8\n",
      "4   15-08   36   16\n",
      "5   15-09   44   21\n",
      "6   15-10   32   21\n",
      "7   15-11   40   11\n",
      "8   15-12   60   31\n",
      "9   16-01   57   32\n",
      "10  16-02  100   90\n",
      "11  16-03  125   94\n",
      "12  16-04   70   51\n",
      "13  16-05   55   32\n",
      "14  16-06   69   40\n",
      "15  16-07  144  118\n",
      "16  16-08  129   63\n",
      "17  16-09  147   58\n",
      "18  16-10  276  137\n",
      "19  16-11  279  238\n",
      "cnn\n",
      "     date  neg  pos\n",
      "0   15-04    7    3\n",
      "1   15-05   13    5\n",
      "2   15-06   12   14\n",
      "3   15-07   34   12\n",
      "4   15-08   45   22\n",
      "5   15-09   95   47\n",
      "6   15-10   62   59\n",
      "7   15-11   65   34\n",
      "8   15-12  123   57\n",
      "9   16-01  123   53\n",
      "10  16-02  162  110\n",
      "11  16-03  199  116\n",
      "12  16-04  111   65\n",
      "13  16-05  126   60\n",
      "14  16-06  147   57\n",
      "15  16-07  216  153\n",
      "16  16-08  151   62\n",
      "17  16-09  139   82\n",
      "18  16-10  312  135\n",
      "19  16-11  202  196\n",
      "msnbc\n",
      "     date  neg  pos\n",
      "0   15-04   29   12\n",
      "1   15-05   22   12\n",
      "2   15-06   47   12\n",
      "3   15-07   90   25\n",
      "4   15-08   97   32\n",
      "5   15-09  120   24\n",
      "6   15-10  110   55\n",
      "7   15-11   91   36\n",
      "8   15-12   93   37\n",
      "9   16-01  113   62\n",
      "10  16-02  158  123\n",
      "11  16-03  177  125\n",
      "12  16-04  124   81\n",
      "13  16-05  176   78\n",
      "14  16-06  164   80\n",
      "15  16-07  170  109\n",
      "16  16-08  224   93\n",
      "17  16-09  241  118\n",
      "18  16-10  331  134\n",
      "19  16-11  152  110\n"
     ]
    }
   ],
   "source": [
    "# Getting number of positive vs negative posts per month\n",
    "PosNegNumbers = {}\n",
    "\n",
    "for key, value in raw.iteritems():\n",
    "    print key\n",
    "    # x = date, y = # of pos, # of neg\n",
    "    PosNegNumbers[key]=pd.DataFrame(index=range(0,20), columns=[\"date\",\"neg\",\"pos\"]) \n",
    "\n",
    "    index = 0\n",
    "    for y in range(5,7): #2015~2016\n",
    "        for m in range(1,13): #1~12 Jan~Dec\n",
    "            the_date = \"\"\n",
    "            if m < 10:\n",
    "                the_date = \"201\" + str(y) + \"-0\" + str(m)\n",
    "            else:\n",
    "                the_date = \"201\" + str(y) + \"-\" + str(m)\n",
    "                \n",
    "            # Get a sub DataFrame in the given time (y, m) above\n",
    "            subDF = machine_labeled[key].loc[machine_labeled[key]['date'] == the_date]['labels']\n",
    "            if subDF.empty == False: # if not empty\n",
    "                PosNegNumbers[key]['date'][index] = the_date[2:] #yyyy-mm to yy-mm\n",
    "                PosNegNumbers[key]['neg'][index] = subDF.str.count(\"neg\").sum()\n",
    "                PosNegNumbers[key]['pos'][index] = subDF.str.count(\"pos\").sum()\n",
    "                index += 1\n",
    "                \n",
    "    print PosNegNumbers[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///var/folders/r3/skg8cjw97l57x8hgptdqftp80000gn/T/tmpEzjO8i.html\n"
     ]
    }
   ],
   "source": [
    "# Displaying in pygal graph\n",
    "# the number is the ratio neg/pos, so 2 means twice as many negative posts.\n",
    "\n",
    "line_chart = pygal.Line()\n",
    "line_chart.title = \"Media's Attitude\"\n",
    "line_chart.x_labels = map(str, PosNegNumbers['cnn']['date'])\n",
    "for key, value in PosNegNumbers.iteritems():\n",
    "    newList = []\n",
    "    for index, row in PosNegNumbers[key].iterrows():\n",
    "        newList.append(row['neg']*1.0/row['pos'])\n",
    "    line_chart.add(key, newList)\n",
    "line_chart.render_in_browser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which words show up the most in each news media?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File /Users/Haru/Downloads/489Project/python/../Machine_labeled_Haru_/machine_labeled_cbs.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-566ca410ff2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Importing machine_labeled data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmachine_labeled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmachine_labeled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cbs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/../Machine_labeled_Haru_/machine_labeled_cbs.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'texts'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'labels'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'datetime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmachine_labeled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cnn'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/../Machine_labeled_Haru_/machine_labeled_cnn.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'texts'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'labels'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'datetime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmachine_labeled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fox'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/../Machine_labeled_Haru_/machine_labeled_fox.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'texts'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'labels'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'datetime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1211\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3427)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:6861)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File /Users/Haru/Downloads/489Project/python/../Machine_labeled_Haru_/machine_labeled_cbs.csv does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# Importing machine_labeled data\n",
    "machine_labeled = {}\n",
    "machine_labeled['cbs'] = pd.read_csv(os.getcwd() + '/../Machine_labeled_Haru_/machine_labeled_cbs.csv')[['texts', 'labels', 'datetime']]\n",
    "machine_labeled['cnn'] = pd.read_csv(os.getcwd() + '/../Machine_labeled_Haru_/machine_labeled_cnn.csv')[['texts', 'labels', 'datetime']]\n",
    "machine_labeled['fox'] = pd.read_csv(os.getcwd() + '/../Machine_labeled_Haru_/machine_labeled_fox.csv')[['texts', 'labels', 'datetime']]\n",
    "machine_labeled['msnbc'] = pd.read_csv(os.getcwd() + '/../Machine_labeled_Haru_/machine_labeled_msnbc.csv')[['texts', 'labels', 'datetime']]\n",
    "machine_labeled['nyt'] = pd.read_csv(os.getcwd() + '/../Machine_labeled_Haru_/machine_labeled_nyt.csv')[['texts', 'labels', 'datetime']]\n",
    "machine_labeled['usatoday'] = pd.read_csv(os.getcwd() + '/../Machine_labeled_Haru_/machine_labeled_usatoday.csv')[['texts', 'labels', 'datetime']]\n",
    "machine_labeled['wsj'] = pd.read_csv(os.getcwd() + '/../Machine_labeled_Haru_/machine_labeled_wsj.csv')[['texts', 'labels', 'datetime']]\n",
    "\n",
    "for key, value in machine_labeled.iteritems():\n",
    "    machine_labeled[key] = machine_labeled[key][[\"texts\", \"labels\"]] # get rid of datetime column\n",
    "    \n",
    "    print key\n",
    "    print machine_labeled[key].head(3)\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lemmanize the sentences (e.g. is, are, am ->> be)\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "for key, value in machine_labeled.iteritems():\n",
    "    new_index = range(0, len(machine_labeled[key].index))\n",
    "    new_sample = pd.DataFrame(index=new_index, columns=(\"texts\", \"sources\", \"labels\"))\n",
    "    i=0\n",
    "    for text in machine_labeled[key]['texts']:\n",
    "        text = re.sub(r'\\W+' , ' ', text) # Getting rid of punctuation\n",
    "        blob = TextBlob(text.decode(\"utf-8\"))\n",
    "        newtexts = \"\"\n",
    "\n",
    "        for sentence in blob.sentences:\n",
    "            newtext = \"\"\n",
    "\n",
    "            for word in sentence.words:\n",
    "                newtext += \" \" + word.lemmatize('v') # 'v' for 'verb'\n",
    "\n",
    "            newtexts += newtext\n",
    "        new_sample['texts'].loc[i] = newtexts\n",
    "        i += 1\n",
    "\n",
    "    machine_labeled[key]['texts'] = new_sample['texts']\n",
    "    \n",
    "    print key\n",
    "    print machine_labeled[key].head()\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting rid of too common words (e.g. the, an)\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "\n",
    "for key, value in machine_labeled.iteritems():\n",
    "    new_index = range(0, len(machine_labeled[key].index))\n",
    "    new_sample = pd.DataFrame(index=new_index, columns=(\"texts\", \"sources\", \"labels\"))\n",
    "\n",
    "    for index, row in machine_labeled[key].iterrows():\n",
    "        # If this new post is different post, then append what we worked so far\n",
    "        words = [w for w in row['texts'].split() if not w in stopwords.words(\"english\")]\n",
    "        new_sample['texts'].loc[index] = ' '.join(words)\n",
    "\n",
    "    machine_labeled[key]['texts'] = new_sample['texts']\n",
    "    \n",
    "    print key\n",
    "    print machine_labeled[key].head()\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finding out word frequencies\n",
    "#     e.g. \"I think therefore I am\"\n",
    "# wordlist = [\"I\", \"think\", \"therefore\", \"am\"]\n",
    "# wordfreq = (2, 1, 1, 1)\n",
    "# wordpairs = [(\"I\", 2), (\"think\", 1), (\"therefore\", 1), (\"am\", 1)]\n",
    "\n",
    "for key, value in machine_labeled.iteritems():\n",
    "    machine_labeled[key][\"wordlist\"] = \"\"\n",
    "    machine_labeled[key][\"wordfreq\"] = \"\"\n",
    "    \n",
    "    for index, row in machine_labeled[key].iterrows():\n",
    "        wordlist = machine_labeled[key][\"texts\"][index].split()\n",
    "\n",
    "        wordpair = []\n",
    "        for w in list(set(wordlist)): # get unique members of wordlist\n",
    "            wordpair.append({w:wordlist.count(w)})\n",
    "\n",
    "        machine_labeled[key][\"wordlist\"][index] = wordlist\n",
    "        machine_labeled[key][\"wordfreq\"][index] = wordpair\n",
    "        \n",
    "    print key\n",
    "    print machine_labeled[key].head()\n",
    "    print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finding out word frequencies per POSTS (per media)\n",
    "#     so if CNN has 5 posts that includes a substring \"Clinton\", \n",
    "#     which each of them mentions the name 1, 2, 3, 8, and 9 times,\n",
    "#     this will still record (\"Clinton\", 5)\n",
    "from itertools import islice\n",
    "import copy \n",
    "\n",
    "wordfreq_per_posts = {}\n",
    "\n",
    "for key, value in machine_labeled.iteritems():\n",
    "    wordfreq_per_posts[key] = \"\"\n",
    "    total_string = \"\"\n",
    "    \n",
    "    for index, row in machine_labeled[key].iterrows():\n",
    "        # \"join\" joins [\"1\", \"2\", \"3\"] to \"1 2 3\"\n",
    "        total_string += ' '.join(machine_labeled[key][\"wordlist\"][index]) + ' '\n",
    "\n",
    "    full_wordlist = total_string.split()\n",
    "    wordlist = []\n",
    "    wordfreq = []\n",
    "    for w in list(set(full_wordlist)):\n",
    "        wordlist.append(w)\n",
    "        wordfreq.append(full_wordlist.count(w))\n",
    "    \n",
    "    sorted_wordfreq = copy.deepcopy(wordfreq)\n",
    "    sorted_wordfreq.sort(reverse=True)\n",
    "    \n",
    "    sorted_wordpair = []\n",
    "    for num in sorted_wordfreq:\n",
    "        index = wordfreq.index(num)\n",
    "        sorted_wordpair.append({wordlist[index]: num})\n",
    "    \n",
    "    wordfreq_per_posts[key] = sorted_wordpair\n",
    "    \n",
    "    print key\n",
    "    for a_tuple in wordfreq_per_posts[key][:3]: \n",
    "        print a_tuple\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An attempt to draw a network diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# something like this\n",
    "# http://d3wakyvh0y1ay.cloudfront.net/content/spbds/2/1/2053951715572916/F3.large.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://networkx.readthedocs.io/en/stable/examples/drawing/chess_masters.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "c\n",
      "d\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Random graph from given degree sequence.\n",
    "Draw degree rank plot and graph with matplotlib.\n",
    "\"\"\"\n",
    "# Author: Aric Hagberg <aric.hagberg@gmail.com>\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.gnp_random_graph(100,0.02)\n",
    "print \"a\"\n",
    "degree_sequence=sorted(nx.degree(G).values(),reverse=True) # degree sequence\n",
    "#print \"Degree sequence\", degree_sequence\n",
    "dmax=max(degree_sequence)\n",
    "print \"b\"\n",
    "plt.loglog(degree_sequence,'b-',marker='o')\n",
    "plt.title(\"Degree rank plot\")\n",
    "plt.ylabel(\"degree\")\n",
    "plt.xlabel(\"rank\")\n",
    "print \"c\"\n",
    "# draw graph in inset\n",
    "plt.axes([0.45,0.45,0.45,0.45])\n",
    "Gcc=sorted(nx.connected_component_subgraphs(G), key = len, reverse=True)[0]\n",
    "pos=nx.spring_layout(Gcc)\n",
    "plt.axis('off')\n",
    "nx.draw_networkx_nodes(Gcc,pos,node_size=20)\n",
    "nx.draw_networkx_edges(Gcc,pos,alpha=0.4)\n",
    "print \"d\"\n",
    "plt.savefig(\"degree_histogram.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Draw a graph with matplotlib, color edges.\n",
    "You must have matplotlib>=87.7 for this to work.\n",
    "\"\"\"\n",
    "# Author: Aric Hagberg (hagberg@lanl.gov)\n",
    "#try:\n",
    "import matplotlib.pyplot as plt\n",
    "#except:\n",
    "#    raise\n",
    "#\n",
    "import networkx as nx\n",
    "\n",
    "G=nx.star_graph(20)\n",
    "print \"a\"\n",
    "pos=nx.spring_layout(G)\n",
    "print \"b\"\n",
    "colors=range(20)\n",
    "print \"c\"\n",
    "nx.draw(G,pos,node_color='#A0CBE2',edge_color=colors,width=4,edge_cmap=plt.cm.Blues,with_labels=False)\n",
    "print \"d\"\n",
    "#plt.savefig(\"edge_colormap.png\") # save as png\n",
    "plt.show() # display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G=nx.star_graph(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos=nx.spring_layout(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors=range(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.draw(G,pos,node_color='#A0CBE2',edge_color=colors,width=4,edge_cmap=plt.cm.Blues,with_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "#An example using Graph as a weighted network.\n",
    "\n",
    "# Author: Aric Hagberg (hagberg@lanl.gov)\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "G=nx.Graph()\n",
    "\n",
    "G.add_edge('a','b',weight=0.6)\n",
    "G.add_edge('a','c',weight=0.2)\n",
    "G.add_edge('c','d',weight=0.1)\n",
    "G.add_edge('c','e',weight=0.7)\n",
    "G.add_edge('c','f',weight=0.9)\n",
    "G.add_edge('a','d',weight=0.3)\n",
    "print \"a\"\n",
    "\n",
    "elarge=[(u,v) for (u,v,d) in G.edges(data=True) if d['weight'] >0.5]\n",
    "esmall=[(u,v) for (u,v,d) in G.edges(data=True) if d['weight'] <=0.5]\n",
    "\n",
    "pos=nx.spring_layout(G) # positions for all nodes\n",
    "print \"b\"\n",
    "\n",
    "# nodes\n",
    "nx.draw_networkx_nodes(G,pos,node_size=700)\n",
    "print \"c\"\n",
    "\n",
    "# edges\n",
    "nx.draw_networkx_edges(G,pos,edgelist=elarge,\n",
    "                    width=6)\n",
    "nx.draw_networkx_edges(G,pos,edgelist=esmall,\n",
    "                    width=6,alpha=0.5,edge_color='b',style='dashed')\n",
    "print \"d\"\n",
    "\n",
    "# labels\n",
    "nx.draw_networkx_labels(G,pos,font_size=20,font_family='sans-serif')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.savefig(\"weighted_graph.png\") # save as png\n",
    "print \"e\"\n",
    "plt.show() # display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "[('a', 'b')]\n",
      "b\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "#An example using Graph as a weighted network.\n",
    "\n",
    "# Author: Aric Hagberg (hagberg@lanl.gov)\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "G=nx.Graph()\n",
    "\n",
    "G.add_edge('a','b',weight=0.6)\n",
    "G.add_edge('a','c',weight=0.2)\n",
    "print \"a\"\n",
    "\n",
    "elarge=[(u,v) for (u,v,d) in G.edges(data=True) if d['weight'] >0.5]\n",
    "esmall=[(u,v) for (u,v,d) in G.edges(data=True) if d['weight'] <=0.5]\n",
    "print elarge\n",
    "\n",
    "pos=nx.spring_layout(G) # positions for all nodes\n",
    "print \"b\"\n",
    "\n",
    "# nodes\n",
    "nx.draw_networkx_nodes(G,pos,nodelist=['a'], node_size=700)\n",
    "print \"c\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Supervised Learning w/ manual scripting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task 1: Load the texts\n",
    "import pandas as pd\n",
    "import glob, os         # for reading all .txt files\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.base import ClassifierMixin\n",
    "#from sklearn.dummy import DummyClassifier\n",
    "#from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "#from sklearn.multiclass import OneVsRestClassifier\n",
    "#from sklearn.multiclass import OneVsOneClassifier\n",
    "#from sklearn.multiclass import OutputCodeClassifier\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "#from sklearn.calibration import CalibratedClassifierCV\n",
    "#from sklearn.semi_supervised import LabelPropagation\n",
    "import math\n",
    "import scipy.sparse as sp\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from textblob import TextBlob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 5)\n",
      "(1402, 4)\n",
      "                                               texts   sources  labels  \\\n",
      "0  It is unusual for a president's children to be...       cnn       1   \n",
      "1  Just hours after Mitt Romney blasted Donald J....       cbs      -1   \n",
      "2  Hillary Clinton wins the state of Washington, ...       cbs       1   \n",
      "3  \"We have to take the jobs away from other coun...       fox      -1   \n",
      "4  Jessica Leeds, 74, of Manhattan, was one of tw...  usatoday      -1   \n",
      "\n",
      "                         statusID  \n",
      "0    5550296508_10155606385896509  \n",
      "1  131459315949_10153427911075950  \n",
      "2  131459315949_10154056641215950  \n",
      "3   15704546335_10153385284896336  \n",
      "4   13652355666_10153933280450667  \n"
     ]
    }
   ],
   "source": [
    "# THREE ANSWERS TO ONE\n",
    "sample = pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/Turk/6source_results_filtered.csv') \n",
    "#sample['Answer.sentiment'] = sample['Answer.sentiment'].map({'Positive':1, 'Neutral':0, 'Negative':-1})\n",
    "\n",
    "new_index = range(0, len(sample.index)/3)\n",
    "new_sample = pd.DataFrame(index=new_index, columns=['texts', 'sources', 'labels', 'statusID'])\n",
    "\n",
    "text = \"\"\n",
    "source = \"\"\n",
    "statusID = \"\"\n",
    "#acc_value = 0 # accumulated score per post\n",
    "#num_posts = 0 # number of same text posts\n",
    "pos = 0 # number of 'positive' per post\n",
    "neu = 0 # number of 'neutral' per post\n",
    "neg = 0 # number of 'negative' per post\n",
    "pd_index = 0\n",
    "\n",
    "for index, row in sample.iterrows():\n",
    "    # If this new post is different post, then append what we worked so far\n",
    "    if text != row['Input.content']:\n",
    "        if text != \"\":\n",
    "            if pos >= 2:\n",
    "                new_sample.loc[[pd_index], ['labels']] = 1\n",
    "            elif neu >= 2: # could be commented out for pos/neg\n",
    "                new_sample.loc[[pd_index], ['labels']] = 0\n",
    "            elif neg >= 2:\n",
    "                new_sample.loc[[pd_index], ['labels']] = -1\n",
    "            else: # 1:1:1\n",
    "                #continue # discard\n",
    "                pd_index -= 1 # to keep the same index\n",
    "\n",
    "            #new_sample.loc[[pd_index], ['score']] = acc_value*1.0/num_posts\n",
    "            new_sample.loc[[pd_index], ['sources']] = source\n",
    "            new_sample.loc[[pd_index], ['texts']] = text\n",
    "            new_sample.loc[[pd_index], ['statusID']] = statusID\n",
    "            pd_index += 1\n",
    "            \n",
    "        # Assign a new post\n",
    "        text = row['Input.content']\n",
    "        source = row['Input.source']\n",
    "        statusID = row['Input.statis_id']\n",
    "        #acc_value = 0\n",
    "        #num_posts = 0\n",
    "        pos = 0\n",
    "        neu = 0\n",
    "        neg = 0\n",
    "        \n",
    "    if row['Answer.sentiment'] == \"Positive\":\n",
    "        #acc_value += 1\n",
    "        #num_posts += 1\n",
    "        pos += 1\n",
    "    elif row['Answer.sentiment'] == \"Neutral\":\n",
    "        #num_posts += 1\n",
    "        neu += 1\n",
    "    elif row['Answer.sentiment'] == \"Negative\":\n",
    "        #acc_value -= 1\n",
    "        #num_posts += 1\n",
    "        neg += 1\n",
    "\n",
    "# not perfect, so we have some extra NaN rows (all should be filled, technically)\n",
    "new_sample = new_sample.dropna()\n",
    "        \n",
    "# change 0.333, 0.666, ... to whole number by x 3\n",
    "#new_sample.loc[:, 'score'] *= 3\n",
    "new_sample['labels'] = new_sample['labels'].astype('int')\n",
    "    \n",
    "#new_sample['sources'] = new_sample['sources'].map({'msnbc':4, \n",
    "#                                                   'cnn':3, \n",
    "#                                                   'cbs':2,\n",
    "#                                                   'usatoday':1,\n",
    "#                                                   'wsj':-1,\n",
    "#                                                   'fox':-2})\n",
    "#new_sample['sources'] = new_sample['sources'].astype('int')\n",
    "print sample.shape\n",
    "sample = new_sample\n",
    "\n",
    "#cutoff = int(math.ceil(len(sample.index)*0.9)) # 90%\n",
    "#train = pd.DataFrame({'labels':sample['labels'][:cutoff], 'texts':sample['texts'][:cutoff], 'sources':sample['sources'][:cutoff]})\n",
    "#test  = pd.DataFrame({'labels':sample['labels'][cutoff:], 'texts':sample['texts'][cutoff:], 'sources':sample['sources'][cutoff:]})\n",
    "#print train.shape\n",
    "#print test.shape\n",
    "print sample.shape\n",
    "print sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               texts   sources  labels  \\\n",
      "0  It is unusual for a president's children to be...       cnn       1   \n",
      "1  Just hours after Mitt Romney blasted Donald J....       cbs      -1   \n",
      "2  Hillary Clinton wins the state of Washington, ...       cbs       1   \n",
      "3  \"We have to take the jobs away from other coun...       fox      -1   \n",
      "4  Jessica Leeds, 74, of Manhattan, was one of tw...  usatoday      -1   \n",
      "\n",
      "                         statusID candidates  \n",
      "0    5550296508_10155606385896509      trump  \n",
      "1  131459315949_10153427911075950      trump  \n",
      "2  131459315949_10154056641215950    clinton  \n",
      "3   15704546335_10153385284896336      trump  \n",
      "4   13652355666_10153933280450667      trump  \n"
     ]
    }
   ],
   "source": [
    "# classifying candidates\n",
    "def classify_post (row):\n",
    "   status = row['texts'].lower()\n",
    "   if (('donald' in status) or ('trump' in status)) and (('hillary' in status) or ('clinton' in status)) :\n",
    "      return 'other'\n",
    "   elif ('donald' in status) or ('trump' in status) :\n",
    "      return 'trump'\n",
    "   elif ('hillary' in status) or ('clinton' in status) :\n",
    "      return 'clinton'\n",
    "   return 'other'\n",
    "\n",
    "sample['candidates'] = sample.apply (lambda row: classify_post (row),axis=1)\n",
    "\n",
    "print sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Haru/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1402, 6)\n",
      "(1402, 6)\n",
      "                                               texts   sources  labels  \\\n",
      "0  It is unusual for a president's children to be...       cnn       1   \n",
      "1  Just hours after Mitt Romney blasted Donald J....       cbs      -1   \n",
      "2  Hillary Clinton wins the state of Washington, ...       cbs       1   \n",
      "3  \"We have to take the jobs away from other coun...       fox      -1   \n",
      "4  Jessica Leeds, 74, of Manhattan, was one of tw...  usatoday      -1   \n",
      "\n",
      "                         statusID candidates                dates  \n",
      "0    5550296508_10155606385896509      trump  2016-11-15 00:00:02  \n",
      "1  131459315949_10153427911075950      trump  2016-03-03 14:44:04  \n",
      "2  131459315949_10154056641215950    clinton  2016-11-09 00:03:03  \n",
      "3   15704546335_10153385284896336      trump  2015-06-23 02:00:01  \n",
      "4   13652355666_10153933280450667      trump  2016-10-13 21:42:41  \n"
     ]
    }
   ],
   "source": [
    "raw = {}\n",
    "raw['cbs'] = pd.read_csv('/Users/Haru/Documents/! college/4. Fall 2016/489Project/Facebook_RAW/CBSNews_facebook_statuses.csv')\n",
    "raw['cnn'] = pd.read_csv('/Users/Haru/Documents/! college/4. Fall 2016/489Project/Facebook_RAW/cnn_facebook_statuses.csv', encoding='utf-8')\n",
    "raw['fox'] = pd.read_csv('/Users/Haru/Documents/! college/4. Fall 2016/489Project/Facebook_RAW/FoxNews_facebook_statuses.csv', encoding='utf-8')\n",
    "raw['msnbc'] = pd.read_csv('/Users/Haru/Documents/! college/4. Fall 2016/489Project/Facebook_RAW/msnbc_facebook_statuses.csv', encoding='utf-8') \n",
    "raw['nyt'] = pd.read_csv('/Users/Haru/Documents/! college/4. Fall 2016/489Project/Facebook_RAW/nytimes_facebook_statuses.csv', encoding='utf-8') \n",
    "raw['usatoday'] = pd.read_csv('/Users/Haru/Documents/! college/4. Fall 2016/489Project/Facebook_RAW/usatoday_facebook_statuses.csv', encoding='utf-8') \n",
    "raw['wsj'] = pd.read_csv('/Users/Haru/Documents/! college/4. Fall 2016/489Project/Facebook_RAW/wsj_facebook_statuses.csv', encoding='utf-8') \n",
    "\n",
    "sample['dates'] = \"\"\n",
    "\n",
    "for index, row in sample.iterrows():\n",
    "    the_source = row['sources']\n",
    "    \n",
    "    raw_index = list(raw[the_source]['status_id']).index(row['statusID'])\n",
    "    sample['dates'][index] = raw[the_source]['status_published'][raw_index]\n",
    "\n",
    "print sample.dropna().shape\n",
    "print sample.shape\n",
    "print sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Haru/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Haru/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>sources</th>\n",
       "      <th>labels</th>\n",
       "      <th>statusID</th>\n",
       "      <th>candidates</th>\n",
       "      <th>dates</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It is unusual for a president's children to be...</td>\n",
       "      <td>cnn</td>\n",
       "      <td>1</td>\n",
       "      <td>5550296508_10155606385896509</td>\n",
       "      <td>trump</td>\n",
       "      <td>2016-11-15 00:00:02</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just hours after Mitt Romney blasted Donald J....</td>\n",
       "      <td>cbs</td>\n",
       "      <td>-1</td>\n",
       "      <td>131459315949_10153427911075950</td>\n",
       "      <td>trump</td>\n",
       "      <td>2016-03-03 14:44:04</td>\n",
       "      <td>2016-03</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hillary Clinton wins the state of Washington, ...</td>\n",
       "      <td>cbs</td>\n",
       "      <td>1</td>\n",
       "      <td>131459315949_10154056641215950</td>\n",
       "      <td>clinton</td>\n",
       "      <td>2016-11-09 00:03:03</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"We have to take the jobs away from other coun...</td>\n",
       "      <td>fox</td>\n",
       "      <td>-1</td>\n",
       "      <td>15704546335_10153385284896336</td>\n",
       "      <td>trump</td>\n",
       "      <td>2015-06-23 02:00:01</td>\n",
       "      <td>2015-06</td>\n",
       "      <td>02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jessica Leeds, 74, of Manhattan, was one of tw...</td>\n",
       "      <td>usatoday</td>\n",
       "      <td>-1</td>\n",
       "      <td>13652355666_10153933280450667</td>\n",
       "      <td>trump</td>\n",
       "      <td>2016-10-13 21:42:41</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts   sources  labels  \\\n",
       "0  It is unusual for a president's children to be...       cnn       1   \n",
       "1  Just hours after Mitt Romney blasted Donald J....       cbs      -1   \n",
       "2  Hillary Clinton wins the state of Washington, ...       cbs       1   \n",
       "3  \"We have to take the jobs away from other coun...       fox      -1   \n",
       "4  Jessica Leeds, 74, of Manhattan, was one of tw...  usatoday      -1   \n",
       "\n",
       "                         statusID candidates                dates     date  \\\n",
       "0    5550296508_10155606385896509      trump  2016-11-15 00:00:02  2016-11   \n",
       "1  131459315949_10153427911075950      trump  2016-03-03 14:44:04  2016-03   \n",
       "2  131459315949_10154056641215950    clinton  2016-11-09 00:03:03  2016-11   \n",
       "3   15704546335_10153385284896336      trump  2015-06-23 02:00:01  2015-06   \n",
       "4   13652355666_10153933280450667      trump  2016-10-13 21:42:41  2016-10   \n",
       "\n",
       "  hour  \n",
       "0   00  \n",
       "1   14  \n",
       "2   00  \n",
       "3   02  \n",
       "4   21  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['date'] = \"\"\n",
    "sample['hour'] = \"\"\n",
    "\n",
    "for index, row in sample.iterrows():\n",
    "    date_obj = datetime.strptime(row['dates'], \"%Y-%m-%d %H:%M:%S\")\n",
    "    sample['date'][index] = date_obj.strftime(\"%Y-%m\")\n",
    "    sample['hour'][index] = date_obj.strftime(\"%H\")\n",
    "\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d26b781c0228>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mnewtext\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'v'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 'v' for 'verb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mnewtexts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnewtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/textblob/decorators.pyc\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/textblob/blob.pyc\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, pos)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcached_property\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/nltk/stem/wordnet.pyc\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__reader_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# This is where the magic happens!  Transform ourselves into\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/nltk/corpus/reader/wordnet.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, omw_reader)\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0;31m# Load the indices for lemmas and synset offsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1055\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_lemma_pos_offset_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         \u001b[0;31m# load the exception file data into memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/nltk/corpus/reader/wordnet.pyc\u001b[0m in \u001b[0;36m_load_lemma_pos_offset_map\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m                     \u001b[0;31m# same as number of synsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m                     \u001b[0mn_senses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_next_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0mn_synsets\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn_senses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Lemmanize the sentences (e.g. is, are, am ->> be)\n",
    "# Might be better to skip it\n",
    "\n",
    "new_index = range(0, len(sample.index))\n",
    "new_sample = pd.DataFrame(index=new_index, columns=(\"texts\", \"sources\", \"labels\"))\n",
    "i=0\n",
    "for text in sample['texts']:\n",
    "    blob = TextBlob(text)\n",
    "    newtexts = \"\"\n",
    "\n",
    "    for sentence in blob.sentences:\n",
    "        newtext = \"\"\n",
    "\n",
    "        for word in sentence.words:\n",
    "            newtext += \" \" + word.lemmatize('v') # 'v' for 'verb'\n",
    "\n",
    "        newtexts += newtext\n",
    "    new_sample['texts'].loc[i] = newtexts\n",
    "    i += 1\n",
    "    \n",
    "sample['texts'] = new_sample['texts']\n",
    "print sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  texts   sources  labels  \\\n",
      "0     It unusual president 's children clue White Ho...       cnn       1   \n",
      "1     Just hours Mitt Romney blast Donald J Trump sp...       cbs      -1   \n",
      "2     Hillary Clinton win state Washington CBS News ...       cbs       1   \n",
      "3     We take job away countries make product 're ta...       fox      -1   \n",
      "4     Jessica Leeds 74 Manhattan one two women quote...  usatoday      -1   \n",
      "5     Ash Carter warn Russia policy Syria consequenc...       cbs      -1   \n",
      "6     It 's Rubio Watch Donald J Trump use water bot...       fox       0   \n",
      "7     Online Trump supporters discuss monitor poll p...  usatoday      -1   \n",
      "8     This mogul say 's member lucky sperm club How ...       cnn       1   \n",
      "9     Live Hillary Clinton speak NAACP Convention Ci...       fox       0   \n",
      "10    BREAKING Hillary Clinton choose Virginia Senat...       cbs       0   \n",
      "11    The billboard translate read Donald Trump ca n...  usatoday      -1   \n",
      "12    How weird world politics 2015 We n't even need...       cnn      -1   \n",
      "13    NEW A Chicago police spokesperson tell CBS New...       cbs       0   \n",
      "14    Hillary Clinton bad things Donald J Trump tell...       cbs      -1   \n",
      "15    Tim Kaine defend work Clinton Foundation say C...       fox       1   \n",
      "16    Just Clinton Trump square would become one nas...  usatoday       0   \n",
      "17    Chelsea take Twitter announce second child mot...       cbs       1   \n",
      "18         One adviser describe Clinton obsess Jeb Bush  usatoday      -1   \n",
      "19    If look American human 're totally disgust 's ...       fox      -1   \n",
      "20    A new report show donors Clinton Foundation sp...       cbs      -1   \n",
      "21    Donald J Trump 's list include six federal app...       wsj       0   \n",
      "22    Sen Elizabeth Warren launch blister attack Don...       cnn      -1   \n",
      "23    Former KKK grand wizard David Duke n't win sen...  usatoday       0   \n",
      "24    We work heal divisions long campaign Paul Ryan...       cbs       1   \n",
      "25    For 40 years Donald J Trump part corruption Wa...       fox      -1   \n",
      "26    Michelle Obama President Obama Joe Biden campa...       cbs      -1   \n",
      "27    Paul Ryan I think Donald Trump pull enormous p...       cbs       1   \n",
      "28    The 10-Point Gerard Baker World Economic Forum...       wsj       0   \n",
      "29    The school secretary wear orange jumpsuit Depa...  usatoday       0   \n",
      "...                                                 ...       ...     ...   \n",
      "1372                       Donald Trump one wish weight  usatoday       0   \n",
      "1373  Hillary Clinton win South Dakota Democratic pr...  usatoday       1   \n",
      "1374  BREAKING The FBI review additional email Hilla...  usatoday      -1   \n",
      "1375  President Obama 's lower court appointments co...  usatoday       0   \n",
      "1376  When Barack Obama win Ohio 2008 2012 support w...  usatoday      -1   \n",
      "1377  Eight years hard-fought primary battle Michell...  usatoday       1   \n",
      "1378  Hillary Clinton might rescue Republicans inevi...  usatoday      -1   \n",
      "1379  Sen Bernie Sanders lay groundwork post-electio...  usatoday       1   \n",
      "1380  Trump may peak early Or may begin something un...  usatoday       1   \n",
      "1381  During primaries one common attack Bernie Sand...  usatoday      -1   \n",
      "1382                              Trumpy biggest seller  usatoday       0   \n",
      "1383  Bernie Sanders could give Hillary Clinton long...  usatoday       1   \n",
      "1384  Trump blast Clinton since make deplorables com...  usatoday      -1   \n",
      "1385  Police remove Rose Hamid wear hijab stand sile...  usatoday       0   \n",
      "1386  Hillary Clinton may corrupt person ever seek p...  usatoday      -1   \n",
      "1387               I one word Donald Trump Basta Enough  usatoday      -1   \n",
      "1388  Donald J Trump longer commit sue women accuse ...  usatoday       0   \n",
      "1389  Trump say would nice Republican Party get behi...  usatoday       1   \n",
      "1390  Mike Pence offer full-throated endorsement Don...  usatoday       1   \n",
      "1391             The remark n't sit well Donald J Trump  usatoday      -1   \n",
      "1392        Donald J Trump lot say Tuesday 's DemDebate  usatoday       0   \n",
      "1393  Do favor Clinton tell New York City Mayor Bill...  usatoday       0   \n",
      "1394  The American people deserve get full complete ...  usatoday       0   \n",
      "1395  Donald J Trump call Hillary Clinton candidate ...  usatoday      -1   \n",
      "1396  The bill go Gov Mary Fallin whose name float p...  usatoday       0   \n",
      "1397  With smile face help poll worker far-off fanta...  usatoday       1   \n",
      "1398  Donald J Trump continue lead Republican field ...  usatoday       0   \n",
      "1399  Donald Trump tell 'The Wall Street Journal may...  usatoday       1   \n",
      "1400  Nothing make prouder champion say Hillary Clin...  usatoday       1   \n",
      "1401  The latest release include around 150 email cl...  usatoday       0   \n",
      "\n",
      "                            statusID candidates                dates     date  \\\n",
      "0       5550296508_10155606385896509      trump  2016-11-15 00:00:02  2016-11   \n",
      "1     131459315949_10153427911075950      trump  2016-03-03 14:44:04  2016-03   \n",
      "2     131459315949_10154056641215950    clinton  2016-11-09 00:03:03  2016-11   \n",
      "3      15704546335_10153385284896336      trump  2015-06-23 02:00:01  2015-06   \n",
      "4      13652355666_10153933280450667      trump  2016-10-13 21:42:41  2016-10   \n",
      "5     131459315949_10153397324575950    clinton  2016-02-17 07:39:27  2016-02   \n",
      "6      15704546335_10154088259801336      trump  2016-02-26 14:00:14  2016-02   \n",
      "7      13652355666_10153959863300667      trump  2016-10-20 22:41:48  2016-10   \n",
      "8       5550296508_10154648040171509      trump  2016-04-01 16:28:02  2016-04   \n",
      "9      15704546335_10154454708126336    clinton  2016-07-18 11:42:36  2016-07   \n",
      "10    131459315949_10153757406760950    clinton  2016-07-22 19:17:14  2016-07   \n",
      "11     13652355666_10153948842760667      trump  2016-10-17 22:00:00  2016-10   \n",
      "12      5550296508_10154315359966509      trump  2015-12-22 07:22:01  2015-12   \n",
      "13    131459315949_10153447192105950      trump  2016-03-11 22:40:47  2016-03   \n",
      "14    131459315949_10154071618380950      other  2016-11-13 19:15:54  2016-11   \n",
      "15     15704546335_10154668092296336    clinton  2016-10-04 21:56:11  2016-10   \n",
      "16     13652355666_10153932123850667      other  2016-10-13 16:00:00  2016-10   \n",
      "17    131459315949_10153292327635950    clinton  2015-12-21 16:28:32  2015-12   \n",
      "18     13652355666_10153924828260667    clinton  2016-10-11 11:43:00  2016-10   \n",
      "19     15704546335_10154713987721336      trump  2016-10-20 00:06:02  2016-10   \n",
      "20    131459315949_10153837579360950    clinton  2016-08-24 08:30:05  2016-08   \n",
      "21      8304333127_10154314158863128      trump  2016-05-18 16:40:02  2016-05   \n",
      "22      5550296508_10154898645306509      trump  2016-06-09 19:31:23  2016-06   \n",
      "23     13652355666_10154026471955667      other  2016-11-09 01:40:00  2016-11   \n",
      "24    131459315949_10154058345805950      trump  2016-11-09 11:28:20  2016-11   \n",
      "25     15704546335_10154103175901336      trump  2016-03-03 23:55:33  2016-03   \n",
      "26    131459315949_10153976233385950    clinton  2016-10-14 07:24:03  2016-10   \n",
      "27    131459315949_10154058369590950      trump  2016-11-09 11:35:20  2016-11   \n",
      "28      8304333127_10153976433233128      trump  2016-01-20 08:30:00  2016-01   \n",
      "29     13652355666_10154007889465667    clinton  2016-11-03 20:13:57  2016-11   \n",
      "...                              ...        ...                  ...      ...   \n",
      "1372   13652355666_10153846997160667      trump  2016-09-15 16:30:00  2016-09   \n",
      "1373   13652355666_10153585700530667    clinton  2016-06-07 22:58:39  2016-06   \n",
      "1374   13652355666_10153985134535667    clinton  2016-10-28 12:23:55  2016-10   \n",
      "1375   13652355666_10153844454430667      other  2016-09-14 17:30:00  2016-09   \n",
      "1376   13652355666_10154030994500667      trump  2016-11-09 23:24:43  2016-11   \n",
      "1377   13652355666_10153981680050667    clinton  2016-10-27 12:54:48  2016-10   \n",
      "1378   13652355666_10153690408580667      other  2016-07-23 13:10:00  2016-07   \n",
      "1379   13652355666_10153987891575667      other  2016-10-29 09:50:01  2016-10   \n",
      "1380   13652355666_10152975490600667      trump  2015-08-04 12:03:00  2015-08   \n",
      "1381   13652355666_10154027073720667    clinton  2016-11-09 03:20:00  2016-11   \n",
      "1382   13652355666_10153574967355667      trump  2016-06-05 13:29:00  2016-06   \n",
      "1383   13652355666_10153653526225667    clinton  2016-07-07 17:25:13  2016-07   \n",
      "1384   13652355666_10153920173020667      other  2016-10-09 21:33:45  2016-10   \n",
      "1385   13652355666_10153260556620667      trump  2016-01-10 07:33:00  2016-01   \n",
      "1386   13652355666_10153620528550667      other  2016-06-22 10:21:25  2016-06   \n",
      "1387   13652355666_10153584498190667      trump  2016-06-07 13:40:57  2016-06   \n",
      "1388   13652355666_10153981193040667      trump  2016-10-27 09:23:42  2016-10   \n",
      "1389   13652355666_10153613555685667      trump  2016-06-19 11:07:00  2016-06   \n",
      "1390   13652355666_10153922104885667      trump  2016-10-10 13:21:24  2016-10   \n",
      "1391   13652355666_10153831940125667      trump  2016-09-10 09:48:53  2016-09   \n",
      "1392   13652355666_10153109194405667      trump  2015-10-14 02:33:00  2015-10   \n",
      "1393   13652355666_10153466063975667    clinton  2016-04-10 15:15:00  2016-04   \n",
      "1394   13652355666_10153985983565667    clinton  2016-10-28 18:43:09  2016-10   \n",
      "1395   13652355666_10153781627400667      other  2016-08-24 22:40:00  2016-08   \n",
      "1396   13652355666_10153544883210667      trump  2016-05-19 23:11:00  2016-05   \n",
      "1397   13652355666_10153967236665667    clinton  2016-10-23 00:45:10  2016-10   \n",
      "1398   13652355666_10152991654495667      trump  2015-08-11 14:27:01  2015-08   \n",
      "1399    13652355666_1742117792779408      trump  2016-11-11 20:53:04  2016-11   \n",
      "1400    13652355666_1740277859630068    clinton  2016-11-09 12:57:37  2016-11   \n",
      "1401   13652355666_10153033465715667    clinton  2015-09-01 03:00:01  2015-09   \n",
      "\n",
      "     hour  \n",
      "0      00  \n",
      "1      14  \n",
      "2      00  \n",
      "3      02  \n",
      "4      21  \n",
      "5      07  \n",
      "6      14  \n",
      "7      22  \n",
      "8      16  \n",
      "9      11  \n",
      "10     19  \n",
      "11     22  \n",
      "12     07  \n",
      "13     22  \n",
      "14     19  \n",
      "15     21  \n",
      "16     16  \n",
      "17     16  \n",
      "18     11  \n",
      "19     00  \n",
      "20     08  \n",
      "21     16  \n",
      "22     19  \n",
      "23     01  \n",
      "24     11  \n",
      "25     23  \n",
      "26     07  \n",
      "27     11  \n",
      "28     08  \n",
      "29     20  \n",
      "...   ...  \n",
      "1372   16  \n",
      "1373   22  \n",
      "1374   12  \n",
      "1375   17  \n",
      "1376   23  \n",
      "1377   12  \n",
      "1378   13  \n",
      "1379   09  \n",
      "1380   12  \n",
      "1381   03  \n",
      "1382   13  \n",
      "1383   17  \n",
      "1384   21  \n",
      "1385   07  \n",
      "1386   10  \n",
      "1387   13  \n",
      "1388   09  \n",
      "1389   11  \n",
      "1390   13  \n",
      "1391   09  \n",
      "1392   02  \n",
      "1393   15  \n",
      "1394   18  \n",
      "1395   22  \n",
      "1396   23  \n",
      "1397   00  \n",
      "1398   14  \n",
      "1399   20  \n",
      "1400   12  \n",
      "1401   03  \n",
      "\n",
      "[1402 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Getting rid of too common words (e.g. the, an)\n",
    "\n",
    "#print stopwords.words(\"english\") \n",
    "\n",
    "new_index = range(0, len(sample.index))\n",
    "new_sample = pd.DataFrame(index=new_index, columns=(\"texts\", \"sources\", \"labels\"))\n",
    "\n",
    "i=0\n",
    "for index, row in sample.iterrows():\n",
    "    # If this new post is different post, then append what we worked so far\n",
    "    words = [w for w in row['texts'].split() if not w in stopwords.words(\"english\")]\n",
    "    new_sample['texts'].loc[i] = ' '.join(words)\n",
    "    i += 1\n",
    "\n",
    "sample['texts'] = new_sample['texts']\n",
    "print sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 95 0 0 300\t 0.2013\n",
      "5 95 0 4 300\t 0.2416\n",
      "5 95 0 5 12300\t 0.2685\n",
      "5 95 1 24 12300\n",
      "5 93 0 11 9300\t 0.2752\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-05030010c3d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m                         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_combined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_combined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[0;32m-> 1028\u001b[0;31m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1029\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1081\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[1;32m   1082\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m                                      X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m             residual = loss.negative_gradient(y, y_pred, k=k,\n\u001b[0;32m--> 763\u001b[0;31m                                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0;31m# induce regression tree on residuals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mnegative_gradient\u001b[0;34m(self, y, pred, k, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;34m\"\"\"Compute negative gradient for the ``k``-th class. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         return y - np.nan_to_num(np.exp(pred[:, k] -\n\u001b[0;32m--> 564\u001b[0;31m                                         logsumexp(pred, axis=1)))\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/utils/extmath.pyc\u001b[0m in \u001b[0;36mlogsumexp\u001b[0;34m(arr, axis)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mvmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for j in range(0,2): # stop_words: k=0 'engl' k=1 none\n",
    "\n",
    "cutoff = int(math.ceil(len(sample.index)*0.9)) # 90%\n",
    "train = pd.DataFrame({'labels':sample['labels'][:cutoff], \n",
    "                      'texts':sample['texts'][:cutoff], \n",
    "                      'sources':sample['sources'][:cutoff], \n",
    "                      'candidates':sample['candidates'][:cutoff], \n",
    "                      'date':sample['date'][:cutoff], \n",
    "                      'hour':sample['hour'][:cutoff]})\n",
    "test  = pd.DataFrame({'labels':sample['labels'][cutoff:], \n",
    "                      'texts':sample['texts'][cutoff:], \n",
    "                      'sources':sample['sources'][cutoff:], \n",
    "                      'candidates':sample['candidates'][cutoff:], \n",
    "                      'date':sample['date'][cutoff:], \n",
    "                      'hour':sample['hour'][cutoff:]})\n",
    "\n",
    "best_score = 0\n",
    "\n",
    "for h in range(5,18,2): # min_df from 0.05 to 0.30, adding 0.02 every time\n",
    "    for i in range(95,72,-2): # max_df from 0.95 to 0.70, -0.02 every time\n",
    "        for k in range(0,2): # k=0 CountVectorizer (count), k=1 TfidfVectorizer (weighed)\n",
    "            for l in range(0,25): \n",
    "                for m in range(300,13001,1000): # max_features (how many most frequent words should we consider)\n",
    "\n",
    "                    if k==0:\n",
    "                        tf_vectorizer = CountVectorizer(max_df=i/100.0, \n",
    "                                                        min_df=h/100.0, \n",
    "                                                        max_features = m)\n",
    "                    elif k==1:\n",
    "                        tf_vectorizer = TfidfVectorizer(max_df=i/100.0, \n",
    "                                                        min_df=h/100.0, \n",
    "                                                        max_features = m)\n",
    "\n",
    "                    train_text_tf_  = tf_vectorizer.fit_transform(train['texts'].values)\n",
    "                    test_text_tf_  = tf_vectorizer.transform(test['texts'].values)\n",
    "\n",
    "                    train_source_tf_ = tf_vectorizer.fit_transform(train['sources'].values)\n",
    "                    test_source_tf_ = tf_vectorizer.transform(test['sources'].values)\n",
    "\n",
    "                    train_candidate = tf_vectorizer.fit_transform(train['candidates'].values)\n",
    "                    test_candidate = tf_vectorizer.transform(test['candidates'].values)\n",
    "\n",
    "                    train_date = tf_vectorizer.fit_transform(train['date'].values)\n",
    "                    test_date = tf_vectorizer.transform(test['date'].values)\n",
    "                    \n",
    "                    train_hour = tf_vectorizer.fit_transform(train['hour'].values)\n",
    "                    test_hour = tf_vectorizer.transform(test['hour'].values)\n",
    "                    ####\n",
    "                    train_combined = sp.hstack([train_text_tf_, train_source_tf_], format='csr')\n",
    "                    test_combined = sp.hstack([test_text_tf_, test_source_tf_], format='csr')\n",
    "\n",
    "                    train_combined = sp.hstack([train_combined, train_candidate], format='csr')\n",
    "                    test_combined = sp.hstack([test_combined, test_candidate], format='csr')\n",
    "\n",
    "                    train_combined = sp.hstack([train_combined, train_date], format='csr')\n",
    "                    test_combined = sp.hstack([test_combined, test_date], format='csr')\n",
    "\n",
    "                    train_combined = sp.hstack([train_combined, train_hour], format='csr')\n",
    "                    test_combined = sp.hstack([test_combined, test_hour], format='csr')\n",
    "\n",
    "                    # CHANGE ARGS TOO\n",
    "                    if l==0:\n",
    "                        clf = MultinomialNB()\n",
    "                    elif l==1:\n",
    "                        clf = GaussianNB()\n",
    "                    elif l==2:\n",
    "                        clf = BernoulliNB()\n",
    "                    elif l==3:\n",
    "                        clf = LogisticRegression(random_state=1)\n",
    "                    elif l==4:\n",
    "                        clf = LogisticRegressionCV()\n",
    "                    elif l==5:\n",
    "                        clf = SGDClassifier()\n",
    "                    elif l==6:\n",
    "                        clf = Ridge()\n",
    "                    elif l==7:\n",
    "                        clf = RidgeClassifier()\n",
    "                    elif l==8:\n",
    "                        clf = RidgeClassifierCV()\n",
    "                    elif l==9:\n",
    "                        clf = ElasticNet()\n",
    "                    elif l==10:\n",
    "                        clf = LassoLars()\n",
    "                    elif l==11:\n",
    "                        clf = PassiveAggressiveClassifier()\n",
    "                    elif l==12:\n",
    "                        clf = SVC(random_state=1)\n",
    "                    elif l==13:\n",
    "                        clf = LinearSVC()\n",
    "                    elif l==14:\n",
    "                        clf = KNeighborsClassifier(n_neighbors=9)\n",
    "                    elif l==15:\n",
    "                        clf = NearestCentroid()\n",
    "                    elif l==16:\n",
    "                        clf = GradientBoostingClassifier()\n",
    "                    elif l==17:\n",
    "                        clf = BaggingClassifier(random_state=1)\n",
    "                    elif l==18:\n",
    "                        clf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "                    elif l==19:\n",
    "                        clf = AdaBoostClassifier()\n",
    "                    elif l==20:\n",
    "                        clf = ExtraTreesClassifier()\n",
    "                    elif l==21:\n",
    "                        clf = GradientBoostingClassifier()\n",
    "                    elif l==22:\n",
    "                        clf = VotingClassifier(estimators=\n",
    "                                [('l01', MultinomialNB()),\n",
    "                                ('l02', GaussianNB()),\n",
    "                                ('l03', BernoulliNB()),\n",
    "                                ('l04', LogisticRegression(random_state=1)),\n",
    "                                ('l13', SVC(random_state=1)),\n",
    "                                ('l15', KNeighborsClassifier(n_neighbors=9)),\n",
    "                                ('l18', BaggingClassifier(random_state=1)),\n",
    "                                ('l19', RandomForestClassifier(n_estimators=100, random_state=1)),\n",
    "                                ('l24', DecisionTreeClassifier(random_state=1))])\n",
    "                    elif l==23:\n",
    "                        clf = ExtraTreeClassifier()\n",
    "                    elif l==24:\n",
    "                        clf = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "                    clf.fit(train_combined.toarray(), train['labels'])\n",
    "                    score = clf.score(test_combined.toarray(), test['labels'])\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        print \"%d %d %d %d %d\\t %.4f\" % (h,i,k,l,m,score)\n",
    "        print \"%d %d %d %d %d\" % (h,i,k,l,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 20,000+ possibilities and running 8000+ cases, <br>\n",
    "It never hit 0.7.. ->> For train:test = 1:1 size\n",
    "\n",
    "Train:test = 10:1 size. I do get up to 0.8+ <br>\n",
    "Now we also have up to (more reasonable) 14400 cases (all expected to run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning w/ TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  18%|█▊        | 21/120 [00:15<01:01,  1.60pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.567128522453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  35%|███▌      | 42/120 [00:21<00:11,  6.74pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.567128522453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  52%|█████▏    | 62/120 [01:07<03:11,  3.30s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.567128522453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  68%|██████▊   | 82/120 [01:49<00:22,  1.72pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.571469654091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  84%|████████▍ | 101/120 [01:53<00:02,  8.83pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 0.571469654091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: BernoulliNB(Normalizer(input_matrix, 26), 0.040000000000000001, 0.33000000000000002)\n",
      "0.549321012559\n",
      "1402 5 80 0 50 5 20 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  18%|█▊        | 21/120 [01:02<00:59,  1.68pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.567894748329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  34%|███▍      | 41/120 [01:15<01:16,  1.04pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.567894748329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  49%|████▉     | 59/120 [01:30<00:48,  1.25pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.567894748329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  68%|██████▊   | 81/120 [01:40<00:06,  6.04pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.567894748329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  84%|████████▍ | 101/120 [01:50<00:08,  2.28pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 0.567894748329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: DecisionTreeClassifier(input_matrix)\n",
      "0.505894778536\n",
      "1402 5 80 0 50 5 20 11\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GP closed prematurely - will use current best pipeline\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A pipeline has not yet been optimized. Please call fit() first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-0a2a4b563941>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m                                                   verbosity=2)\n\u001b[1;32m     44\u001b[0m                             \u001b[0mtpot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"%d %d %d %d %d %d %d %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/tpot/base.pyc\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, testing_features, testing_classes)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \"\"\"\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fitted_pipeline\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             raise ValueError('A pipeline has not yet been optimized. '\n\u001b[0m\u001b[1;32m    442\u001b[0m                              'Please call fit() first.')\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: A pipeline has not yet been optimized. Please call fit() first."
     ]
    }
   ],
   "source": [
    "#for i in range(0,2): # 'Negative' from sample = -1 or 0\n",
    "#  if i==0:\n",
    "#      sample['Answer.sentiment'] = sample['Answer.sentiment'].map({'Positive':1, 'Negative':-1})\n",
    "#  elif i==1:\n",
    "#      sample['Answer.sentiment'] = sample['Answer.sentiment'].map({'Positive':1, 'Negative':0})\n",
    "\n",
    "for j in range(5,21,2): # min_df\n",
    "    for k in range(80,101,2): # max_df\n",
    "        for l in range(0,2): # l=0 CountVectorizer (count), l=1 TfidfVectorizer (weighed)\n",
    "            for m in range(50,101,10): # train:test = m/100 : (1-m/100) so 50:50 to 90:10\n",
    "                for n in range(5,7): # generation (# of TPOT iteration)\n",
    "                    for p in range(20,26,5): # pop_size p \n",
    "                        for q in range(10,12): # k-fold number\n",
    "                            print \"\"    \n",
    "\n",
    "                            #sample = pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/sample_data_results.csv') \n",
    "\n",
    "                            # label ('positive','Negative') ->> (#,#) (e.g. (1,-1) or (1,0))\n",
    "                            #sample['Answer.sentiment'] = sample['Answer.sentiment'].map({'Positive':1, 'Neutral':0, 'Negative':-1})\n",
    "\n",
    "                            if l==0:\n",
    "                                tf_vectorizer = CountVectorizer(min_df=j/100.0, max_df=k/100.0, max_features=5000)\n",
    "                            elif l==1:\n",
    "                                tf_vectorizer = TfidfVectorizer(min_df=j/100.0, max_df=k/100.0, max_features=5000)\n",
    "\n",
    "                            #sample_input_tf  = tf_vectorizer.fit_transform(sample['Input.content'].values)\n",
    "                            text_tf_  = tf_vectorizer.fit_transform(sample['texts'].values)\n",
    "                            source_tf_ = tf_vectorizer.fit_transform(sample['sources'].values)\n",
    "                            candidates_tf_ = tf_vectorizer.fit_transform(sample['candidates'].values)\n",
    "                            date_tf_ = tf_vectorizer.fit_transform(sample['date'].values)\n",
    "                            hour_tf_ = tf_vectorizer.fit_transform(sample['hour'].values)\n",
    "\n",
    "                            combined = sp.hstack([text_tf_, source_tf_], format='csr')\n",
    "                            combined = sp.hstack([combined, candidates_tf_], format='csr')\n",
    "                            combined = sp.hstack([combined, date_tf_], format='csr')\n",
    "                            combined = sp.hstack([combined, hour_tf_], format='csr')\n",
    "\n",
    "                            X_train, X_test, y_train, y_test = train_test_split(combined, sample['labels'].values,\n",
    "                                                                train_size=m/100.0, test_size=(1-m/100.0))#, random_state=)\n",
    "\n",
    "                            # Official website example: gen=5, pop_size=20, verbo=2\n",
    "                            tpot = TPOTClassifier(generations=n, population_size=p, num_cv_folds=q,\n",
    "                                                  verbosity=2)\n",
    "                            tpot.fit(X_train, y_train)\n",
    "                            print(tpot.score(X_test, y_test))\n",
    "                            print \"%d %d %d %d %d %d %d %d\" % (i,j,k,l,m,n,p,q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer, Pos=1 Neg=-1, train:test=75:25, size=872 in TPOT = 0.78 <br>\n",
    "CountVectorizer, Pos=1 Neg=-1, train:test=92:08, size=872 in TPOT = 0.78 <br>\n",
    "CountVectorizer, Pos=1 Neg =0, train:test=92:08, size=872 in TPOT = 0.86 <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "matter lives series black super city moon mckinnon photo parks landing photoshopped tweeted kate kill coalition spray concluded blasts camera\n",
      "Topic #1:\n",
      "carson ben suggesting damage rise dr faced backlash caused later khan health trump care hero views polls insults young rhetoric\n",
      "Topic #2:\n",
      "clinton hillary trump donald today eye opener matters said campaign world 90 seconds benghazi face emails presidential debate gave lawmakers\n",
      "Topic #3:\n",
      "trump donald cbsn clinton ws http hillary said president news cbs new says watch republican just gop campaign obama election\n",
      "Topic #4:\n",
      "percent cruz ted quickly twitter compared pope internet election wife users support francis black voting actor ceo economic 42 bowl\n",
      "Topic #5:\n",
      "sanders bernie demdebate mitt romney caucuses malley heated president conway kellyanne cause iowa debate used called michigan asked wage strong\n",
      "Topic #6:\n",
      "ready rights human play tear tactics apart false chief comeback columbia civil communities way harlem loser fully passionate enthusiastic attempts\n",
      "Topic #7:\n",
      "mission muslims election officials mcdonald trump national entering ban shut thought ivanka coming said benghazi police man presidential season donald\n",
      "Topic #8:\n",
      "team sarah shooting palin transition mark insists warning carrying streets trump cuban boycott mass rahm laquanmcdonald christmas stop asking worse\n",
      "Topic #9:\n",
      "tax returns assault warned audit sexual planned parenthood releasing note tack following involve irs rare cycle led turned flooded hundreds\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sample = pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/Facebook_RAW/CBSNews_facebook_statuses.csv') \n",
    "sample.append(pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/Facebook_RAW/cnn_facebook_statuses.csv'))\n",
    "sample.append(pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/Facebook_RAW/facebook_sample_data.txt'))\n",
    "sample.append(pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/Facebook_RAW/FoxNews_facebook_statuses.csv'))\n",
    "sample.append(pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/Facebook_RAW/msnbc_facebook_statuses.csv'))\n",
    "sample.append(pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/Facebook_RAW/nytimes_facebook_statuses.csv'))\n",
    "sample.append(pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/Facebook_RAW/usatoday_facebook_statuses.csv'))\n",
    "sample.append(pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/Facebook_RAW/wsj_facebook_statuses.csv'))\n",
    "\n",
    "#print sample.head()\n",
    "\n",
    "cutoff = int(math.ceil(len(sample.index)*0.92)) # 92%\n",
    "train = pd.DataFrame({'texts':sample['status_message'][:cutoff]})\n",
    "test  = pd.DataFrame({'texts':sample['status_message'][cutoff:]})\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, analyzer = \"word\",\n",
    "                                   #max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(train['texts'].values)\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, analyzer = \"word\",\n",
    "                                #max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(train['texts'].values)\n",
    "\n",
    "lda = LatentDirichletAllocation(#n_topics=n_topics, \n",
    "                                max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "lda.fit(tf)\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble & Bagging (Bootstrap AGgregating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nope... http://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So text extraction + ..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task 1: Load the texts\n",
    "import pandas as pd\n",
    "import glob, os         # for reading all .txt files\n",
    "import csv\n",
    "import numpy as np\n",
    "from textblob import TextBlob # use kernel Python[Root]\n",
    "\n",
    "cutoff = 436\n",
    "\n",
    "# Read sample texts\n",
    "sample = pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/sample_data_results.csv') \n",
    "\n",
    "# Lemmatize\n",
    "new_sample = pd.DataFrame(columns=(\"Input.content\", \"Answer.sentiment\"))\n",
    "i=0\n",
    "for text in sample['Input.content']:\n",
    "    blob = TextBlob(text)\n",
    "    newtexts = \"\"\n",
    "\n",
    "    for sentence in blob.sentences:\n",
    "        newtext = \"\"\n",
    "#        print sentence.dict\n",
    "\n",
    "        for word in sentence.words:\n",
    "            newtext += \" \" + word.lemmatize('v') # 'v' for 'verb'\n",
    "\n",
    "        newtexts += newtext\n",
    "        new_sample['texts'].loc[i] = newtexts\n",
    "        i += 1\n",
    "\n",
    "i=0\n",
    "for answer in sample['Answer.sentiment']: # updating answers\n",
    "    new_sample['Answer.sentiment'].loc[i] = answer\n",
    "    i += 1\n",
    "    \n",
    "sample = new_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.DataFrame({'label':sample['Answer.sentiment'][:cutoff], 'texts':sample['Input.content'][:cutoff]})\n",
    "test  = pd.DataFrame({'label':sample['Answer.sentiment'][cutoff:], 'texts':sample['Input.content'][cutoff:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 6 0 0 1 - 0.7778\n",
      "1 6 0 0 2 - 0.6389\n",
      "1 6 0 1 1 - 0.7778\n",
      "1 6 0 1 2 - 0.6389\n",
      "1 6 1 0 1 - 0.7778\n",
      "1 6 1 0 2 - 0.6389\n",
      "1 6 1 1 1 - 0.7222\n",
      "1 6 1 1 2 - 0.6944\n",
      "1 7 0 0 1 - 0.8056\n",
      "1 7 0 0 2 - 0.6389\n",
      "1 7 0 1 1 - 0.8056\n",
      "1 7 0 1 2 - 0.6389\n",
      "1 7 1 0 1 - 0.8056\n",
      "1 7 1 0 2 - 0.6389\n",
      "1 7 1 1 1 - 0.7222\n",
      "1 7 1 1 2 - 0.6389\n",
      "1 8 0 0 1 - 0.8056\n",
      "1 8 0 0 2 - 0.6389\n",
      "1 8 0 1 1 - 0.8056\n",
      "1 8 0 1 2 - 0.6389\n",
      "1 8 1 0 1 - 0.8056\n",
      "1 8 1 0 2 - 0.6389\n",
      "1 8 1 1 1 - 0.7222\n",
      "1 8 1 1 2 - 0.6389\n",
      "1 9 0 0 1 - 0.8056\n",
      "1 9 0 0 2 - 0.5833\n",
      "1 9 0 1 1 - 0.8056\n",
      "1 9 0 1 2 - 0.5833\n",
      "1 9 1 0 1 - 0.8056\n",
      "1 9 1 0 2 - 0.5833\n",
      "1 9 1 1 1 - 0.7222\n",
      "1 9 1 1 2 - 0.6389\n",
      "1 10 0 0 1 - 0.8056\n",
      "1 10 0 0 2 - 0.6389\n",
      "1 10 0 1 1 - 0.8056\n",
      "1 10 0 1 2 - 0.6389\n",
      "1 10 1 0 1 - 0.8056\n",
      "1 10 1 0 2 - 0.6389\n",
      "1 10 1 1 1 - 0.7222\n",
      "1 10 1 1 2 - 0.6944\n",
      "1 11 0 0 1 - 0.8056\n",
      "1 11 0 0 2 - 0.6389\n",
      "1 11 0 1 1 - 0.8056\n",
      "1 11 0 1 2 - 0.6389\n",
      "1 11 1 0 1 - 0.8056\n",
      "1 11 1 0 2 - 0.6389\n",
      "1 11 1 1 1 - 0.7222\n",
      "1 11 1 1 2 - 0.6944\n",
      "1 12 0 0 1 - 0.8056\n",
      "1 12 0 0 2 - 0.6389\n",
      "1 12 0 1 1 - 0.8056\n",
      "1 12 0 1 2 - 0.6389\n",
      "1 12 1 0 1 - 0.8056\n",
      "1 12 1 0 2 - 0.6389\n",
      "1 12 1 1 1 - 0.7222\n",
      "1 12 1 1 2 - 0.6944\n",
      "1 13 0 0 1 - 0.8056\n",
      "1 13 0 0 2 - 0.6389\n",
      "1 13 0 1 1 - 0.8056\n",
      "1 13 0 1 2 - 0.6389\n",
      "1 13 1 0 1 - 0.8056\n",
      "1 13 1 0 2 - 0.6389\n",
      "1 13 1 1 1 - 0.7222\n",
      "1 13 1 1 2 - 0.6944\n",
      "1 14 0 0 1 - 0.8056\n",
      "1 14 0 0 2 - 0.6389\n",
      "1 14 0 1 1 - 0.8056\n",
      "1 14 0 1 2 - 0.6389\n",
      "1 14 1 0 1 - 0.8056\n",
      "1 14 1 0 2 - 0.6389\n",
      "1 14 1 1 1 - 0.7222\n",
      "1 14 1 1 2 - 0.6944\n",
      "1 15 0 0 1 - 0.8056\n",
      "1 15 0 0 2 - 0.5833\n",
      "1 15 0 1 1 - 0.8056\n",
      "1 15 0 1 2 - 0.5833\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e18b73c0799f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                         \u001b[0mtf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                     \u001b[0mtrain_tf_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'texts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                     \u001b[0mtest_tf_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'texts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 839\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 241\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mtoken_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtoken_pattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Task 4: feature engineering\n",
    "for h in range(1,30): # min_df\n",
    "    for i in range(h+5,100): # max_df\n",
    "        for j in range(0,2): # stop_words: k=0 'engl' k=1 none\n",
    "            for k in range(0,2): # k=0 CountVectorizer (count), k=1 TfidfVectorizer (weighed)\n",
    "                for l in range(0,3): # l=0 MultinomialNB, l=1 GaussianNB, l=2 BernoulliNB              \n",
    "\n",
    "                    if j==0 & k==0:\n",
    "                        tf_vectorizer = CountVectorizer(max_df=i/100.0, min_df=h/100.0, stop_words='english', analyzer = \"word\")\n",
    "                    elif j==1 & k==0:\n",
    "                        tf_vectorizer = CountVectorizer(max_df=i/100.0, min_df=h/100.0, analyzer = \"word\")\n",
    "                    elif j==0 & k==1:\n",
    "                        tf_vectorizer = TfidfVectorizer(max_df=i/100.0, min_df=h/100.0, stop_words='english', analyzer = \"word\")\n",
    "                    elif j==1 & k==1:\n",
    "                        tf_vectorizer = TfidfVectorizer(max_df=i/100.0, min_df=h/100.0, analyzer = \"word\")\n",
    "\n",
    "                    train_tf_ = tf_vectorizer.fit_transform(train['texts'].values)\n",
    "                    test_tf_  = tf_vectorizer.transform(test['texts'].values)\n",
    "\n",
    "                    if l==0:\n",
    "                        clf = MultinomialNB()\n",
    "                    elif l==1:\n",
    "                        clf = GaussianNB()\n",
    "                    elif l==2:\n",
    "                        clf = BernoulliNB()\n",
    "\n",
    "                    if l==0 | l==2:\n",
    "                        clf.fit(train_tf_, train['label'])\n",
    "                        print \"%d %d %d %d %d - %.4f\" % (h,i,j,k,l,clf.score(test_tf_, test['label']))\n",
    "                    elif l==1:\n",
    "                        clf.fit(train_tf_.toarray(), train['label'])\n",
    "                        print \"%d %d %d %d %d - %.4f\" % (h,i,j,k,l,clf.score(test_tf_.toarray(), test['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
