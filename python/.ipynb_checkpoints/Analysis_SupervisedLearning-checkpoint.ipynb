{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-cd983f85b09e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# run json-to-pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Part-of-speech Tagging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfT' is not defined"
     ]
    }
   ],
   "source": [
    "# https://textblob.readthedocs.io/en/dev/\n",
    "\n",
    "# How to install TextBlob\n",
    "#     1. pip install -U textblob\n",
    "#     2. python -m textblob.download_corpora\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(dfT['text'][0]) # run json-to-pandas\n",
    "\n",
    "# Part-of-speech Tagging\n",
    "print blob.tags\n",
    "print\n",
    "\n",
    "# Noun Phrase Extraction¶\n",
    "print blob.noun_phrases\n",
    "print \n",
    "\n",
    "# Tokenization\n",
    "print blob.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0166666666667\n",
      "\n",
      "One---One\n",
      "of---of\n",
      "China---China\n",
      "'s---'s\n",
      "first---first\n",
      "female---female\n",
      "fighter---fighter\n",
      "pilots---pilot\n",
      "was---be\n",
      "killed---kill\n",
      "in---in\n",
      "a---a\n",
      "training---train\n",
      "accident---accident\n",
      "according---accord\n",
      "to---to\n",
      "state-run---state-run\n",
      "media---media\n",
      "reports…---reports…\n",
      "https---https\n",
      "t.co/DoEZLme8Cq---t.co/DoEZLme8Cq\n"
     ]
    }
   ],
   "source": [
    "# The subjectivity is a float within the range [0.0, 1.0] \n",
    "# where 0.0 is very objective and 1.0 is very subjective\n",
    "for sentence in blob.sentences:\n",
    "    print sentence.sentiment.polarity\n",
    "print\n",
    "\n",
    "# Lemmatize each word\n",
    "for sentence in blob.sentences:    \n",
    "    for word in sentence.words:\n",
    "        print \"%s---%s\" % (word, word.lemmatize('v')) # 'v' for 'verb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.nltk.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank # to draw a parse tree\n",
    "\n",
    "sentence = dfT['text'][0] # run json-to-pandas\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "# Identify named entities - Make parse tree?\n",
    "# You might need to call nltk.download() and down load some packages\n",
    "entities = nltk.chunk.ne_chunk(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.728\n",
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.7 : 1.0\n",
      "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
      "              astounding = True              pos : neg    =     10.3 : 1.0\n",
      "             fascination = True              pos : neg    =     10.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Example from http://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    " \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    " \n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    " \n",
    "negcutoff = len(negfeats)*3/4\n",
    "poscutoff = len(posfeats)*3/4\n",
    " \n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Supervised Learning w/ manual scripting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task 1: Load the texts\n",
    "import pandas as pd\n",
    "import glob, os         # for reading all .txt files\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.base import ClassifierMixin\n",
    "#from sklearn.dummy import DummyClassifier\n",
    "#from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "#from sklearn.multiclass import OneVsRestClassifier\n",
    "#from sklearn.multiclass import OneVsOneClassifier\n",
    "#from sklearn.multiclass import OutputCodeClassifier\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "#from sklearn.calibration import CalibratedClassifierCV\n",
    "#from sklearn.semi_supervised import LabelPropagation\n",
    "import math\n",
    "import scipy.sparse as sp\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  texts   sources  labels\n",
      "0     It is unusual for a president's children to be...       cbs       1\n",
      "1     Just hours after Mitt Romney blasted Donald J....       cbs      -1\n",
      "2     Hillary Clinton wins the state of Washington, ...       fox       1\n",
      "3     \"We have to take the jobs away from other coun...  usatoday      -1\n",
      "4     Jessica Leeds, 74, of Manhattan, was one of tw...       cbs      -1\n",
      "5     Ash Carter warns Russia its policy in Syria wi...       cbs      -1\n",
      "6     \"It's Rubio!\" Watch Donald J. Trump use a wate...  usatoday       0\n",
      "7     Online, some Trump supporters discuss monitori...       cnn      -1\n",
      "8     This mogul once said he's a member of \"the luc...       fox       1\n",
      "9     Live: Hillary Clinton speaks at the NAACP Conv...       cbs       0\n",
      "10    BREAKING: Hillary Clinton has chosen Virginia ...  usatoday       0\n",
      "11    The billboard translated reads: \"Donald Trump,...       cnn      -1\n",
      "12    How weird was the world of politics in 2015? W...       cbs      -1\n",
      "13    NEW: A Chicago police spokesperson told CBS Ne...       cbs       0\n",
      "14    Hillary Clinton \"did some bad things,\" Donald ...       fox      -1\n",
      "15    Tim Kaine defended the work of the Clinton Fou...  usatoday       1\n",
      "16    Just before Clinton and Trump squared off in w...       cbs       0\n",
      "17    Chelsea took to Twitter to announce her second...  usatoday       1\n",
      "18    One adviser described Clinton as obsessed with...       fox      -1\n",
      "19    \"If you look at it as an American, as a human,...       cbs      -1\n",
      "20    A new report shows donors to the Clinton Found...       wsj      -1\n",
      "21    Donald J. Trump's  list includes six federal a...       cnn       0\n",
      "22    Sen. Elizabeth Warren launched a blistering at...  usatoday      -1\n",
      "23    Former KKK grand wizard David Duke didn't win ...       cbs       0\n",
      "24    \"We have to work to heal the divisions of a lo...       fox       1\n",
      "25    For 40 years, Donald J. Trump has been part of...       cbs      -1\n",
      "26    Michelle Obama, President Obama and Joe Biden ...       cbs      -1\n",
      "27    Paul Ryan: \"I think what Donald Trump just pul...       wsj       1\n",
      "28    The 10-Point: Gerard Baker on the World Econom...  usatoday       0\n",
      "29    The school secretary wore an orange jumpsuit a...       cnn       0\n",
      "...                                                 ...       ...     ...\n",
      "1372   Donald Trump has this one wish about his weight.  usatoday       0\n",
      "1373  Hillary Clinton wins the South Dakota Democrat...  usatoday       1\n",
      "1374  BREAKING: The FBI will review additional email...  usatoday      -1\n",
      "1375  President Obama's lower court appointments cou...  usatoday       0\n",
      "1376  When Barack Obama won Ohio in 2008 and 2012, h...  usatoday      -1\n",
      "1377  Eight years after a hard-fought primary battle...  usatoday       1\n",
      "1378  Hillary Clinton might rescue Republicans from ...  usatoday      -1\n",
      "1379  Sen. Bernie Sanders is laying the groundwork f...  usatoday       1\n",
      "1380  \"Trump may be peaking early. Or it may be the ...  usatoday       1\n",
      "1381  During the primaries, one of the most common a...  usatoday      -1\n",
      "1382                    \"Trumpy\" is her biggest seller.  usatoday       0\n",
      "1383  Bernie Sanders could give Hillary Clinton that...  usatoday       1\n",
      "1384  Trump has blasted Clinton since she made the \"...  usatoday      -1\n",
      "1385  Police removed Rose Hamid, who was wearing a h...  usatoday       0\n",
      "1386  \"Hillary Clinton may be the most corrupt perso...  usatoday      -1\n",
      "1387  \"I have just one word for Donald Trump: Basta!...  usatoday      -1\n",
      "1388  Donald J. Trump is no longer committed to suin...  usatoday       0\n",
      "1389  Trump says \"it would be nice\" if the Republica...  usatoday       1\n",
      "1390  Mike Pence offered a full-throated endorsement...  usatoday       1\n",
      "1391  The remarks didn't sit well with Donald J. Trump.  usatoday      -1\n",
      "1392  Donald J. Trump had a lot to say about Tuesday...  usatoday       0\n",
      "1393  Do me a favor, Clinton told New York City Mayo...  usatoday       0\n",
      "1394  \"The American people deserve to get the full a...  usatoday       0\n",
      "1395  Donald J. Trump called Hillary Clinton a candi...  usatoday      -1\n",
      "1396  The bill now goes to Gov. Mary Fallin, whose n...  usatoday       0\n",
      "1397  With a smile on her face  and some help from a...  usatoday       1\n",
      "1398  Donald J. Trump continues to lead the Republic...  usatoday       0\n",
      "1399  Donald Trump told 'The Wall Street Journal' th...  usatoday       1\n",
      "1400  Nothing has made me prouder than to be your ch...  usatoday       1\n",
      "1401  The latest release includes \"around 150\" email...  usatoday       0\n",
      "\n",
      "[1402 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# THREE ANSWERS TO ONE\n",
    "sample = pd.read_csv('/Users/Haru/Downloads/489Project/Turk/6source_results_filtered.csv') \n",
    "#sample['Answer.sentiment'] = sample['Answer.sentiment'].map({'Positive':1, 'Neutral':0, 'Negative':-1})\n",
    "\n",
    "new_index = range(0, len(sample.index)/3)\n",
    "new_sample = pd.DataFrame(index=new_index, columns=['texts', 'sources', 'labels'])\n",
    "\n",
    "text = \"\"\n",
    "#acc_value = 0 # accumulated score per post\n",
    "#num_posts = 0 # number of same text posts\n",
    "pos = 0 # number of 'positive' per post\n",
    "neu = 0 # number of 'neutral' per post\n",
    "neg = 0 # number of 'negative' per post\n",
    "pd_index = 0\n",
    "\n",
    "for index, row in sample.iterrows():\n",
    "    # If this new post is different post, then append what we worked so far\n",
    "    if text != row['Input.content']:\n",
    "        if text != \"\":\n",
    "            if pos >= 2:\n",
    "                new_sample.loc[[pd_index], ['labels']] = 1\n",
    "            elif neu >= 2: # could be commented out for pos/neg\n",
    "                new_sample.loc[[pd_index], ['labels']] = 0\n",
    "            elif neg >= 2:\n",
    "                new_sample.loc[[pd_index], ['labels']] = -1\n",
    "            else: # 1:1:1\n",
    "                #continue # discard\n",
    "                pd_index -= 1 # to keep the same index\n",
    "\n",
    "            #new_sample.loc[[pd_index], ['score']] = acc_value*1.0/num_posts\n",
    "            new_sample.loc[[pd_index], ['sources']] = row['Input.source']\n",
    "            new_sample.loc[[pd_index], ['texts']] = text\n",
    "            pd_index += 1\n",
    "            \n",
    "        # Assign a new post\n",
    "        text = row['Input.content']\n",
    "        #acc_value = 0\n",
    "        #num_posts = 0\n",
    "        pos = 0\n",
    "        neu = 0\n",
    "        neg = 0\n",
    "        \n",
    "    if row['Answer.sentiment'] == \"Positive\":\n",
    "        #acc_value += 1\n",
    "        #num_posts += 1\n",
    "        pos += 1\n",
    "    elif row['Answer.sentiment'] == \"Neutral\":\n",
    "        #num_posts += 1\n",
    "        neu += 1\n",
    "    elif row['Answer.sentiment'] == \"Negative\":\n",
    "        #acc_value -= 1\n",
    "        #num_posts += 1\n",
    "        neg += 1\n",
    "\n",
    "# not perfect, so we have some extra NaN rows (all should be filled, technically)\n",
    "new_sample = new_sample.dropna()\n",
    "        \n",
    "# change 0.333, 0.666, ... to whole number by x 3\n",
    "#new_sample.loc[:, 'score'] *= 3\n",
    "new_sample['labels'] = new_sample['labels'].astype('int')\n",
    "    \n",
    "#new_sample['sources'] = new_sample['sources'].map({'msnbc':4, \n",
    "#                                                   'cnn':3, \n",
    "#                                                   'cbs':2,\n",
    "#                                                   'usatoday':1,\n",
    "#                                                   'wsj':-1,\n",
    "#                                                   'fox':-2})\n",
    "#new_sample['sources'] = new_sample['sources'].astype('int')\n",
    "\n",
    "sample = new_sample\n",
    "\n",
    "#cutoff = int(math.ceil(len(sample.index)*0.9)) # 90%\n",
    "#train = pd.DataFrame({'labels':sample['labels'][:cutoff], 'texts':sample['texts'][:cutoff], 'sources':sample['sources'][:cutoff]})\n",
    "#test  = pd.DataFrame({'labels':sample['labels'][cutoff:], 'texts':sample['texts'][cutoff:], 'sources':sample['sources'][cutoff:]})\n",
    "#print train.shape\n",
    "#print test.shape\n",
    "print sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               texts   sources  labels  \\\n",
      "0  It is unusual for a president's children to be...       cbs       1   \n",
      "1  Just hours after Mitt Romney blasted Donald J....       cbs      -1   \n",
      "2  Hillary Clinton wins the state of Washington, ...       fox       1   \n",
      "3  \"We have to take the jobs away from other coun...  usatoday      -1   \n",
      "4  Jessica Leeds, 74, of Manhattan, was one of tw...       cbs      -1   \n",
      "\n",
      "  candidates  \n",
      "0      trump  \n",
      "1      trump  \n",
      "2    clinton  \n",
      "3      trump  \n",
      "4      trump  \n"
     ]
    }
   ],
   "source": [
    "# classifying candidates\n",
    "def classify_post (row):\n",
    "   status = row['texts'].lower()\n",
    "   if (('donald' in status) or ('trump' in status)) and (('hillary' in status) or ('clinton' in status)) :\n",
    "      return 'other'\n",
    "   elif ('donald' in status) or ('trump' in status) :\n",
    "      return 'trump'\n",
    "   elif ('hillary' in status) or ('clinton' in status) :\n",
    "      return 'clinton'\n",
    "   return 'other'\n",
    "\n",
    "sample['candidates'] = sample.apply (lambda row: classify_post (row),axis=1)\n",
    "\n",
    "cutoff = int(math.ceil(len(sample.index)*0.9)) # 90%\n",
    "train = pd.DataFrame({'labels':sample['labels'][:cutoff], \n",
    "                      'texts':sample['texts'][:cutoff], \n",
    "                      'sources':sample['sources'][:cutoff], \n",
    "                      'candidates':sample['candidates'][:cutoff]})\n",
    "test  = pd.DataFrame({'labels':sample['labels'][cutoff:], \n",
    "                      'texts':sample['texts'][cutoff:], \n",
    "                      'sources':sample['sources'][cutoff:], \n",
    "                      'candidates':sample['candidates'][cutoff:]})\n",
    "\n",
    "print sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  texts   sources  labels  \\\n",
      "0      It be unusual for a president 's children to ...       cbs       1   \n",
      "1      Just hours after Mitt Romney blast Donald J T...       cbs      -1   \n",
      "2      Hillary Clinton win the state of Washington C...       fox       1   \n",
      "3      We have to take the job away from other count...  usatoday      -1   \n",
      "4      Jessica Leeds 74 of Manhattan be one of two w...       cbs      -1   \n",
      "5      Ash Carter warn Russia its policy in Syria wi...       cbs      -1   \n",
      "6      It 's Rubio Watch Donald J Trump use a water ...  usatoday       0   \n",
      "7      Online some Trump supporters discuss monitor ...       cnn      -1   \n",
      "8      This mogul once say he 's a member of the luc...       fox       1   \n",
      "9      Live Hillary Clinton speak at the NAACP Conve...       cbs       0   \n",
      "10     BREAKING Hillary Clinton have choose Virginia...  usatoday       0   \n",
      "11     The billboard translate read Donald Trump he ...       cnn      -1   \n",
      "12     How weird be the world of politics in 2015 We...       cbs      -1   \n",
      "13     NEW A Chicago police spokesperson tell CBS Ne...       cbs       0   \n",
      "14     Hillary Clinton do some bad things Donald J T...       fox      -1   \n",
      "15     Tim Kaine defend the work of the Clinton Foun...  usatoday       1   \n",
      "16     Just before Clinton and Trump square off in w...       cbs       0   \n",
      "17     Chelsea take to Twitter to announce her secon...  usatoday       1   \n",
      "18     One adviser describe Clinton as obsess with J...       fox      -1   \n",
      "19     If you look at it as an American as a human a...       cbs      -1   \n",
      "20     A new report show donors to the Clinton Found...       wsj      -1   \n",
      "21     Donald J Trump 's list include six federal ap...       cnn       0   \n",
      "22     Sen Elizabeth Warren launch a blister attack ...  usatoday      -1   \n",
      "23     Former KKK grand wizard David Duke do n't win...       cbs       0   \n",
      "24     We have to work to heal the divisions of a lo...       fox       1   \n",
      "25     For 40 years Donald J Trump have be part of t...       cbs      -1   \n",
      "26     Michelle Obama President Obama and Joe Biden ...       cbs      -1   \n",
      "27     Paul Ryan I think what Donald Trump just pull...       wsj       1   \n",
      "28     The 10-Point Gerard Baker on the World Econom...  usatoday       0   \n",
      "29     The school secretary wear an orange jumpsuit ...       cnn       0   \n",
      "...                                                 ...       ...     ...   \n",
      "1372   Donald Trump have this one wish about his weight  usatoday       0   \n",
      "1373   Hillary Clinton win the South Dakota Democrat...  usatoday       1   \n",
      "1374   BREAKING The FBI will review additional email...  usatoday      -1   \n",
      "1375   President Obama 's lower court appointments c...  usatoday       0   \n",
      "1376   When Barack Obama win Ohio in 2008 and 2012 h...  usatoday      -1   \n",
      "1377   Eight years after a hard-fought primary battl...  usatoday       1   \n",
      "1378   Hillary Clinton might rescue Republicans from...  usatoday      -1   \n",
      "1379   Sen Bernie Sanders be lay the groundwork for ...  usatoday       1   \n",
      "1380   Trump may be peak early Or it may be the begi...  usatoday       1   \n",
      "1381   During the primaries one of the most common a...  usatoday      -1   \n",
      "1382                       Trumpy be her biggest seller  usatoday       0   \n",
      "1383   Bernie Sanders could give Hillary Clinton tha...  usatoday       1   \n",
      "1384   Trump have blast Clinton since she make the d...  usatoday      -1   \n",
      "1385   Police remove Rose Hamid who be wear a hijab ...  usatoday       0   \n",
      "1386   Hillary Clinton may be the most corrupt perso...  usatoday      -1   \n",
      "1387   I have just one word for Donald Trump Basta E...  usatoday      -1   \n",
      "1388   Donald J Trump be no longer commit to sue the...  usatoday       0   \n",
      "1389   Trump say it would be nice if the Republican ...  usatoday       1   \n",
      "1390   Mike Pence offer a full-throated endorsement ...  usatoday       1   \n",
      "1391     The remark do n't sit well with Donald J Trump  usatoday      -1   \n",
      "1392   Donald J Trump have a lot to say about Tuesda...  usatoday       0   \n",
      "1393   Do me a favor Clinton tell New York City Mayo...  usatoday       0   \n",
      "1394   The American people deserve to get the full a...  usatoday       0   \n",
      "1395   Donald J Trump call Hillary Clinton a candida...  usatoday      -1   \n",
      "1396   The bill now go to Gov Mary Fallin whose name...  usatoday       0   \n",
      "1397   With a smile on her face and some help from a...  usatoday       1   \n",
      "1398   Donald J Trump continue to lead the Republica...  usatoday       0   \n",
      "1399   Donald Trump tell 'The Wall Street Journal th...  usatoday       1   \n",
      "1400   Nothing have make me prouder than to be your ...  usatoday       1   \n",
      "1401   The latest release include around 150 email t...  usatoday       0   \n",
      "\n",
      "     candidates  \n",
      "0         trump  \n",
      "1         trump  \n",
      "2       clinton  \n",
      "3         trump  \n",
      "4         trump  \n",
      "5       clinton  \n",
      "6         trump  \n",
      "7         trump  \n",
      "8         trump  \n",
      "9       clinton  \n",
      "10      clinton  \n",
      "11        trump  \n",
      "12        trump  \n",
      "13        trump  \n",
      "14        other  \n",
      "15      clinton  \n",
      "16        other  \n",
      "17      clinton  \n",
      "18      clinton  \n",
      "19        trump  \n",
      "20      clinton  \n",
      "21        trump  \n",
      "22        trump  \n",
      "23        other  \n",
      "24        trump  \n",
      "25        trump  \n",
      "26      clinton  \n",
      "27        trump  \n",
      "28        trump  \n",
      "29      clinton  \n",
      "...         ...  \n",
      "1372      trump  \n",
      "1373    clinton  \n",
      "1374    clinton  \n",
      "1375      other  \n",
      "1376      trump  \n",
      "1377    clinton  \n",
      "1378      other  \n",
      "1379      other  \n",
      "1380      trump  \n",
      "1381    clinton  \n",
      "1382      trump  \n",
      "1383    clinton  \n",
      "1384      other  \n",
      "1385      trump  \n",
      "1386      other  \n",
      "1387      trump  \n",
      "1388      trump  \n",
      "1389      trump  \n",
      "1390      trump  \n",
      "1391      trump  \n",
      "1392      trump  \n",
      "1393    clinton  \n",
      "1394    clinton  \n",
      "1395      other  \n",
      "1396      trump  \n",
      "1397    clinton  \n",
      "1398      trump  \n",
      "1399      trump  \n",
      "1400    clinton  \n",
      "1401    clinton  \n",
      "\n",
      "[1402 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Lemmanize the sentences (e.g. is, are, am ->> be)\n",
    "# Might be better to skip it\n",
    "\n",
    "new_index = range(0, len(sample.index))\n",
    "new_sample = pd.DataFrame(index=new_index, columns=(\"texts\", \"sources\", \"labels\"))\n",
    "i=0\n",
    "for text in sample['texts']:\n",
    "    blob = TextBlob(text)\n",
    "    newtexts = \"\"\n",
    "\n",
    "    for sentence in blob.sentences:\n",
    "        newtext = \"\"\n",
    "\n",
    "        for word in sentence.words:\n",
    "            newtext += \" \" + word.lemmatize('v') # 'v' for 'verb'\n",
    "\n",
    "        newtexts += newtext\n",
    "    new_sample['texts'].loc[i] = newtexts\n",
    "    i += 1\n",
    "    \n",
    "sample['texts'] = new_sample['texts']\n",
    "print sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  texts   sources  labels  \\\n",
      "0     It unusual president's children clued White Ho...       cbs       1   \n",
      "1     Just hours Mitt Romney blasted Donald J. Trump...       cbs      -1   \n",
      "2     Hillary Clinton wins state Washington, CBS New...       fox       1   \n",
      "3     \"We take jobs away countries making product, t...  usatoday      -1   \n",
      "4     Jessica Leeds, 74, Manhattan, one two women qu...       cbs      -1   \n",
      "5     Ash Carter warns Russia policy Syria consequen...       cbs      -1   \n",
      "6     \"It's Rubio!\" Watch Donald J. Trump use water ...  usatoday       0   \n",
      "7     Online, Trump supporters discuss monitoring po...       cnn      -1   \n",
      "8     This mogul said he's member \"the lucky sperm c...       fox       1   \n",
      "9     Live: Hillary Clinton speaks NAACP Convention ...       cbs       0   \n",
      "10    BREAKING: Hillary Clinton chosen Virginia Sena...  usatoday       0   \n",
      "11    The billboard translated reads: \"Donald Trump,...       cnn      -1   \n",
      "12    How weird world politics 2015? We didn't even ...       cbs      -1   \n",
      "13    NEW: A Chicago police spokesperson told CBS Ne...       cbs       0   \n",
      "14    Hillary Clinton \"did bad things,\" Donald J. Tr...       fox      -1   \n",
      "15    Tim Kaine defended work Clinton Foundation, sa...  usatoday       1   \n",
      "16    Just Clinton Trump squared would become one na...       cbs       0   \n",
      "17    Chelsea took Twitter announce second child, mo...  usatoday       1   \n",
      "18     One adviser described Clinton obsessed Jeb Bush.       fox      -1   \n",
      "19    \"If look American, human, you're totally disgu...       cbs      -1   \n",
      "20    A new report shows donors Clinton Foundation s...       wsj      -1   \n",
      "21    Donald J. Trump's list includes six federal ap...       cnn       0   \n",
      "22    Sen. Elizabeth Warren launched blistering atta...  usatoday      -1   \n",
      "23    Former KKK grand wizard David Duke didn't win ...       cbs       0   \n",
      "24    \"We work heal divisions long campaign,\" Paul R...       fox       1   \n",
      "25    For 40 years, Donald J. Trump part corruption ...       cbs      -1   \n",
      "26    Michelle Obama, President Obama Joe Biden camp...       cbs      -1   \n",
      "27    Paul Ryan: \"I think Donald Trump pulled enormo...       wsj       1   \n",
      "28    The 10-Point: Gerard Baker World Economic Foru...  usatoday       0   \n",
      "29    The school secretary wore orange jumpsuit \"Dep...       cnn       0   \n",
      "...                                                 ...       ...     ...   \n",
      "1372                      Donald Trump one wish weight.  usatoday       0   \n",
      "1373  Hillary Clinton wins South Dakota Democratic p...  usatoday       1   \n",
      "1374  BREAKING: The FBI review additional emails Hil...  usatoday      -1   \n",
      "1375  President Obama's lower court appointments cou...  usatoday       0   \n",
      "1376  When Barack Obama Ohio 2008 2012, support wasn...  usatoday      -1   \n",
      "1377  Eight years hard-fought primary battle, Michel...  usatoday       1   \n",
      "1378  Hillary Clinton might rescue Republicans inevi...  usatoday      -1   \n",
      "1379  Sen. Bernie Sanders laying groundwork post-ele...  usatoday       1   \n",
      "1380  \"Trump may peaking early. Or may beginning som...  usatoday       1   \n",
      "1381  During primaries, one common attacks Bernie Sa...  usatoday      -1   \n",
      "1382                           \"Trumpy\" biggest seller.  usatoday       0   \n",
      "1383  Bernie Sanders could give Hillary Clinton long...  usatoday       1   \n",
      "1384  Trump blasted Clinton since made \"deplorables\"...  usatoday      -1   \n",
      "1385  Police removed Rose Hamid, wearing hijab, stoo...  usatoday       0   \n",
      "1386  \"Hillary Clinton may corrupt person ever seek ...  usatoday      -1   \n",
      "1387          \"I one word Donald Trump: Basta! Enough!\"  usatoday      -1   \n",
      "1388  Donald J. Trump longer committed suing women a...  usatoday       0   \n",
      "1389  Trump says \"it would nice\" Republican Party ge...  usatoday       1   \n",
      "1390  Mike Pence offered full-throated endorsement D...  usatoday       1   \n",
      "1391       The remarks didn't sit well Donald J. Trump.  usatoday      -1   \n",
      "1392      Donald J. Trump lot say Tuesday's #DemDebate.  usatoday       0   \n",
      "1393  Do favor, Clinton told New York City Mayor Bil...  usatoday       0   \n",
      "1394  \"The American people deserve get full complete...  usatoday       0   \n",
      "1395  Donald J. Trump called Hillary Clinton candida...  usatoday      -1   \n",
      "1396  The bill goes Gov. Mary Fallin, whose name flo...  usatoday       0   \n",
      "1397  With smile face help poll worker far-off fanta...  usatoday       1   \n",
      "1398  Donald J. Trump continues lead Republican fiel...  usatoday       0   \n",
      "1399  Donald Trump told 'The Wall Street Journal' ma...  usatoday       1   \n",
      "1400  Nothing made prouder champion, says Hillary Cl...  usatoday       1   \n",
      "1401  The latest release includes \"around 150\" email...  usatoday       0   \n",
      "\n",
      "     candidates  \n",
      "0         trump  \n",
      "1         trump  \n",
      "2       clinton  \n",
      "3         trump  \n",
      "4         trump  \n",
      "5       clinton  \n",
      "6         trump  \n",
      "7         trump  \n",
      "8         trump  \n",
      "9       clinton  \n",
      "10      clinton  \n",
      "11        trump  \n",
      "12        trump  \n",
      "13        trump  \n",
      "14        other  \n",
      "15      clinton  \n",
      "16        other  \n",
      "17      clinton  \n",
      "18      clinton  \n",
      "19        trump  \n",
      "20      clinton  \n",
      "21        trump  \n",
      "22        trump  \n",
      "23        other  \n",
      "24        trump  \n",
      "25        trump  \n",
      "26      clinton  \n",
      "27        trump  \n",
      "28        trump  \n",
      "29      clinton  \n",
      "...         ...  \n",
      "1372      trump  \n",
      "1373    clinton  \n",
      "1374    clinton  \n",
      "1375      other  \n",
      "1376      trump  \n",
      "1377    clinton  \n",
      "1378      other  \n",
      "1379      other  \n",
      "1380      trump  \n",
      "1381    clinton  \n",
      "1382      trump  \n",
      "1383    clinton  \n",
      "1384      other  \n",
      "1385      trump  \n",
      "1386      other  \n",
      "1387      trump  \n",
      "1388      trump  \n",
      "1389      trump  \n",
      "1390      trump  \n",
      "1391      trump  \n",
      "1392      trump  \n",
      "1393    clinton  \n",
      "1394    clinton  \n",
      "1395      other  \n",
      "1396      trump  \n",
      "1397    clinton  \n",
      "1398      trump  \n",
      "1399      trump  \n",
      "1400    clinton  \n",
      "1401    clinton  \n",
      "\n",
      "[1402 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Getting rid of toocommon words (e.g. the, an)\n",
    "\n",
    "#print stopwords.words(\"english\") \n",
    "\n",
    "new_index = range(0, len(sample.index))\n",
    "new_sample = pd.DataFrame(index=new_index, columns=(\"texts\", \"sources\", \"labels\"))\n",
    "\n",
    "i=0\n",
    "for index, row in sample.iterrows():\n",
    "    # If this new post is different post, then append what we worked so far\n",
    "    words = [w for w in row['texts'].split() if not w in stopwords.words(\"english\")]\n",
    "    new_sample['texts'].loc[i] = ' '.join(words)\n",
    "    i += 1\n",
    "\n",
    "sample['texts'] = new_sample['texts']\n",
    "print sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 95 0 0 300\t 0.4857\n",
      "5 95 0 0 1300\n",
      "5 95 0 0 2300\n",
      "5 95 0 0 3300\n",
      "5 95 0 0 4300\n",
      "5 95 0 0 5300\n",
      "5 95 0 0 6300\n",
      "5 95 0 0 7300\n",
      "5 95 0 0 8300\n",
      "5 95 0 0 9300\n",
      "5 95 0 0 10300\n",
      "5 95 0 0 11300\n",
      "5 95 0 0 12300\n",
      "5 95 0 1 300\n",
      "5 95 0 1 1300\n",
      "5 95 0 1 2300\n",
      "5 95 0 1 3300\n",
      "5 95 0 1 4300\n",
      "5 95 0 1 5300\n",
      "5 95 0 1 6300\n",
      "5 95 0 1 7300\n",
      "5 95 0 1 8300\n",
      "5 95 0 1 9300\n",
      "5 95 0 1 10300\n",
      "5 95 0 1 11300\n",
      "5 95 0 1 12300\n",
      "5 95 0 2 300\t 0.5214\n",
      "5 95 0 2 1300\n",
      "5 95 0 2 2300\n",
      "5 95 0 2 3300\n",
      "5 95 0 2 4300\n",
      "5 95 0 2 5300\n",
      "5 95 0 2 6300\n",
      "5 95 0 2 7300\n",
      "5 95 0 2 8300\n",
      "5 95 0 2 9300\n",
      "5 95 0 2 10300\n",
      "5 95 0 2 11300\n",
      "5 95 0 2 12300\n",
      "5 95 0 3 300\n",
      "5 95 0 3 1300\n",
      "5 95 0 3 2300\n",
      "5 95 0 3 3300\n",
      "5 95 0 3 4300\n",
      "5 95 0 3 5300\n",
      "5 95 0 3 6300\n",
      "5 95 0 3 7300\n",
      "5 95 0 3 8300\n",
      "5 95 0 3 9300\n",
      "5 95 0 3 10300\n",
      "5 95 0 3 11300\n",
      "5 95 0 3 12300\n",
      "5 95 0 4 300\n",
      "5 95 0 4 1300\n",
      "5 95 0 4 2300\n",
      "5 95 0 4 3300\n",
      "5 95 0 4 4300\n",
      "5 95 0 4 5300\n",
      "5 95 0 4 6300\n",
      "5 95 0 4 7300\n",
      "5 95 0 4 8300\n",
      "5 95 0 4 9300\n",
      "5 95 0 4 10300\n",
      "5 95 0 4 11300\n",
      "5 95 0 4 12300\n",
      "5 95 0 5 300\n",
      "5 95 0 5 1300\n",
      "5 95 0 5 2300\n",
      "5 95 0 5 3300\n",
      "5 95 0 5 4300\n",
      "5 95 0 5 5300\n",
      "5 95 0 5 6300\n",
      "5 95 0 5 7300\n",
      "5 95 0 5 8300\n",
      "5 95 0 5 9300\n",
      "5 95 0 5 10300\n",
      "5 95 0 5 11300\n",
      "5 95 0 5 12300\n",
      "5 95 0 6 300\n",
      "5 95 0 6 1300\n",
      "5 95 0 6 2300\n",
      "5 95 0 6 3300\n",
      "5 95 0 6 4300\n",
      "5 95 0 6 5300\n",
      "5 95 0 6 6300\n",
      "5 95 0 6 7300\n",
      "5 95 0 6 8300\n",
      "5 95 0 6 9300\n",
      "5 95 0 6 10300\n",
      "5 95 0 6 11300\n",
      "5 95 0 6 12300\n",
      "5 95 0 7 300\n",
      "5 95 0 7 1300\n",
      "5 95 0 7 2300\n",
      "5 95 0 7 3300\n",
      "5 95 0 7 4300\n",
      "5 95 0 7 5300\n",
      "5 95 0 7 6300\n",
      "5 95 0 7 7300\n",
      "5 95 0 7 8300\n",
      "5 95 0 7 9300\n",
      "5 95 0 7 10300\n",
      "5 95 0 7 11300\n",
      "5 95 0 7 12300\n",
      "5 95 0 8 300\n",
      "5 95 0 8 1300\n",
      "5 95 0 8 2300\n",
      "5 95 0 8 3300\n",
      "5 95 0 8 4300\n",
      "5 95 0 8 5300\n",
      "5 95 0 8 6300\n",
      "5 95 0 8 7300\n",
      "5 95 0 8 8300\n",
      "5 95 0 8 9300\n",
      "5 95 0 8 10300\n",
      "5 95 0 8 11300\n",
      "5 95 0 8 12300\n",
      "5 95 0 9 300\n",
      "5 95 0 9 1300\n",
      "5 95 0 9 2300\n",
      "5 95 0 9 3300\n",
      "5 95 0 9 4300\n",
      "5 95 0 9 5300\n",
      "5 95 0 9 6300\n",
      "5 95 0 9 7300\n",
      "5 95 0 9 8300\n",
      "5 95 0 9 9300\n",
      "5 95 0 9 10300\n",
      "5 95 0 9 11300\n",
      "5 95 0 9 12300\n",
      "5 95 0 10 300\n",
      "5 95 0 10 1300\n",
      "5 95 0 10 2300\n",
      "5 95 0 10 3300\n",
      "5 95 0 10 4300\n",
      "5 95 0 10 5300\n",
      "5 95 0 10 6300\n",
      "5 95 0 10 7300\n",
      "5 95 0 10 8300\n",
      "5 95 0 10 9300\n",
      "5 95 0 10 10300\n",
      "5 95 0 10 11300\n",
      "5 95 0 10 12300\n",
      "5 95 0 11 300\n",
      "5 95 0 11 1300\n",
      "5 95 0 11 2300\n",
      "5 95 0 11 3300\n",
      "5 95 0 11 4300\n",
      "5 95 0 11 5300\n",
      "5 95 0 11 6300\n",
      "5 95 0 11 7300\n",
      "5 95 0 11 8300\n",
      "5 95 0 11 9300\n",
      "5 95 0 11 10300\n",
      "5 95 0 11 11300\n",
      "5 95 0 11 12300\n",
      "5 95 0 12 300\n",
      "5 95 0 12 1300\n",
      "5 95 0 12 2300\n",
      "5 95 0 12 3300\n",
      "5 95 0 12 4300\n",
      "5 95 0 12 5300\n",
      "5 95 0 12 6300\n",
      "5 95 0 12 7300\n",
      "5 95 0 12 8300\n",
      "5 95 0 12 9300\n",
      "5 95 0 12 10300\n",
      "5 95 0 12 11300\n",
      "5 95 0 12 12300\n",
      "5 95 0 13 300\n",
      "5 95 0 13 1300\n",
      "5 95 0 13 2300\n",
      "5 95 0 13 3300\n",
      "5 95 0 13 4300\n",
      "5 95 0 13 5300\n",
      "5 95 0 13 6300\n",
      "5 95 0 13 7300\n",
      "5 95 0 13 8300\n",
      "5 95 0 13 9300\n",
      "5 95 0 13 10300\n",
      "5 95 0 13 11300\n",
      "5 95 0 13 12300\n",
      "5 95 0 14 300\n",
      "5 95 0 14 1300\n",
      "5 95 0 14 2300\n",
      "5 95 0 14 3300\n",
      "5 95 0 14 4300\n",
      "5 95 0 14 5300\n",
      "5 95 0 14 6300\n",
      "5 95 0 14 7300\n",
      "5 95 0 14 8300\n",
      "5 95 0 14 9300\n",
      "5 95 0 14 10300\n",
      "5 95 0 14 11300\n",
      "5 95 0 14 12300\n",
      "5 95 0 15 300\n",
      "5 95 0 15 1300\n",
      "5 95 0 15 2300\n",
      "5 95 0 15 3300\n",
      "5 95 0 15 4300\n",
      "5 95 0 15 5300\n",
      "5 95 0 15 6300\n",
      "5 95 0 15 7300\n",
      "5 95 0 15 8300\n",
      "5 95 0 15 9300\n",
      "5 95 0 15 10300\n",
      "5 95 0 15 11300\n",
      "5 95 0 15 12300\n",
      "5 95 0 16 300\n",
      "5 95 0 16 1300\n",
      "5 95 0 16 2300\n",
      "5 95 0 16 3300\n",
      "5 95 0 16 4300\n",
      "5 95 0 16 5300\n",
      "5 95 0 16 6300\n",
      "5 95 0 16 7300\n",
      "5 95 0 16 8300\n",
      "5 95 0 16 9300\n",
      "5 95 0 16 10300\n",
      "5 95 0 16 11300\n",
      "5 95 0 16 12300\n",
      "5 95 0 17 300\n",
      "5 95 0 17 1300\n",
      "5 95 0 17 2300\n",
      "5 95 0 17 3300\n",
      "5 95 0 17 4300\n",
      "5 95 0 17 5300\n",
      "5 95 0 17 6300\n",
      "5 95 0 17 7300\n",
      "5 95 0 17 8300\n",
      "5 95 0 17 9300\n",
      "5 95 0 17 10300\n",
      "5 95 0 17 11300\n",
      "5 95 0 17 12300\n",
      "5 95 0 18 300\n",
      "5 95 0 18 1300\n",
      "5 95 0 18 2300\n",
      "5 95 0 18 3300\n",
      "5 95 0 18 4300\n",
      "5 95 0 18 5300\n",
      "5 95 0 18 6300\n",
      "5 95 0 18 7300\n",
      "5 95 0 18 8300\n",
      "5 95 0 18 9300\n",
      "5 95 0 18 10300\n",
      "5 95 0 18 11300\n",
      "5 95 0 18 12300\n",
      "5 95 0 19 300\n",
      "5 95 0 19 1300\n",
      "5 95 0 19 2300\n",
      "5 95 0 19 3300\n",
      "5 95 0 19 4300\n",
      "5 95 0 19 5300\n",
      "5 95 0 19 6300\n",
      "5 95 0 19 7300\n",
      "5 95 0 19 8300\n",
      "5 95 0 19 9300\n",
      "5 95 0 19 10300\n",
      "5 95 0 19 11300\n",
      "5 95 0 19 12300\n",
      "5 95 0 20 300\n",
      "5 95 0 20 1300\n",
      "5 95 0 20 2300\n",
      "5 95 0 20 3300\n",
      "5 95 0 20 4300\n",
      "5 95 0 20 5300\n",
      "5 95 0 20 6300\n",
      "5 95 0 20 7300\n",
      "5 95 0 20 8300\n",
      "5 95 0 20 9300\n",
      "5 95 0 20 10300\n",
      "5 95 0 20 11300\n",
      "5 95 0 20 12300\n",
      "5 95 0 21 300\n",
      "5 95 0 21 1300\n",
      "5 95 0 21 2300\n",
      "5 95 0 21 3300\n",
      "5 95 0 21 4300\n",
      "5 95 0 21 5300\n",
      "5 95 0 21 6300\n",
      "5 95 0 21 7300\n",
      "5 95 0 21 8300\n",
      "5 95 0 21 9300\n",
      "5 95 0 21 10300\n",
      "5 95 0 21 11300\n",
      "5 95 0 21 12300\n",
      "5 95 0 22 300\n",
      "5 95 0 22 1300\n",
      "5 95 0 22 2300\n",
      "5 95 0 22 3300\n",
      "5 95 0 22 4300\n",
      "5 95 0 22 5300\n",
      "5 95 0 22 6300\n",
      "5 95 0 22 7300\n",
      "5 95 0 22 8300\n",
      "5 95 0 22 9300\n",
      "5 95 0 22 10300\n",
      "5 95 0 22 11300\n",
      "5 95 0 22 12300\n",
      "5 95 0 23 300\n",
      "5 95 0 23 1300\n",
      "5 95 0 23 2300\n",
      "5 95 0 23 3300\n",
      "5 95 0 23 4300\n",
      "5 95 0 23 5300\n",
      "5 95 0 23 6300\n",
      "5 95 0 23 7300\n",
      "5 95 0 23 8300\n",
      "5 95 0 23 9300\n",
      "5 95 0 23 10300\n",
      "5 95 0 23 11300\n",
      "5 95 0 23 12300\n",
      "5 95 0 24 300\n",
      "5 95 0 24 1300\n",
      "5 95 0 24 2300\n",
      "5 95 0 24 3300\n",
      "5 95 0 24 4300\n",
      "5 95 0 24 5300\n",
      "5 95 0 24 6300\n",
      "5 95 0 24 7300\n",
      "5 95 0 24 8300\n",
      "5 95 0 24 9300\n",
      "5 95 0 24 10300\n",
      "5 95 0 24 11300\n",
      "5 95 0 24 12300\n",
      "5 95 1 0 300\n",
      "5 95 1 0 1300\n",
      "5 95 1 0 2300\n",
      "5 95 1 0 3300\n",
      "5 95 1 0 4300\n",
      "5 95 1 0 5300\n",
      "5 95 1 0 6300\n",
      "5 95 1 0 7300\n",
      "5 95 1 0 8300\n",
      "5 95 1 0 9300\n",
      "5 95 1 0 10300\n",
      "5 95 1 0 11300\n",
      "5 95 1 0 12300\n",
      "5 95 1 1 300\n",
      "5 95 1 1 1300\n",
      "5 95 1 1 2300\n",
      "5 95 1 1 3300\n",
      "5 95 1 1 4300\n",
      "5 95 1 1 5300\n",
      "5 95 1 1 6300\n",
      "5 95 1 1 7300\n",
      "5 95 1 1 8300\n",
      "5 95 1 1 9300\n",
      "5 95 1 1 10300\n",
      "5 95 1 1 11300\n",
      "5 95 1 1 12300\n",
      "5 95 1 2 300\n",
      "5 95 1 2 1300\n",
      "5 95 1 2 2300\n",
      "5 95 1 2 3300\n",
      "5 95 1 2 4300\n",
      "5 95 1 2 5300\n",
      "5 95 1 2 6300\n",
      "5 95 1 2 7300\n",
      "5 95 1 2 8300\n",
      "5 95 1 2 9300\n",
      "5 95 1 2 10300\n",
      "5 95 1 2 11300\n",
      "5 95 1 2 12300\n",
      "5 95 1 3 300\n",
      "5 95 1 3 1300\n",
      "5 95 1 3 2300\n",
      "5 95 1 3 3300\n",
      "5 95 1 3 4300\n",
      "5 95 1 3 5300\n",
      "5 95 1 3 6300\n",
      "5 95 1 3 7300\n",
      "5 95 1 3 8300\n",
      "5 95 1 3 9300\n",
      "5 95 1 3 10300\n",
      "5 95 1 3 11300\n",
      "5 95 1 3 12300\n",
      "5 95 1 4 300\n",
      "5 95 1 4 1300\n",
      "5 95 1 4 2300\n",
      "5 95 1 4 3300\n",
      "5 95 1 4 4300\n",
      "5 95 1 4 5300\n",
      "5 95 1 4 6300\n",
      "5 95 1 4 7300\n",
      "5 95 1 4 8300\n",
      "5 95 1 4 9300\n",
      "5 95 1 4 10300\n",
      "5 95 1 4 11300\n",
      "5 95 1 4 12300\n",
      "5 95 1 5 300\n",
      "5 95 1 5 1300\n",
      "5 95 1 5 2300\n",
      "5 95 1 5 3300\n",
      "5 95 1 5 4300\n",
      "5 95 1 5 5300\n",
      "5 95 1 5 6300\n",
      "5 95 1 5 7300\n",
      "5 95 1 5 8300\n",
      "5 95 1 5 9300\n",
      "5 95 1 5 10300\n",
      "5 95 1 5 11300\n",
      "5 95 1 5 12300\n",
      "5 95 1 6 300\n",
      "5 95 1 6 1300\n",
      "5 95 1 6 2300\n",
      "5 95 1 6 3300\n",
      "5 95 1 6 4300\n",
      "5 95 1 6 5300\n",
      "5 95 1 6 6300\n",
      "5 95 1 6 7300\n",
      "5 95 1 6 8300\n",
      "5 95 1 6 9300\n",
      "5 95 1 6 10300\n",
      "5 95 1 6 11300\n",
      "5 95 1 6 12300\n",
      "5 95 1 7 300\n",
      "5 95 1 7 1300\n",
      "5 95 1 7 2300\n",
      "5 95 1 7 3300\n",
      "5 95 1 7 4300\n",
      "5 95 1 7 5300\n",
      "5 95 1 7 6300\n",
      "5 95 1 7 7300\n",
      "5 95 1 7 8300\n",
      "5 95 1 7 9300\n",
      "5 95 1 7 10300\n",
      "5 95 1 7 11300\n",
      "5 95 1 7 12300\n",
      "5 95 1 8 300\n",
      "5 95 1 8 1300\n",
      "5 95 1 8 2300\n",
      "5 95 1 8 3300\n",
      "5 95 1 8 4300\n",
      "5 95 1 8 5300\n",
      "5 95 1 8 6300\n",
      "5 95 1 8 7300\n",
      "5 95 1 8 8300\n",
      "5 95 1 8 9300\n",
      "5 95 1 8 10300\n",
      "5 95 1 8 11300\n",
      "5 95 1 8 12300\n",
      "5 95 1 9 300\n",
      "5 95 1 9 1300\n",
      "5 95 1 9 2300\n",
      "5 95 1 9 3300\n",
      "5 95 1 9 4300\n",
      "5 95 1 9 5300\n",
      "5 95 1 9 6300\n",
      "5 95 1 9 7300\n",
      "5 95 1 9 8300\n",
      "5 95 1 9 9300\n",
      "5 95 1 9 10300\n",
      "5 95 1 9 11300\n",
      "5 95 1 9 12300\n",
      "5 95 1 10 300\n",
      "5 95 1 10 1300\n",
      "5 95 1 10 2300\n",
      "5 95 1 10 3300\n",
      "5 95 1 10 4300\n",
      "5 95 1 10 5300\n",
      "5 95 1 10 6300\n",
      "5 95 1 10 7300\n",
      "5 95 1 10 8300\n",
      "5 95 1 10 9300\n",
      "5 95 1 10 10300\n",
      "5 95 1 10 11300\n",
      "5 95 1 10 12300\n",
      "5 95 1 11 300\n",
      "5 95 1 11 1300\n",
      "5 95 1 11 2300\n",
      "5 95 1 11 3300\n",
      "5 95 1 11 4300\n",
      "5 95 1 11 5300\n",
      "5 95 1 11 6300\n",
      "5 95 1 11 7300\n",
      "5 95 1 11 8300\n",
      "5 95 1 11 9300\n",
      "5 95 1 11 10300\n",
      "5 95 1 11 11300\n",
      "5 95 1 11 12300\n",
      "5 95 1 12 300\n",
      "5 95 1 12 1300\n",
      "5 95 1 12 2300\n",
      "5 95 1 12 3300\n",
      "5 95 1 12 4300\n",
      "5 95 1 12 5300\n",
      "5 95 1 12 6300\n",
      "5 95 1 12 7300\n",
      "5 95 1 12 8300\n",
      "5 95 1 12 9300\n",
      "5 95 1 12 10300\n",
      "5 95 1 12 11300\n",
      "5 95 1 12 12300\n",
      "5 95 1 13 300\n",
      "5 95 1 13 1300\n",
      "5 95 1 13 2300\n",
      "5 95 1 13 3300\n",
      "5 95 1 13 4300\n",
      "5 95 1 13 5300\n",
      "5 95 1 13 6300\n",
      "5 95 1 13 7300\n",
      "5 95 1 13 8300\n",
      "5 95 1 13 9300\n",
      "5 95 1 13 10300\n",
      "5 95 1 13 11300\n",
      "5 95 1 13 12300\n",
      "5 95 1 14 300\n",
      "5 95 1 14 1300\n",
      "5 95 1 14 2300\n",
      "5 95 1 14 3300\n",
      "5 95 1 14 4300\n",
      "5 95 1 14 5300\n",
      "5 95 1 14 6300\n",
      "5 95 1 14 7300\n",
      "5 95 1 14 8300\n",
      "5 95 1 14 9300\n",
      "5 95 1 14 10300\n",
      "5 95 1 14 11300\n",
      "5 95 1 14 12300\n",
      "5 95 1 15 300\n",
      "5 95 1 15 1300\n",
      "5 95 1 15 2300\n",
      "5 95 1 15 3300\n",
      "5 95 1 15 4300\n",
      "5 95 1 15 5300\n",
      "5 95 1 15 6300\n",
      "5 95 1 15 7300\n",
      "5 95 1 15 8300\n",
      "5 95 1 15 9300\n",
      "5 95 1 15 10300\n",
      "5 95 1 15 11300\n",
      "5 95 1 15 12300\n",
      "5 95 1 16 300\n",
      "5 95 1 16 1300\n",
      "5 95 1 16 2300\n"
     ]
    }
   ],
   "source": [
    "#for j in range(0,2): # stop_words: k=0 'engl' k=1 none\n",
    "best_score = 0\n",
    "\n",
    "for h in range(5,18,2): # min_df from 0.05 to 0.30, adding 0.02 every time\n",
    "    for i in range(95,72,-2): # max_df from 0.95 to 0.70, -0.02 every time\n",
    "        for k in range(0,2): # k=0 CountVectorizer (count), k=1 TfidfVectorizer (weighed)\n",
    "            for l in range(0,25): \n",
    "                for m in range(300,13001,1000): # max_features (how many most frequent words should we consider)\n",
    "\n",
    "                    if k==0:\n",
    "                        tf_vectorizer = CountVectorizer(max_df=i/100.0, \n",
    "                                                        min_df=h/100.0, \n",
    "                                                        max_features = m, \n",
    "                                                        analyzer = \"word\")\n",
    "                    elif k==1:\n",
    "                        tf_vectorizer = TfidfVectorizer(max_df=i/100.0, \n",
    "                                                        min_df=h/100.0, \n",
    "                                                        max_features = m,\n",
    "                                                        analyzer = \"word\")\n",
    "\n",
    "                    train_text_tf_  = tf_vectorizer.fit_transform(train['texts'].values)\n",
    "                    test_text_tf_  = tf_vectorizer.transform(test['texts'].values)\n",
    "\n",
    "                    train_source_tf_ = tf_vectorizer.fit_transform(train['sources'].values)\n",
    "                    test_source_tf_ = tf_vectorizer.transform(test['sources'].values)\n",
    "\n",
    "                    train_candidate = tf_vectorizer.fit_transform(train['candidates'].values)\n",
    "                    test_candidate = tf_vectorizer.transform(test['candidates'].values)\n",
    "\n",
    "                    train_combined = sp.hstack([train_text_tf_, train_source_tf_], format='csr')\n",
    "                    test_combined = sp.hstack([test_text_tf_, test_source_tf_], format='csr')\n",
    "\n",
    "                    train_combined = sp.hstack([train_combined, train_candidate], format='csr')\n",
    "                    test_combined = sp.hstack([test_combined, test_candidate], format='csr')\n",
    "\n",
    "                    # CHANGE ARGS TOO\n",
    "                    if l==0:\n",
    "                        clf = MultinomialNB()\n",
    "                    elif l==1:\n",
    "                        clf = GaussianNB()\n",
    "                    elif l==2:\n",
    "                        clf = BernoulliNB()\n",
    "                    elif l==3:\n",
    "                        clf = LogisticRegression(random_state=1)\n",
    "                    elif l==4:\n",
    "                        clf = LogisticRegressionCV()\n",
    "                    elif l==5:\n",
    "                        clf = SGDClassifier()\n",
    "                    elif l==6:\n",
    "                        clf = Ridge()\n",
    "                    elif l==7:\n",
    "                        clf = RidgeClassifier()\n",
    "                    elif l==8:\n",
    "                        clf = RidgeClassifierCV()\n",
    "                    elif l==9:\n",
    "                        clf = ElasticNet()\n",
    "                    elif l==10:\n",
    "                        clf = LassoLars()\n",
    "                    elif l==11:\n",
    "                        clf = PassiveAggressiveClassifier()\n",
    "                    elif l==12:\n",
    "                        clf = SVC(random_state=1)\n",
    "                    elif l==13:\n",
    "                        clf = LinearSVC()\n",
    "                    elif l==14:\n",
    "                        clf = KNeighborsClassifier(n_neighbors=9)\n",
    "                    elif l==15:\n",
    "                        clf = NearestCentroid()\n",
    "                    elif l==16:\n",
    "                        clf = GradientBoostingClassifier()\n",
    "                    elif l==17:\n",
    "                        clf = BaggingClassifier(random_state=1)\n",
    "                    elif l==18:\n",
    "                        clf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "                    elif l==19:\n",
    "                        clf = AdaBoostClassifier()\n",
    "                    elif l==20:\n",
    "                        clf = ExtraTreesClassifier()\n",
    "                    elif l==21:\n",
    "                        clf = GradientBoostingClassifier()\n",
    "                    elif l==22:\n",
    "                        clf = VotingClassifier(estimators=\n",
    "                                [('l01', MultinomialNB()),\n",
    "                                ('l02', GaussianNB()),\n",
    "                                ('l03', BernoulliNB()),\n",
    "                                ('l04', LogisticRegression(random_state=1)),\n",
    "                                ('l13', SVC(random_state=1)),\n",
    "                                ('l15', KNeighborsClassifier(n_neighbors=9)),\n",
    "                                ('l18', BaggingClassifier(random_state=1)),\n",
    "                                ('l19', RandomForestClassifier(n_estimators=100, random_state=1)),\n",
    "                                ('l24', DecisionTreeClassifier(random_state=1))])\n",
    "                    elif l==23:\n",
    "                        clf = ExtraTreeClassifier()\n",
    "                    elif l==24:\n",
    "                        clf = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "                    clf.fit(train_combined.toarray(), train['labels'])\n",
    "                    score = clf.score(test_combined.toarray(), test['labels'])\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        print \"%d %d %d %d %d\\t %.4f\" % (h,i,k,l,m,score)\n",
    "                    else:\n",
    "                        print \"%d %d %d %d %d\" % (h,i,k,l,m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 20,000+ possibilities and running 8000+ cases, <br>\n",
    "It never hit 0.7.. ->> For train:test = 1:1 size\n",
    "\n",
    "Train:test = 10:1 size. I do get up to 0.8+ <br>\n",
    "Now we also have up to (more reasonable) 14400 cases (all expected to run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning w/ TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Optimization Progress:   0%|          | 0/120 [00:00<?, ?pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  17%|█▋        | 20/120 [01:41<03:51,  2.32s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.575650487096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  32%|███▏      | 38/120 [01:54<01:01,  1.33pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.575650487096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  48%|████▊     | 57/120 [02:13<01:01,  1.03pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.575650487096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  66%|██████▌   | 79/120 [02:27<00:22,  1.81pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.576309301759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  83%|████████▎ | 100/120 [02:37<00:11,  1.72pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 0.576309301759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   0%|          | 0/120 [00:00<?, ?pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: LinearSVC(input_matrix, 0.67000000000000004, 50, True)\n",
      "0.543037941767\n",
      "95 5 80 0 50 5 20 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  17%|█▋        | 20/120 [02:27<06:45,  4.05s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.550133066605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  32%|███▎      | 39/120 [05:48<14:19, 10.61s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.550133066605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  48%|████▊     | 58/120 [11:20<26:21, 25.51s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.550133066605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  68%|██████▊   | 81/120 [20:00<12:27, 19.16s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.550133066605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GP closed prematurely - will use current best pipeline\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-db650b823231>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m                             tpot = TPOTClassifier(generations=n, population_size=p, num_cv_folds=q,\n\u001b[1;32m     38\u001b[0m                                                   verbosity=2)\n\u001b[0;32m---> 39\u001b[0;31m                             \u001b[0mtpot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"%d %d %d %d %d %d %d %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/tpot/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, classes)\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m                     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fitted_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbosity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimized_pipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    324\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 326\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    740\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    348\u001b[0m                                            self.min_impurity_split)\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for i in range(0,2): # 'Negative' from sample = -1 or 0\n",
    "#  if i==0:\n",
    "#      sample['Answer.sentiment'] = sample['Answer.sentiment'].map({'Positive':1, 'Negative':-1})\n",
    "#  elif i==1:\n",
    "#      sample['Answer.sentiment'] = sample['Answer.sentiment'].map({'Positive':1, 'Negative':0})\n",
    "\n",
    "for j in range(5,21,2): # min_df\n",
    "    for k in range(80,101,2): # max_df\n",
    "        for l in range(0,2): # l=0 CountVectorizer (count), l=1 TfidfVectorizer (weighed)\n",
    "            for m in range(50,101,10): # train:test = m/100 : (1-m/100) so 50:50 to 90:10\n",
    "                for n in range(5,7): # generation (# of TPOT iteration)\n",
    "                    for p in range(20,26,5): # pop_size p \n",
    "                        for q in range(10,12): # k-fold number\n",
    "                            print \"\"    \n",
    "\n",
    "                            #sample = pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/sample_data_results.csv') \n",
    "\n",
    "                            # label ('positive','Negative') ->> (#,#) (e.g. (1,-1) or (1,0))\n",
    "                            #sample['Answer.sentiment'] = sample['Answer.sentiment'].map({'Positive':1, 'Neutral':0, 'Negative':-1})\n",
    "\n",
    "                            if l==0:\n",
    "                                tf_vectorizer = CountVectorizer(min_df=j/100.0, max_df=k/100.0, max_features=5000, analyzer = \"word\")\n",
    "                            elif l==1:\n",
    "                                tf_vectorizer = TfidfVectorizer(min_df=j/100.0, max_df=k/100.0, max_features=5000,analyzer = \"word\")\n",
    "\n",
    "                            #sample_input_tf  = tf_vectorizer.fit_transform(sample['Input.content'].values)\n",
    "                            text_tf_  = tf_vectorizer.fit_transform(sample['texts'].values)\n",
    "                            source_tf_ = tf_vectorizer.fit_transform(sample['sources'].values)\n",
    "                            candidates_tf_ = tf_vectorizer.fit_transform(sample['candidates'].values)\n",
    "                            combined = sp.hstack([text_tf_, source_tf_], format='csr')\n",
    "                            combined = sp.hstack([combined, candidates_tf_], format='csr')\n",
    "\n",
    "                            X_train, X_test, y_train, y_test = train_test_split(combined, sample['labels'].values,\n",
    "                                                                train_size=m/100.0, test_size=(1-m/100.0))#, random_state=)\n",
    "\n",
    "                            # Official website example: gen=5, pop_size=20, verbo=2\n",
    "                            tpot = TPOTClassifier(generations=n, population_size=p, num_cv_folds=q,\n",
    "                                                  verbosity=2)\n",
    "                            tpot.fit(X_train, y_train)\n",
    "                            print(tpot.score(X_test, y_test))\n",
    "                            print \"%d %d %d %d %d %d %d %d\" % (i,j,k,l,m,n,p,q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer, Pos=1 Neg=-1, train:test=75:25, size=872 in TPOT = 0.78 <br>\n",
    "CountVectorizer, Pos=1 Neg=-1, train:test=92:08, size=872 in TPOT = 0.78 <br>\n",
    "CountVectorizer, Pos=1 Neg =0, train:test=92:08, size=872 in TPOT = 0.86 <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "trump donald presidency nigel farage loathsome creature calls brexit leader obama inside national guru security mind change climate experts increasingly\n",
      "Topic #1:\n",
      "trump new york protesters win yorkers database tweets unfair hours info light calling mayor nyc deleted undocumented praise calls denounce\n",
      "Topic #2:\n",
      "rt help trump author historian lady future shyness cnnnewsroom detect melania speaker ryan deportation erecting force planning paul cnnpolitics allies\n",
      "Topic #3:\n",
      "america like americans hope trump promised watching does reid white fear feel tears innocent celebrate nationalists breath says deep everybody\n",
      "Topic #4:\n",
      "trump obamacare pulling going pol rug backing repeal day interview pledge appeared country end needs replace open agreed work democrats\n",
      "Topic #5:\n",
      "trump says kelly denies advance saw megyn debate report book question children gets advising rudy cnnsotu government giuliani jobs lead\n",
      "Topic #6:\n",
      "president cast ballots million unqualified candidates voters nearly 18 lady help trump win historian cities aid rendell fearful rt future\n",
      "Topic #7:\n",
      "election packing resolve american fear presidential express muslims shock start obama calls trump nigel loathsome creature leader brexit supporter farage\n",
      "Topic #8:\n",
      "trump donald elect president life voted clinton air win asranomani won state team administration new campaign protests muslim says force\n",
      "Topic #9:\n",
      "trump donald clinton hillary white people president house paul ryan says elect hate presidential sources just reince priebus chief staff\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, analyzer = \"word\",\n",
    "                                   #max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(train['texts'].values)\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, analyzer = \"word\",\n",
    "                                #max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(train['texts'].values)\n",
    "\n",
    "lda = LatentDirichletAllocation(#n_topics=n_topics, \n",
    "                                max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "lda.fit(tf)\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble & Bagging (Bootstrap AGgregating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nope... http://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So text extraction + ..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task 1: Load the texts\n",
    "import pandas as pd\n",
    "import glob, os         # for reading all .txt files\n",
    "import csv\n",
    "import numpy as np\n",
    "from textblob import TextBlob # use kernel Python[Root]\n",
    "\n",
    "cutoff = 436\n",
    "\n",
    "# Read sample texts\n",
    "sample = pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/sample_data_results.csv') \n",
    "\n",
    "# Lemmatize\n",
    "new_sample = pd.DataFrame(columns=(\"Input.content\", \"Answer.sentiment\"))\n",
    "i=0\n",
    "for text in sample['Input.content']:\n",
    "    blob = TextBlob(text)\n",
    "    newtexts = \"\"\n",
    "\n",
    "    for sentence in blob.sentences:\n",
    "        newtext = \"\"\n",
    "#        print sentence.dict\n",
    "\n",
    "        for word in sentence.words:\n",
    "            newtext += \" \" + word.lemmatize('v') # 'v' for 'verb'\n",
    "\n",
    "        newtexts += newtext\n",
    "        new_sample['texts'].loc[i] = newtexts\n",
    "        i += 1\n",
    "\n",
    "i=0\n",
    "for answer in sample['Answer.sentiment']: # updating answers\n",
    "    new_sample['Answer.sentiment'].loc[i] = answer\n",
    "    i += 1\n",
    "    \n",
    "sample = new_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.DataFrame({'label':sample['Answer.sentiment'][:cutoff], 'texts':sample['Input.content'][:cutoff]})\n",
    "test  = pd.DataFrame({'label':sample['Answer.sentiment'][cutoff:], 'texts':sample['Input.content'][cutoff:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 6 0 0 1 - 0.7778\n",
      "1 6 0 0 2 - 0.6389\n",
      "1 6 0 1 1 - 0.7778\n",
      "1 6 0 1 2 - 0.6389\n",
      "1 6 1 0 1 - 0.7778\n",
      "1 6 1 0 2 - 0.6389\n",
      "1 6 1 1 1 - 0.7222\n",
      "1 6 1 1 2 - 0.6944\n",
      "1 7 0 0 1 - 0.8056\n",
      "1 7 0 0 2 - 0.6389\n",
      "1 7 0 1 1 - 0.8056\n",
      "1 7 0 1 2 - 0.6389\n",
      "1 7 1 0 1 - 0.8056\n",
      "1 7 1 0 2 - 0.6389\n",
      "1 7 1 1 1 - 0.7222\n",
      "1 7 1 1 2 - 0.6389\n",
      "1 8 0 0 1 - 0.8056\n",
      "1 8 0 0 2 - 0.6389\n",
      "1 8 0 1 1 - 0.8056\n",
      "1 8 0 1 2 - 0.6389\n",
      "1 8 1 0 1 - 0.8056\n",
      "1 8 1 0 2 - 0.6389\n",
      "1 8 1 1 1 - 0.7222\n",
      "1 8 1 1 2 - 0.6389\n",
      "1 9 0 0 1 - 0.8056\n",
      "1 9 0 0 2 - 0.5833\n",
      "1 9 0 1 1 - 0.8056\n",
      "1 9 0 1 2 - 0.5833\n",
      "1 9 1 0 1 - 0.8056\n",
      "1 9 1 0 2 - 0.5833\n",
      "1 9 1 1 1 - 0.7222\n",
      "1 9 1 1 2 - 0.6389\n",
      "1 10 0 0 1 - 0.8056\n",
      "1 10 0 0 2 - 0.6389\n",
      "1 10 0 1 1 - 0.8056\n",
      "1 10 0 1 2 - 0.6389\n",
      "1 10 1 0 1 - 0.8056\n",
      "1 10 1 0 2 - 0.6389\n",
      "1 10 1 1 1 - 0.7222\n",
      "1 10 1 1 2 - 0.6944\n",
      "1 11 0 0 1 - 0.8056\n",
      "1 11 0 0 2 - 0.6389\n",
      "1 11 0 1 1 - 0.8056\n",
      "1 11 0 1 2 - 0.6389\n",
      "1 11 1 0 1 - 0.8056\n",
      "1 11 1 0 2 - 0.6389\n",
      "1 11 1 1 1 - 0.7222\n",
      "1 11 1 1 2 - 0.6944\n",
      "1 12 0 0 1 - 0.8056\n",
      "1 12 0 0 2 - 0.6389\n",
      "1 12 0 1 1 - 0.8056\n",
      "1 12 0 1 2 - 0.6389\n",
      "1 12 1 0 1 - 0.8056\n",
      "1 12 1 0 2 - 0.6389\n",
      "1 12 1 1 1 - 0.7222\n",
      "1 12 1 1 2 - 0.6944\n",
      "1 13 0 0 1 - 0.8056\n",
      "1 13 0 0 2 - 0.6389\n",
      "1 13 0 1 1 - 0.8056\n",
      "1 13 0 1 2 - 0.6389\n",
      "1 13 1 0 1 - 0.8056\n",
      "1 13 1 0 2 - 0.6389\n",
      "1 13 1 1 1 - 0.7222\n",
      "1 13 1 1 2 - 0.6944\n",
      "1 14 0 0 1 - 0.8056\n",
      "1 14 0 0 2 - 0.6389\n",
      "1 14 0 1 1 - 0.8056\n",
      "1 14 0 1 2 - 0.6389\n",
      "1 14 1 0 1 - 0.8056\n",
      "1 14 1 0 2 - 0.6389\n",
      "1 14 1 1 1 - 0.7222\n",
      "1 14 1 1 2 - 0.6944\n",
      "1 15 0 0 1 - 0.8056\n",
      "1 15 0 0 2 - 0.5833\n",
      "1 15 0 1 1 - 0.8056\n",
      "1 15 0 1 2 - 0.5833\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e18b73c0799f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                         \u001b[0mtf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                     \u001b[0mtrain_tf_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'texts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                     \u001b[0mtest_tf_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'texts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 839\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 241\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mtoken_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtoken_pattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Task 4: feature engineering\n",
    "for h in range(1,30): # min_df\n",
    "    for i in range(h+5,100): # max_df\n",
    "        for j in range(0,2): # stop_words: k=0 'engl' k=1 none\n",
    "            for k in range(0,2): # k=0 CountVectorizer (count), k=1 TfidfVectorizer (weighed)\n",
    "                for l in range(0,3): # l=0 MultinomialNB, l=1 GaussianNB, l=2 BernoulliNB              \n",
    "\n",
    "                    if j==0 & k==0:\n",
    "                        tf_vectorizer = CountVectorizer(max_df=i/100.0, min_df=h/100.0, stop_words='english', analyzer = \"word\")\n",
    "                    elif j==1 & k==0:\n",
    "                        tf_vectorizer = CountVectorizer(max_df=i/100.0, min_df=h/100.0, analyzer = \"word\")\n",
    "                    elif j==0 & k==1:\n",
    "                        tf_vectorizer = TfidfVectorizer(max_df=i/100.0, min_df=h/100.0, stop_words='english', analyzer = \"word\")\n",
    "                    elif j==1 & k==1:\n",
    "                        tf_vectorizer = TfidfVectorizer(max_df=i/100.0, min_df=h/100.0, analyzer = \"word\")\n",
    "\n",
    "                    train_tf_ = tf_vectorizer.fit_transform(train['texts'].values)\n",
    "                    test_tf_  = tf_vectorizer.transform(test['texts'].values)\n",
    "\n",
    "                    if l==0:\n",
    "                        clf = MultinomialNB()\n",
    "                    elif l==1:\n",
    "                        clf = GaussianNB()\n",
    "                    elif l==2:\n",
    "                        clf = BernoulliNB()\n",
    "\n",
    "                    if l==0 | l==2:\n",
    "                        clf.fit(train_tf_, train['label'])\n",
    "                        print \"%d %d %d %d %d - %.4f\" % (h,i,j,k,l,clf.score(test_tf_, test['label']))\n",
    "                    elif l==1:\n",
    "                        clf.fit(train_tf_.toarray(), train['label'])\n",
    "                        print \"%d %d %d %d %d - %.4f\" % (h,i,j,k,l,clf.score(test_tf_.toarray(), test['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
