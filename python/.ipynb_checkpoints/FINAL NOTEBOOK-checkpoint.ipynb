{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab 'Em By The Posts\n",
    "\n",
    "### Introduction:\n",
    "\n",
    "*describe what we are doing - assumptions - about the data - : Provide an overview of the project goals and the motivation for it. Consider that this will be read by people who did not see your project proposal.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Data\n",
    "\n",
    "#### How it works\n",
    "*description of how it works - just quick high level*\n",
    "\n",
    "A BIG Thank You to minimaxir for the beautiful code with great descriptions\n",
    "\n",
    "##### Source: https://github.com/minimaxir/facebook-page-post-scraper/blob/master/examples/how_to_build_facebook_scraper.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import json\n",
    "import datetime\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# !!!! APP_ID and APP_SECRET !!!!! SHOULD NOT BE POSTED TO GITHUB\n",
    "\n",
    "#APP_ID=\"\"\n",
    "#APP_SECRET=\"\"\n",
    "\n",
    "ACCESS_TOKEN = APP_ID + \"|\" + APP_SECRET\n",
    "\n",
    "# Here you can specify which profile to search for\n",
    "#page_id = 'nytimes'\n",
    "\n",
    "# or do what we did below and have an array of multiple profiles to search for all at once\n",
    "RELEVANT_NEWS_SITES = ['usatoday','wsj','CBSNews','msnbc', 'nytimes','FoxNews','cnn']\n",
    "\n",
    "WORDS_IN_RELEVNT_STATUSES = ['donald','trump','hillary','clinton','electoral','election','president-elect']\n",
    "\n",
    "OLDEST_RELEVANT_DATE = [2015,4,12]\n",
    "MOST_RECENT_RELEVANT_DATE = [2016,11,16]\n",
    "\n",
    "def testFacebookPageData(page_id, access_token):\n",
    "    \n",
    "    # construct the URL string\n",
    "    base = \"https://graph.facebook.com/v2.4\"\n",
    "    node = \"/\" + page_id\n",
    "    parameters = \"/?access_token=%s\" % access_token\n",
    "    url = base + node + parameters\n",
    "    \n",
    "    # retrieve data\n",
    "    req = urllib2.Request(url)\n",
    "    response = urllib2.urlopen(req)\n",
    "    data = json.loads(response.read())\n",
    "    \n",
    "    print json.dumps(data, indent=4, sort_keys=True)\n",
    "\n",
    "def request_until_succeed(url):\n",
    "    req = urllib2.Request(url)\n",
    "    success = False\n",
    "    while success is False:\n",
    "        try: \n",
    "            response = urllib2.urlopen(req)\n",
    "            if response.getcode() == 200:\n",
    "                success = True\n",
    "        except Exception, e:\n",
    "            print e\n",
    "            time.sleep(5)\n",
    "            \n",
    "            print \"Error for URL %s: %s\" % (url, datetime.datetime.now())\n",
    "\n",
    "    return response.read()    \n",
    "\n",
    "\n",
    "def testFacebookPageFeedData(page_id, access_token):\n",
    "    \n",
    "    # construct the URL string\n",
    "    base = \"https://graph.facebook.com/v2.4\"\n",
    "    node = \"/\" + page_id + \"/feed\" # changed\n",
    "    parameters = \"/?access_token=%s\" % access_token\n",
    "    url = base + node + parameters\n",
    "    \n",
    "    # retrieve data\n",
    "    data = json.loads(request_until_succeed(url))\n",
    "    \n",
    "    return data\n",
    "    #return json.dumps(data, indent=4, sort_keys=True)\n",
    "\n",
    "def getFacebookPageFeedData(page_id, access_token, num_statuses):\n",
    "    \n",
    "    # construct the URL string\n",
    "    base = \"https://graph.facebook.com\"\n",
    "    node = \"/\" + page_id + \"/feed\" \n",
    "    parameters = \"/?fields=message,link,created_time,type,name,id,likes.limit(1).summary(true),comments.limit(1).summary(true),shares&limit=%s&access_token=%s\" % (num_statuses, access_token) # changed\n",
    "    url = base + node + parameters\n",
    "    \n",
    "    # retrieve data\n",
    "    data = json.loads(request_until_succeed(url))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def processFacebookPageFeedStatus(status):\n",
    "    \n",
    "    # The status is now a Python dictionary, so for top-level items,\n",
    "    # we can simply call the key.\n",
    "    \n",
    "    # Additionally, some items may not always exist,\n",
    "    # so must check for existence first\n",
    "    \n",
    "    status_id = status['id']\n",
    "    status_message = '' if 'message' not in status.keys() else status['message'].encode('utf-8')\n",
    "    link_name = '' if 'name' not in status.keys() else status['name'].encode('utf-8')\n",
    "    status_type = status['type']\n",
    "    status_link = '' if 'link' not in status.keys() else status['link']\n",
    "    \n",
    "    \n",
    "    # Time needs special care since a) it's in UTC and\n",
    "    # b) it's not easy to use in statistical programs.\n",
    "    \n",
    "    status_published = datetime.datetime.strptime(status['created_time'],'%Y-%m-%dT%H:%M:%S+0000')\n",
    "    status_published = status_published + datetime.timedelta(hours=-5) # EST\n",
    "    status_published = status_published.strftime('%Y-%m-%d %H:%M:%S') # best time format for spreadsheet programs\n",
    "    \n",
    "    # Nested items require chaining dictionary keys.\n",
    "    \n",
    "    num_likes = 0 if 'likes' not in status.keys() else status['likes']['summary']['total_count']\n",
    "    try:\n",
    "        num_comments = 0 if 'comments' not in status.keys() else status['comments']['summary']['total_count']\n",
    "    except KeyError as e:\n",
    "        print e\n",
    "        num_comments = 0\n",
    "    num_shares = 0 if 'shares' not in status.keys() else status['shares']['count']\n",
    "    \n",
    "    # return a tuple of all processed data\n",
    "    return (status_id, status_message, link_name, status_type, status_link,\n",
    "           status_published, num_likes, num_comments, num_shares)\n",
    "\n",
    "def scrapeFacebookPageFeedStatus(page_id, access_token):\n",
    "    with open('%s_facebook_statuses.csv' % page_id, 'wb') as file:\n",
    "        w = csv.writer(file)\n",
    "        w.writerow([\"status_id\", \"status_message\", \"link_name\", \"status_type\", \"status_link\",\n",
    "           \"status_published\", \"num_likes\", \"num_comments\", \"num_shares\"])\n",
    "        \n",
    "        has_next_page = True\n",
    "        num_processed = 0   # keep a count on how many we've processed\n",
    "        scrape_starttime = datetime.datetime.now()\n",
    "        \n",
    "        print \"Scraping %s Facebook Page: %s\\n\" % (page_id, scrape_starttime)\n",
    "        \n",
    "        statuses = getFacebookPageFeedData(page_id, access_token, 100)\n",
    "        \n",
    "        while has_next_page:\n",
    "            for status in statuses['data']:\n",
    "\n",
    "                status_data = processFacebookPageFeedStatus(status)\n",
    "                date = status_data[5].split(\"-\")\n",
    "                year = int(date[0])\n",
    "                month = int(date[1])\n",
    "                day = int(date[2].split()[0])\n",
    "                status=status_data[1]\n",
    "\n",
    "                #print year,month,day\n",
    "                #print \"status:\",status\n",
    "\n",
    "\n",
    "                if datetime.datetime(year,month,day) < datetime.datetime(OLDEST_RELEVANT_DATE[0],OLDEST_RELEVANT_DATE[1],OLDEST_RELEVANT_DATE[2]):\n",
    "                    return\n",
    "                if datetime.datetime(year,month,day) < datetime.datetime(MOST_RECENT_RELEVANT_DATE[0],MOST_RECENT_RELEVANT_DATE[1],MOST_RECENT_RELEVANT_DATE[2]):\n",
    "                    if any(x in status.lower() for x in WORDS_IN_RELEVNT_STATUSES):\n",
    "                        w.writerow(status_data)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                \n",
    "                # output progress occasionally to make sure code is not stalling\n",
    "                num_processed += 1\n",
    "                if num_processed % 1000 == 0:\n",
    "                    print \"%s Statuses Processed: %s\" % (num_processed, datetime.datetime.now())\n",
    "                    \n",
    "            # if there is no next page, we're done.\n",
    "            if 'paging' in statuses.keys():\n",
    "                statuses = json.loads(request_until_succeed(statuses['paging']['next']))\n",
    "            else:\n",
    "                has_next_page = False\n",
    "                \n",
    "        \n",
    "        print \"\\nDone!\\n%s Statuses Processed in %s\" % (num_processed, datetime.datetime.now() - scrape_starttime)\n",
    "\n",
    "'''\n",
    "facebook_json = getFacebookPageFeedData(page_id, ACCESS_TOKEN,100)\n",
    "\n",
    "file_path = \"facebook_sample_data.txt\"\n",
    "with open(file_path, 'a') as outfile:\n",
    "    json.dump(facebook_json, outfile)\n",
    "    outfile.write(\"\\n\")\n",
    "'''\n",
    "\n",
    "'''\n",
    "test_status = getFacebookPageFeedData(page_id, ACCESS_TOKEN, 1)[\"data\"][0]\n",
    "#print json.dumps(test_status, indent=4, sort_keys=True)\n",
    "\n",
    "processed_test_status = processFacebookPageFeedStatus(test_status)\n",
    "print processed_test_status\n",
    "'''\n",
    "\n",
    "for page_id in RELEVANT_NEWS_SITES:\n",
    "    scrapeFacebookPageFeedStatus(page_id, ACCESS_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cs489]",
   "language": "python",
   "name": "conda-env-cs489-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
