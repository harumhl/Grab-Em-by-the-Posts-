{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'One', u'CD'), (u'of', u'IN'), (u'China', u'NNP'), (u\"'s\", u'POS'), (u'first', u'JJ'), (u'female', u'NN'), (u'fighter', u'NN'), (u'pilots', u'NNS'), (u'was', u'VBD'), (u'killed', u'VBN'), (u'in', u'IN'), (u'a', u'DT'), (u'training', u'NN'), (u'accident', u'NN'), (u'according', u'VBG'), (u'to', u'TO'), (u'state-run', u'JJ'), (u'media', u'NNS'), (u'reports\\u2026', u'NN'), (u'https', u'NN'), (u'//t.co/DoEZLme8Cq', u'NN')]\n",
      "\n",
      "[u'china', u'female fighter pilots', u'state-run media reports\\u2026 https']\n",
      "\n",
      "[u'One', u'of', u'China', u\"'s\", u'first', u'female', u'fighter', u'pilots', u'was', u'killed', u'in', u'a', u'training', u'accident', u'according', u'to', u'state-run', u'media', u'reports\\u2026', u'https', u't.co/DoEZLme8Cq']\n"
     ]
    }
   ],
   "source": [
    "# https://textblob.readthedocs.io/en/dev/\n",
    "\n",
    "# How to install TextBlob\n",
    "#     1. pip install -U textblob\n",
    "#     2. python -m textblob.download_corpora\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(dfT['text'][0]) # run json-to-pandas\n",
    "\n",
    "# Part-of-speech Tagging\n",
    "print blob.tags\n",
    "print\n",
    "\n",
    "# Noun Phrase Extraction¶\n",
    "print blob.noun_phrases\n",
    "print \n",
    "\n",
    "# Tokenization\n",
    "print blob.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0166666666667\n",
      "\n",
      "One---One\n",
      "of---of\n",
      "China---China\n",
      "'s---'s\n",
      "first---first\n",
      "female---female\n",
      "fighter---fighter\n",
      "pilots---pilot\n",
      "was---be\n",
      "killed---kill\n",
      "in---in\n",
      "a---a\n",
      "training---train\n",
      "accident---accident\n",
      "according---accord\n",
      "to---to\n",
      "state-run---state-run\n",
      "media---media\n",
      "reports…---reports…\n",
      "https---https\n",
      "t.co/DoEZLme8Cq---t.co/DoEZLme8Cq\n"
     ]
    }
   ],
   "source": [
    "# The subjectivity is a float within the range [0.0, 1.0] \n",
    "# where 0.0 is very objective and 1.0 is very subjective\n",
    "for sentence in blob.sentences:\n",
    "    print sentence.sentiment.polarity\n",
    "print\n",
    "\n",
    "# Lemmatize each word\n",
    "for sentence in blob.sentences:    \n",
    "    for word in sentence.words:\n",
    "        print \"%s---%s\" % (word, word.lemmatize('v')) # 'v' for 'verb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.nltk.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank # to draw a parse tree\n",
    "\n",
    "sentence = dfT['text'][0] # run json-to-pandas\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "# Identify named entities - Make parse tree?\n",
    "# You might need to call nltk.download() and down load some packages\n",
    "entities = nltk.chunk.ne_chunk(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.728\n",
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.7 : 1.0\n",
      "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
      "              astounding = True              pos : neg    =     10.3 : 1.0\n",
      "             fascination = True              pos : neg    =     10.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Example from http://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    " \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    " \n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    " \n",
    "negcutoff = len(negfeats)*3/4\n",
    "poscutoff = len(posfeats)*3/4\n",
    " \n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Supervised Learning w/ manual scripting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task 1: Load the texts\n",
    "import pandas as pd\n",
    "import glob, os         # for reading all .txt files\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import ARDRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2758, 2)\n",
      "(239, 2)\n"
     ]
    }
   ],
   "source": [
    "# Read sample texts\n",
    "sample = pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/Turk/Batch_2606965_batch_results.csv') \n",
    "sample['Answer.sentiment'] = sample['Answer.sentiment'].map({'Positive':1, 'Neutral':0, 'Negative':-1})\n",
    "\n",
    "# Split train and test sets\n",
    "cutoff = int(math.ceil(len(sample.index)*0.92)) # 92%\n",
    "train = pd.DataFrame({'label':sample['Answer.sentiment'][:cutoff], 'texts':sample['Input.content'][:cutoff]})\n",
    "test  = pd.DataFrame({'label':sample['Answer.sentiment'][cutoff:], 'texts':sample['Input.content'][cutoff:]})\n",
    "\n",
    "print train.shape\n",
    "print test.shape\n",
    "#sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 texts  label\n",
      "0    Thousands of demonstrators filled the streets ...      1\n",
      "1    From Barack Obama's 2003 Senate campaign to Hi...      1\n",
      "2    Slovenian-Americans in Cleveland talked about ...      0\n",
      "3    \"We support Donald Trump’s candidacy to be our...      1\n",
      "4    \"Hillary Clinton owned her feminism. She sound...      1\n",
      "5    Conservatives have criticized Donald J. Trump ...     -1\n",
      "6    Breaking News: Donald J. Trump has been electe...      1\n",
      "7    Donald J. Trump and his campaign responded to ...     -1\n",
      "8    With voters mobbing Bernie Sanders at events i...      0\n",
      "9    \"The best presidents are aspirational, urging ...      1\n",
      "10   Watch the key moments from the first one-on-on...      0\n",
      "11   \"We tried election observers. There’s a reason...      0\n",
      "12   A former aide to Hillary Clinton testified beh...      0\n",
      "13   \"If Donald J. Trump wins, he will change what ...      1\n",
      "14   Breaking News: Paul Ryan, the House speaker, t...      0\n",
      "15   Hillary Clinton and Bernie Sanders are locked ...      0\n",
      "16   Here are some of the ways Donald J. Trump coul...      0\n",
      "17   \"At long last, major Republicans are highlight...      1\n",
      "18   Breaking News: Bernie Sanders has won Wisconsi...      1\n",
      "19   Martin Shkreli owns the sole copy of a Wu-Tang...      0\n",
      "20   Times columnist Frank Bruni is talking with Ke...      0\n",
      "21   Donald J. Trump has declined to consider a gro...      0\n",
      "22   Here are the highlights from tonight's preside...      1\n",
      "23   What do the Tea Partiers tell us about Donald ...      0\n",
      "24   These are the policy changes America can expec...      0\n",
      "25   It’s Hillary Clinton's first speech as the off...      0\n",
      "26   Donald Trump and Theresa May were burned in ef...      1\n",
      "27              Hillary Clinton is no longer laughing.      0\n",
      "28   Michael Moore has made an earnest but not very...     -1\n",
      "29   “I hope they have kidnapping insurance,” Donal...      0\n",
      "..                                                 ...    ...\n",
      "813  “This should be a really great moment for a Re...     -1\n",
      "814  Donald J. Trump's visit offered a circuslike g...     -1\n",
      "815  At Donald J. Trump's rallies, \"the press is ro...      0\n",
      "816  President Jimmy Carter said GOP animosity towa...      1\n",
      "817  \"With 83% of voters calling Trump obnoxious, h...     -1\n",
      "818  Donald J. Trump and Hillary Clinton squared of...      0\n",
      "819  Donald J. Trump is the bona fide television se...      1\n",
      "820  \"Sometimes I do go a little bit far,\" Donald J...     -1\n",
      "821  Ivanka Trump has played the ambassador.Donald ...      1\n",
      "822  Can Hillary Clinton make inroads with Bernie S...      0\n",
      "823  If you think that electing Donald J. Trump wou...     -1\n",
      "824  Donald J. Trump was so unnerved, he said one o...      0\n",
      "825  \"President Obama gave the most powerful rebuke...      0\n",
      "826  Women who are conflicted about Hillary Clinton...     -1\n",
      "827  Breaking News: A federal appeals court blocked...     -1\n",
      "828  House Speaker Paul Ryan said he was \"sickened\"...     -1\n",
      "829  The prosecutor who exposed President Bill Clin...      0\n",
      "830  Donald J. Trump may not attend the next Republ...      0\n",
      "831  Because Election Day is almost here: A two-min...      0\n",
      "832  In The New York Times Opinion Section, a repre...      1\n",
      "833  \"P.S. You are a loser,\" Donald J. Trump wrote ...     -1\n",
      "834  What went wrong in Libya and what it tells us ...      0\n",
      "835  The tone of the litigation against Donald J. T...      1\n",
      "836  \"Did Democrats cry wolf so many times before T...     -1\n",
      "837  Donald J. Trump arrived at his golf resort in ...      0\n",
      "838  With no establishment candidate emerging at th...      1\n",
      "839  \"I have great respect for women. Nobody has mo...      0\n",
      "840           Fact checks of the Trump-Clinton debate.      0\n",
      "841  Some voters suggested that just a little attit...      1\n",
      "842  Hillary Clinton has promised to take on the Na...      0\n",
      "\n",
      "[843 rows x 2 columns]\n",
      "(800, 2)\n",
      "(43, 2)\n"
     ]
    }
   ],
   "source": [
    "# temp\n",
    "sample = pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/Turk/Batch_2606965_batch_results.csv') \n",
    "\n",
    "new_index = range(0, len(sample.index)/3)\n",
    "new_sample = pd.DataFrame(index=new_index, columns=['texts', 'label'])\n",
    "\n",
    "text = \"\"\n",
    "#acc_value = 0 # accumulated score per post\n",
    "#num_posts = 0 # number of same text posts\n",
    "pos = 0 # number of 'positive' per post\n",
    "neu = 0 # number of 'neutral' per post\n",
    "neg = 0 # number of 'negative' per post\n",
    "pd_index = 0\n",
    "\n",
    "for index, row in sample.iterrows():\n",
    "    # If this new post is different post, then append what we worked so far\n",
    "    if text != row['Input.content']:\n",
    "        if text != \"\":\n",
    "            if pos >= 2:\n",
    "                new_sample.loc[[pd_index], ['label']] = 1\n",
    "            elif neu >= 2: # could be commented out for pos/neg\n",
    "                new_sample.loc[[pd_index], ['label']] = 0\n",
    "            elif neg >= 2:\n",
    "                new_sample.loc[[pd_index], ['label']] = -1\n",
    "            else: # 1:1:1\n",
    "                #continue # discard\n",
    "                pd_index -= 1 # to keep the same index\n",
    "\n",
    "            #new_sample.loc[[pd_index], ['score']] = acc_value*1.0/num_posts\n",
    "            new_sample.loc[[pd_index], ['texts']] = text\n",
    "            pd_index += 1\n",
    "            \n",
    "        # Assign a new post\n",
    "        text = row['Input.content']\n",
    "        #acc_value = 0\n",
    "        #num_posts = 0\n",
    "        pos = 0\n",
    "        neu = 0\n",
    "        neg = 0\n",
    "        \n",
    "    if row['Answer.sentiment'] == \"Positive\":\n",
    "        #acc_value += 1\n",
    "        #num_posts += 1\n",
    "        pos += 1\n",
    "    elif row['Answer.sentiment'] == \"Neutral\":\n",
    "        #num_posts += 1\n",
    "        neu += 1\n",
    "    elif row['Answer.sentiment'] == \"Negative\":\n",
    "        #acc_value -= 1\n",
    "        #num_posts += 1\n",
    "        neg += 1\n",
    "\n",
    "# not perfect, so we have some extra NaN rows (all should be filled, technically)\n",
    "new_sample = new_sample.dropna()\n",
    "        \n",
    "# change 0.333, 0.666, ... to whole number by x 3\n",
    "#new_sample.loc[:, 'score'] *= 3\n",
    "new_sample['label'] = new_sample['label'].astype('int')\n",
    "    \n",
    "sample = new_sample\n",
    "\n",
    "print sample\n",
    "print train.shape\n",
    "print test.shape\n",
    "cutoff = int(math.ceil(len(sample.index)*0.9)) # 90%\n",
    "train = pd.DataFrame({'label':sample['label'][:cutoff], 'texts':sample['texts'][:cutoff]})\n",
    "test  = pd.DataFrame({'label':sample['label'][cutoff:], 'texts':sample['texts'][cutoff:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 100 0 0 0 \t 0.6064\n",
      "1 100 0 0 1 \t 0.5213\n",
      "1 100 0 0 2 \t 0.5745\n",
      "1 100 0 0 3 \t 0.5213\n",
      "1 100 0 0 4 \t 0.5426\n",
      "1 100 0 0 5 \t -0.1510\n",
      "1 100 0 0 6 \t -0.0083\n",
      "1 100 0 0 7 \t -0.0083\n",
      "1 100 0 0 8 \t -0.0120\n",
      "1 100 0 0 10 \t 0.5319\n",
      "1 100 0 0 11 \t -0.2451\n",
      "1 100 0 0 12 \t 0.4787\n",
      "1 100 0 0 13 \t 0.5745\n",
      "1 100 0 0 14 \t -0.3002\n",
      "1 100 0 0 15 \t 0.5426\n",
      "1 100 0 1 0 \t 0.6064\n",
      "1 100 0 1 1 \t 0.5213\n",
      "1 100 0 1 2 \t 0.5745\n",
      "1 100 0 1 3 \t 0.5213\n",
      "1 100 0 1 4 \t 0.5426\n",
      "1 100 0 1 5 \t -0.1510\n",
      "1 100 0 1 6 \t -0.0083\n",
      "1 100 0 1 7 \t -0.0083\n",
      "1 100 0 1 8 \t -0.0091\n",
      "1 100 0 1 10 \t 0.5426\n",
      "1 100 0 1 11 \t -0.2775\n",
      "1 100 0 1 12 \t 0.4787\n",
      "1 100 0 1 13 \t 0.5745\n",
      "1 100 0 1 14 \t -0.3002\n",
      "1 100 0 1 15 \t 0.5426\n",
      "1 100 1 0 0 \t 0.6064\n",
      "1 100 1 0 1 \t 0.5213\n",
      "1 100 1 0 2 \t 0.5745\n",
      "1 100 1 0 3 \t 0.5213\n",
      "1 100 1 0 4 \t 0.5426\n",
      "1 100 1 0 5 \t -0.1510\n",
      "1 100 1 0 6 \t -0.0083\n",
      "1 100 1 0 7 \t -0.0083\n",
      "1 100 1 0 8 \t -0.0099\n",
      "1 100 1 0 10 \t 0.5638\n",
      "1 100 1 0 11 \t -0.1915\n",
      "1 100 1 0 12 \t 0.4787\n",
      "1 100 1 0 13 \t 0.5745\n",
      "1 100 1 0 14 \t -0.3002\n",
      "1 100 1 0 15 \t 0.5426\n",
      "1 100 1 1 0 \t 0.5319\n",
      "1 100 1 1 1 \t 0.5213\n",
      "1 100 1 1 2 \t 0.5426\n",
      "1 100 1 1 3 \t 0.5532\n",
      "1 100 1 1 4 \t 0.5000\n",
      "1 100 1 1 5 \t -0.1186\n",
      "1 100 1 1 6 \t -0.0083\n",
      "1 100 1 1 7 \t -0.0083\n",
      "1 100 1 1 8 \t 0.0040\n",
      "1 100 1 1 10 \t 0.5532\n",
      "1 100 1 1 11 \t -0.2669\n",
      "1 100 1 1 12 \t 0.4894\n",
      "1 100 1 1 13 \t 0.5106\n",
      "1 100 1 1 14 \t -0.8228\n",
      "1 100 1 1 15 \t 0.5638\n",
      "1 99 0 0 0 \t 0.6064\n",
      "1 99 0 0 1 \t 0.5213\n",
      "1 99 0 0 2 \t 0.5745\n",
      "1 99 0 0 3 \t 0.5213\n",
      "1 99 0 0 4 \t 0.5532\n",
      "1 99 0 0 5 \t -0.1510\n",
      "1 99 0 0 6 \t -0.0083\n",
      "1 99 0 0 7 \t -0.0083\n",
      "1 99 0 0 8 \t -0.0103\n",
      "1 99 0 0 10 \t 0.5319\n",
      "1 99 0 0 11 \t -0.2247\n",
      "1 99 0 0 12 \t 0.4787\n",
      "1 99 0 0 13 \t 0.5745\n",
      "1 99 0 0 14 \t -0.3002\n",
      "1 99 0 0 15 \t 0.5426\n",
      "1 99 0 1 0 \t 0.6064\n",
      "1 99 0 1 1 \t 0.5213\n",
      "1 99 0 1 2 \t 0.5745\n",
      "1 99 0 1 3 \t 0.5213\n",
      "1 99 0 1 4 \t 0.5532\n",
      "1 99 0 1 5 \t -0.1510\n",
      "1 99 0 1 6 \t -0.0083\n",
      "1 99 0 1 7 \t -0.0083\n",
      "1 99 0 1 8 \t -0.0089\n",
      "1 99 0 1 10 \t 0.5213\n",
      "1 99 0 1 11 \t -0.1867\n",
      "1 99 0 1 12 \t 0.4787\n",
      "1 99 0 1 13 \t 0.5745\n",
      "1 99 0 1 14 \t -0.3002\n",
      "1 99 0 1 15 \t 0.5426\n",
      "1 99 1 0 0 \t 0.6064\n",
      "1 99 1 0 1 \t 0.5213\n",
      "1 99 1 0 2 \t 0.5745\n",
      "1 99 1 0 3 \t 0.5213\n",
      "1 99 1 0 4 \t 0.4894\n",
      "1 99 1 0 5 \t -0.1510\n",
      "1 99 1 0 6 \t -0.0083\n",
      "1 99 1 0 7 \t -0.0083\n",
      "1 99 1 0 8 \t -0.0115\n",
      "1 99 1 0 10 \t 0.5532\n",
      "1 99 1 0 11 \t -0.1910\n",
      "1 99 1 0 12 \t 0.4787\n",
      "1 99 1 0 13 \t 0.5745\n",
      "1 99 1 0 14 \t -0.3002\n",
      "1 99 1 0 15 \t 0.5426\n",
      "1 99 1 1 0 \t 0.5319\n",
      "1 99 1 1 1 \t 0.5213\n",
      "1 99 1 1 2 \t 0.5426\n",
      "1 99 1 1 3 \t 0.5532\n",
      "1 99 1 1 4 \t 0.4574\n",
      "1 99 1 1 5 \t -0.1186\n",
      "1 99 1 1 6 \t -0.0083\n",
      "1 99 1 1 7 \t -0.0083\n",
      "1 99 1 1 8 \t 0.0039\n",
      "1 99 1 1 10 \t 0.5532\n",
      "1 99 1 1 11 \t -0.0584\n",
      "1 99 1 1 12 \t 0.4894\n",
      "1 99 1 1 13 \t 0.5106\n",
      "1 99 1 1 14 \t -0.8228\n",
      "1 99 1 1 15 \t 0.5638\n",
      "1 98 0 0 0 \t 0.6064\n",
      "1 98 0 0 1 \t 0.5213\n",
      "1 98 0 0 2 \t 0.5745\n",
      "1 98 0 0 3 \t 0.5213\n",
      "1 98 0 0 4 \t 0.5106\n",
      "1 98 0 0 5 \t -0.1510\n",
      "1 98 0 0 6 \t -0.0083\n",
      "1 98 0 0 7 \t -0.0083\n",
      "1 98 0 0 8 \t -0.0078\n",
      "1 98 0 0 10 \t 0.5426\n",
      "1 98 0 0 11 \t -0.2745\n",
      "1 98 0 0 12 \t 0.4787\n",
      "1 98 0 0 13 \t 0.5745\n",
      "1 98 0 0 14 \t -0.3002\n",
      "1 98 0 0 15 \t 0.5426\n",
      "1 98 0 1 0 \t 0.6064\n",
      "1 98 0 1 1 \t 0.5213\n",
      "1 98 0 1 2 \t 0.5745\n",
      "1 98 0 1 3 \t 0.5213\n",
      "1 98 0 1 4 \t 0.5745\n",
      "1 98 0 1 5 \t -0.1510\n",
      "1 98 0 1 6 \t -0.0083\n",
      "1 98 0 1 7 \t -0.0083\n",
      "1 98 0 1 8 \t -0.0076\n",
      "1 98 0 1 10 \t 0.5319\n",
      "1 98 0 1 11 \t -0.2173\n",
      "1 98 0 1 12 \t 0.4787\n",
      "1 98 0 1 13 \t 0.5745\n",
      "1 98 0 1 14 \t -0.3002\n",
      "1 98 0 1 15 \t 0.5426\n",
      "1 98 1 0 0 \t 0.6064\n",
      "1 98 1 0 1 \t 0.5213\n",
      "1 98 1 0 2 \t 0.5745\n",
      "1 98 1 0 3 \t 0.5213\n",
      "1 98 1 0 4 \t 0.5213\n",
      "1 98 1 0 5 \t -0.1510\n",
      "1 98 1 0 6 \t -0.0083\n",
      "1 98 1 0 7 \t -0.0083\n",
      "1 98 1 0 8 \t -0.0097\n",
      "1 98 1 0 10 \t 0.5319\n",
      "1 98 1 0 11 \t -0.3604\n",
      "1 98 1 0 12 \t 0.4787\n",
      "1 98 1 0 13 \t 0.5745\n",
      "1 98 1 0 14 \t -0.3002\n",
      "1 98 1 0 15 \t 0.5426\n",
      "1 98 1 1 0 \t 0.5319\n",
      "1 98 1 1 1 \t 0.5213\n",
      "1 98 1 1 2 \t 0.5426\n",
      "1 98 1 1 3 \t 0.5532\n",
      "1 98 1 1 4 \t 0.5106\n",
      "1 98 1 1 5 \t -0.1186\n",
      "1 98 1 1 6 \t -0.0083\n",
      "1 98 1 1 7 \t -0.0083\n",
      "1 98 1 1 8 \t 0.0042\n",
      "1 98 1 1 10 \t 0.5532\n",
      "1 98 1 1 11 \t -0.1905\n",
      "1 98 1 1 12 \t 0.4894\n",
      "1 98 1 1 13 \t 0.5106\n",
      "1 98 1 1 14 \t -0.8228\n",
      "1 98 1 1 15 \t 0.5638\n",
      "1 97 0 0 0 \t 0.6064\n",
      "1 97 0 0 1 \t 0.5213\n",
      "1 97 0 0 2 \t 0.5745\n",
      "1 97 0 0 3 \t 0.5213\n",
      "1 97 0 0 4 \t 0.6383\n",
      "1 97 0 0 5 \t -0.1510\n",
      "1 97 0 0 6 \t -0.0083\n",
      "1 97 0 0 7 \t -0.0083\n",
      "1 97 0 0 8 \t -0.0088\n",
      "1 97 0 0 10 \t 0.5213\n",
      "1 97 0 0 11 \t -0.2080\n",
      "1 97 0 0 12 \t 0.4787\n",
      "1 97 0 0 13 \t 0.5745\n",
      "1 97 0 0 14 \t -0.3002\n",
      "1 97 0 0 15 \t 0.5426\n",
      "1 97 0 1 0 \t 0.6064\n",
      "1 97 0 1 1 \t 0.5213\n",
      "1 97 0 1 2 \t 0.5745\n",
      "1 97 0 1 3 \t 0.5213\n",
      "1 97 0 1 4 \t 0.5638\n",
      "1 97 0 1 5 \t -0.1510\n",
      "1 97 0 1 6 \t -0.0083\n",
      "1 97 0 1 7 \t -0.0083\n",
      "1 97 0 1 8 \t -0.0075\n",
      "1 97 0 1 10 \t 0.5532\n",
      "1 97 0 1 11 \t -0.2560\n",
      "1 97 0 1 12 \t 0.4787\n",
      "1 97 0 1 13 \t 0.5745\n",
      "1 97 0 1 14 \t -0.3002\n",
      "1 97 0 1 15 \t 0.5426\n",
      "1 97 1 0 0 \t 0.6064\n",
      "1 97 1 0 1 \t 0.5213\n",
      "1 97 1 0 2 \t 0.5745\n",
      "1 97 1 0 3 \t 0.5213\n",
      "1 97 1 0 4 \t 0.5106\n",
      "1 97 1 0 5 \t -0.1510\n",
      "1 97 1 0 6 \t -0.0083\n",
      "1 97 1 0 7 \t -0.0083\n",
      "1 97 1 0 8 \t -0.0086\n",
      "1 97 1 0 10 \t 0.5319\n",
      "1 97 1 0 11 \t -0.2554\n",
      "1 97 1 0 12 \t 0.4787\n",
      "1 97 1 0 13 \t 0.5745\n",
      "1 97 1 0 14 \t -0.3002\n",
      "1 97 1 0 15 \t 0.5426\n",
      "1 97 1 1 0 \t 0.5319\n",
      "1 97 1 1 1 \t 0.5213\n",
      "1 97 1 1 2 \t 0.5426\n",
      "1 97 1 1 3 \t 0.5532\n",
      "1 97 1 1 4 \t 0.5426\n",
      "1 97 1 1 5 \t -0.1186\n",
      "1 97 1 1 6 \t -0.0083\n",
      "1 97 1 1 7 \t -0.0083\n",
      "1 97 1 1 8 \t 0.0044\n",
      "1 97 1 1 10 \t 0.5532\n",
      "1 97 1 1 11 \t -0.2337\n",
      "1 97 1 1 12 \t 0.4894\n",
      "1 97 1 1 13 \t 0.5106\n",
      "1 97 1 1 14 \t -0.8228\n",
      "1 97 1 1 15 \t 0.5638\n",
      "1 96 0 0 0 \t 0.6064\n",
      "1 96 0 0 1 \t 0.5213\n",
      "1 96 0 0 2 \t 0.5745\n",
      "1 96 0 0 3 \t 0.5213\n",
      "1 96 0 0 4 \t 0.4894\n",
      "1 96 0 0 5 \t -0.1510\n",
      "1 96 0 0 6 \t -0.0083\n",
      "1 96 0 0 7 \t -0.0083\n",
      "1 96 0 0 8 \t -0.0104\n",
      "1 96 0 0 10 \t 0.5319\n",
      "1 96 0 0 11 \t -0.2899\n",
      "1 96 0 0 12 \t 0.4787\n",
      "1 96 0 0 13 \t 0.5745\n",
      "1 96 0 0 14 \t -0.3002\n",
      "1 96 0 0 15 \t 0.5426\n",
      "1 96 0 1 0 \t 0.6064\n",
      "1 96 0 1 1 \t 0.5213\n",
      "1 96 0 1 2 \t 0.5745\n",
      "1 96 0 1 3 \t 0.5213\n",
      "1 96 0 1 4 \t 0.5532\n",
      "1 96 0 1 5 \t -0.1510\n",
      "1 96 0 1 6 \t -0.0083\n",
      "1 96 0 1 7 \t -0.0083\n",
      "1 96 0 1 8 \t -0.0134\n",
      "1 96 0 1 10 \t 0.5213\n",
      "1 96 0 1 11 \t -0.2352\n",
      "1 96 0 1 12 \t 0.4787\n",
      "1 96 0 1 13 \t 0.5745\n",
      "1 96 0 1 14 \t -0.3002\n",
      "1 96 0 1 15 \t 0.5426\n",
      "1 96 1 0 0 \t 0.6064\n",
      "1 96 1 0 1 \t 0.5213\n",
      "1 96 1 0 2 \t 0.5745\n",
      "1 96 1 0 3 \t 0.5213\n",
      "1 96 1 0 4 \t 0.5745\n",
      "1 96 1 0 5 \t -0.1510\n",
      "1 96 1 0 6 \t -0.0083\n",
      "1 96 1 0 7 \t -0.0083\n",
      "1 96 1 0 8 \t -0.0109\n",
      "1 96 1 0 10 \t 0.5319\n",
      "1 96 1 0 11 \t -0.3857\n",
      "1 96 1 0 12 \t 0.4787\n",
      "1 96 1 0 13 \t 0.5745\n",
      "1 96 1 0 14 \t -0.3002\n",
      "1 96 1 0 15 \t 0.5426\n",
      "1 96 1 1 0 \t 0.5319\n",
      "1 96 1 1 1 \t 0.5213\n",
      "1 96 1 1 2 \t 0.5426\n",
      "1 96 1 1 3 \t 0.5532\n",
      "1 96 1 1 4 \t 0.4574\n",
      "1 96 1 1 5 \t -0.1186\n",
      "1 96 1 1 6 \t -0.0083\n",
      "1 96 1 1 7 \t -0.0083\n",
      "1 96 1 1 8 \t 0.0039\n",
      "1 96 1 1 10 \t 0.5106\n",
      "1 96 1 1 11 \t -0.1995\n",
      "1 96 1 1 12 \t 0.4894\n",
      "1 96 1 1 13 \t 0.5106\n",
      "1 96 1 1 14 \t -0.8228\n",
      "1 96 1 1 15 \t 0.5638\n",
      "1 95 0 0 0 \t 0.6064\n",
      "1 95 0 0 1 \t 0.5213\n",
      "1 95 0 0 2 \t 0.5745\n",
      "1 95 0 0 3 \t 0.5213\n",
      "1 95 0 0 4 \t 0.4894\n",
      "1 95 0 0 5 \t -0.1510\n",
      "1 95 0 0 6 \t -0.0083\n",
      "1 95 0 0 7 \t -0.0083\n",
      "1 95 0 0 8 \t -0.0107\n",
      "1 95 0 0 10 \t 0.5319\n",
      "1 95 0 0 11 \t -0.2864\n",
      "1 95 0 0 12 \t 0.4787\n",
      "1 95 0 0 13 \t 0.5745\n",
      "1 95 0 0 14 \t -0.3002\n",
      "1 95 0 0 15 \t 0.5426\n",
      "1 95 0 1 0 \t 0.6064\n",
      "1 95 0 1 1 \t 0.5213\n",
      "1 95 0 1 2 \t 0.5745\n",
      "1 95 0 1 3 \t 0.5213\n",
      "1 95 0 1 4 \t 0.5957\n",
      "1 95 0 1 5 \t -0.1510\n",
      "1 95 0 1 6 \t -0.0083\n",
      "1 95 0 1 7 \t -0.0083\n",
      "1 95 0 1 8 \t -0.0095\n",
      "1 95 0 1 10 \t 0.5319\n",
      "1 95 0 1 11 \t -0.1636\n",
      "1 95 0 1 12 \t 0.4787\n",
      "1 95 0 1 13 \t 0.5745\n",
      "1 95 0 1 14 \t -0.3002\n",
      "1 95 0 1 15 \t 0.5426\n",
      "1 95 1 0 0 \t 0.6064\n",
      "1 95 1 0 1 \t 0.5213\n",
      "1 95 1 0 2 \t 0.5745\n",
      "1 95 1 0 3 \t 0.5213\n",
      "1 95 1 0 4 \t 0.4681\n",
      "1 95 1 0 5 \t -0.1510\n",
      "1 95 1 0 6 \t -0.0083\n",
      "1 95 1 0 7 \t -0.0083\n",
      "1 95 1 0 8 \t -0.0090\n",
      "1 95 1 0 10 \t 0.5426\n",
      "1 95 1 0 11 \t -0.3056\n",
      "1 95 1 0 12 \t 0.4787\n",
      "1 95 1 0 13 \t 0.5745\n",
      "1 95 1 0 14 \t -0.3002\n",
      "1 95 1 0 15 \t 0.5426\n",
      "1 95 1 1 0 \t 0.5319\n",
      "1 95 1 1 1 \t 0.5213\n",
      "1 95 1 1 2 \t 0.5426\n",
      "1 95 1 1 3 \t 0.5532\n",
      "1 95 1 1 4 \t 0.4681\n",
      "1 95 1 1 5 \t -0.1186\n",
      "1 95 1 1 6 \t -0.0083\n",
      "1 95 1 1 7 \t -0.0083\n",
      "1 95 1 1 8 \t 0.0039\n",
      "1 95 1 1 10 \t 0.5426\n",
      "1 95 1 1 11 \t -0.2684\n",
      "1 95 1 1 12 \t 0.4894\n",
      "1 95 1 1 13 \t 0.5106\n",
      "1 95 1 1 14 \t -0.8228\n",
      "1 95 1 1 15 \t 0.5638\n",
      "1 94 0 0 0 \t 0.6064\n",
      "1 94 0 0 1 \t 0.5213\n",
      "1 94 0 0 2 \t 0.5745\n",
      "1 94 0 0 3 \t 0.5213\n",
      "1 94 0 0 4 \t 0.5213\n",
      "1 94 0 0 5 \t -0.1510\n",
      "1 94 0 0 6 \t -0.0083\n",
      "1 94 0 0 7 \t -0.0083\n",
      "1 94 0 0 8 \t -0.0075\n",
      "1 94 0 0 10 \t 0.5532\n",
      "1 94 0 0 11 \t -0.2669\n",
      "1 94 0 0 12 \t 0.4787\n",
      "1 94 0 0 13 \t 0.5745\n",
      "1 94 0 0 14 \t -0.3002\n",
      "1 94 0 0 15 \t 0.5426\n",
      "1 94 0 1 0 \t 0.6064\n",
      "1 94 0 1 1 \t 0.5213\n",
      "1 94 0 1 2 \t 0.5745\n",
      "1 94 0 1 3 \t 0.5213\n",
      "1 94 0 1 4 \t 0.4787\n",
      "1 94 0 1 5 \t -0.1510\n",
      "1 94 0 1 6 \t -0.0083\n",
      "1 94 0 1 7 \t -0.0083\n",
      "1 94 0 1 8 \t -0.0091\n",
      "1 94 0 1 10 \t 0.5319\n",
      "1 94 0 1 11 \t -0.2205\n",
      "1 94 0 1 12 \t 0.4787\n",
      "1 94 0 1 13 \t 0.5745\n",
      "1 94 0 1 14 \t -0.3002\n",
      "1 94 0 1 15 \t 0.5426\n",
      "1 94 1 0 0 \t 0.6064\n",
      "1 94 1 0 1 \t 0.5213\n",
      "1 94 1 0 2 \t 0.5745\n",
      "1 94 1 0 3 \t 0.5213\n",
      "1 94 1 0 4 \t 0.5745\n",
      "1 94 1 0 5 \t -0.1510\n",
      "1 94 1 0 6 \t -0.0083\n",
      "1 94 1 0 7 \t -0.0083\n",
      "1 94 1 0 8 \t -0.0087\n",
      "1 94 1 0 10 \t 0.5532\n",
      "1 94 1 0 11 \t -0.3302\n",
      "1 94 1 0 12 \t 0.4787\n",
      "1 94 1 0 13 \t 0.5745\n",
      "1 94 1 0 14 \t -0.3002\n",
      "1 94 1 0 15 \t 0.5426\n",
      "1 94 1 1 0 \t 0.5319\n",
      "1 94 1 1 1 \t 0.5213\n",
      "1 94 1 1 2 \t 0.5426\n",
      "1 94 1 1 3 \t 0.5532\n",
      "1 94 1 1 4 \t 0.5213\n",
      "1 94 1 1 5 \t -0.1186\n",
      "1 94 1 1 6 \t -0.0083\n",
      "1 94 1 1 7 \t -0.0083\n",
      "1 94 1 1 8 \t 0.0036\n",
      "1 94 1 1 10 \t 0.5532\n",
      "1 94 1 1 11 \t -0.2290\n",
      "1 94 1 1 12 \t 0.4894\n",
      "1 94 1 1 13 \t 0.5106\n",
      "1 94 1 1 14 \t -0.8228\n",
      "1 94 1 1 15 \t 0.5638\n",
      "1 93 0 0 0 \t 0.6064\n",
      "1 93 0 0 1 \t 0.5213\n",
      "1 93 0 0 2 \t 0.5745\n",
      "1 93 0 0 3 \t 0.5213\n",
      "1 93 0 0 4 \t 0.5426\n",
      "1 93 0 0 5 \t -0.1510\n",
      "1 93 0 0 6 \t -0.0083\n",
      "1 93 0 0 7 \t -0.0083\n",
      "1 93 0 0 8 \t -0.0090\n",
      "1 93 0 0 10 \t 0.5532\n",
      "1 93 0 0 11 \t -0.4002\n",
      "1 93 0 0 12 \t 0.4787\n",
      "1 93 0 0 13 \t 0.5745\n",
      "1 93 0 0 14 \t -0.3002\n",
      "1 93 0 0 15 \t 0.5426\n",
      "1 93 0 1 0 \t 0.6064\n",
      "1 93 0 1 1 \t 0.5213\n",
      "1 93 0 1 2 \t 0.5745\n",
      "1 93 0 1 3 \t 0.5213\n",
      "1 93 0 1 4 \t 0.5638\n",
      "1 93 0 1 5 \t -0.1510\n",
      "1 93 0 1 6 \t -0.0083\n",
      "1 93 0 1 7 \t -0.0083\n",
      "1 93 0 1 8 \t -0.0098\n",
      "1 93 0 1 10 \t 0.5426\n",
      "1 93 0 1 11 \t -0.2208\n",
      "1 93 0 1 12 \t 0.4787\n",
      "1 93 0 1 13 \t 0.5745\n",
      "1 93 0 1 14 \t -0.3002\n",
      "1 93 0 1 15 \t 0.5426\n",
      "1 93 1 0 0 \t 0.6064\n",
      "1 93 1 0 1 \t 0.5213\n",
      "1 93 1 0 2 \t 0.5745\n",
      "1 93 1 0 3 \t 0.5213\n",
      "1 93 1 0 4 \t 0.5638\n",
      "1 93 1 0 5 \t -0.1510\n",
      "1 93 1 0 6 \t -0.0083\n",
      "1 93 1 0 7 \t -0.0083\n",
      "1 93 1 0 8 \t -0.0086\n",
      "1 93 1 0 10 \t 0.5532\n",
      "1 93 1 0 11 \t -0.3922\n",
      "1 93 1 0 12 \t 0.4787\n",
      "1 93 1 0 13 \t 0.5745\n",
      "1 93 1 0 14 \t -0.3002\n",
      "1 93 1 0 15 \t 0.5426\n",
      "1 93 1 1 0 \t 0.5319\n",
      "1 93 1 1 1 \t 0.5213\n",
      "1 93 1 1 2 \t 0.5426\n",
      "1 93 1 1 3 \t 0.5532\n",
      "1 93 1 1 4 \t 0.5000\n",
      "1 93 1 1 5 \t -0.1186\n",
      "1 93 1 1 6 \t -0.0083\n",
      "1 93 1 1 7 \t -0.0083\n",
      "1 93 1 1 8 \t 0.0039\n",
      "1 93 1 1 10 \t 0.5213\n",
      "1 93 1 1 11 \t -0.2280\n",
      "1 93 1 1 12 \t 0.4894\n",
      "1 93 1 1 13 \t 0.5106\n",
      "1 93 1 1 14 \t -0.8228\n",
      "1 93 1 1 15 \t 0.5638\n",
      "1 92 0 0 0 \t 0.6064\n",
      "1 92 0 0 1 \t 0.5213\n",
      "1 92 0 0 2 \t 0.5745\n",
      "1 92 0 0 3 \t 0.5213\n",
      "1 92 0 0 4 \t 0.5638\n",
      "1 92 0 0 5 \t -0.1510\n",
      "1 92 0 0 6 \t -0.0083\n",
      "1 92 0 0 7 \t -0.0083\n",
      "1 92 0 0 8 \t -0.0097\n",
      "1 92 0 0 10 \t 0.5213\n",
      "1 92 0 0 11 \t -0.2479\n",
      "1 92 0 0 12 \t 0.4787\n",
      "1 92 0 0 13 \t 0.5745\n",
      "1 92 0 0 14 \t -0.3002\n",
      "1 92 0 0 15 \t 0.5426\n",
      "1 92 0 1 0 \t 0.6064\n",
      "1 92 0 1 1 \t 0.5213\n",
      "1 92 0 1 2 \t 0.5745\n",
      "1 92 0 1 3 \t 0.5213\n",
      "1 92 0 1 4 \t 0.5213\n",
      "1 92 0 1 5 \t -0.1510\n",
      "1 92 0 1 6 \t -0.0083\n",
      "1 92 0 1 7 \t -0.0083\n",
      "1 92 0 1 8 \t -0.0103\n",
      "1 92 0 1 10 \t 0.5638\n",
      "1 92 0 1 11 \t -0.2282\n",
      "1 92 0 1 12 \t 0.4787\n",
      "1 92 0 1 13 \t 0.5745\n",
      "1 92 0 1 14 \t -0.3002\n",
      "1 92 0 1 15 \t 0.5426\n",
      "1 92 1 0 0 \t 0.6064\n",
      "1 92 1 0 1 \t 0.5213\n",
      "1 92 1 0 2 \t 0.5745\n",
      "1 92 1 0 3 \t 0.5213\n",
      "1 92 1 0 4 \t 0.5745\n",
      "1 92 1 0 5 \t -0.1510\n",
      "1 92 1 0 6 \t -0.0083\n",
      "1 92 1 0 7 \t -0.0083\n",
      "1 92 1 0 8 \t -0.0075\n",
      "1 92 1 0 10 \t 0.5532\n",
      "1 92 1 0 11 \t -0.2098\n",
      "1 92 1 0 12 \t 0.4787\n",
      "1 92 1 0 13 \t 0.5745\n",
      "1 92 1 0 14 \t -0.3002\n",
      "1 92 1 0 15 \t 0.5426\n",
      "1 92 1 1 0 \t 0.5319\n",
      "1 92 1 1 1 \t 0.5213\n",
      "1 92 1 1 2 \t 0.5426\n",
      "1 92 1 1 3 \t 0.5532\n",
      "1 92 1 1 4 \t 0.4787\n",
      "1 92 1 1 5 \t -0.1186\n",
      "1 92 1 1 6 \t -0.0083\n",
      "1 92 1 1 7 \t -0.0083\n",
      "1 92 1 1 8 \t 0.0046\n",
      "1 92 1 1 10 \t 0.5426\n",
      "1 92 1 1 11 \t -0.2421\n",
      "1 92 1 1 12 \t 0.4894\n",
      "1 92 1 1 13 \t 0.5106\n",
      "1 92 1 1 14 \t -0.8228\n",
      "1 92 1 1 15 \t 0.5638\n",
      "1 91 0 0 0 \t 0.6064\n",
      "1 91 0 0 1 \t 0.5213\n",
      "1 91 0 0 2 \t 0.5745\n",
      "1 91 0 0 3 \t 0.5213\n",
      "1 91 0 0 4 \t 0.5319\n",
      "1 91 0 0 5 \t -0.1510\n",
      "1 91 0 0 6 \t -0.0083\n",
      "1 91 0 0 7 \t -0.0083\n",
      "1 91 0 0 8 \t -0.0106\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-2ed7780cfe92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m7\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m9\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tf_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"%d %d %d %d %d \\t %.4f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tf_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[0;32m-> 1028\u001b[0;31m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1029\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1081\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[1;32m   1082\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m                                      X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    795\u001b[0m                 loss.update_terminal_regions(tree.tree_, X, y, residual, y_pred,\n\u001b[1;32m    796\u001b[0m                                              \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m                                              self.learning_rate, k=k)\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0;31m# add tree to ensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mupdate_terminal_regions\u001b[0;34m(self, tree, X, y, residual, y_pred, sample_weight, sample_mask, learning_rate, k)\u001b[0m\n\u001b[1;32m    250\u001b[0m             self._update_terminal_region(tree, masked_terminal_regions,\n\u001b[1;32m    251\u001b[0m                                          \u001b[0mleaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                                          y_pred[:, k], sample_weight)\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;31m# update predictions (both in-bag and out-of-bag)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_update_terminal_region\u001b[0;34m(self, tree, terminal_regions, leaf, X, y, residual, pred, sample_weight)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0mnumerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0mdenominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdenominator\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Task 4: feature engineering\n",
    "for h in range(1,31): # min_df from 0.10 to 0.30\n",
    "    for i in range(100,69,-1): # max_df from 1.00 to 0.70\n",
    "        for j in range(0,2): # stop_words: k=0 'engl' k=1 none\n",
    "            for k in range(0,2): # k=0 CountVectorizer (count), k=1 TfidfVectorizer (weighed)\n",
    "                for l in range(0,16): # l=0 MultinomialNB, l=1 GaussianNB, l=2 BernoulliNB              \n",
    "                                      # l=3 LogisticRegression, l=4 SGDClassifier, l=5 Ridge\n",
    "                                      # l=6 ElasticNet, l=7 LassoLars, l=8 SGDRegressor\n",
    "                                      # l=9 ARDRegression, l=10 GradientBoostingClassifier, l=11 RandomForestRegressor\n",
    "                                      # l=12 SVC, l=13 LinearSVC, l=14 SVR, l=15 KNeighborsClassifier \n",
    "\n",
    "                    if j==0 & k==0:\n",
    "                        tf_vectorizer = CountVectorizer(max_df=i/100.0, min_df=h/100.0, stop_words='english')\n",
    "                    elif j==1 & k==0:\n",
    "                        tf_vectorizer = CountVectorizer(max_df=i/100.0, min_df=h/100.0)\n",
    "                    elif j==0 & k==1:\n",
    "                        tf_vectorizer = TfidfVectorizer(max_df=i/100.0, min_df=h/100.0, stop_words='english')\n",
    "                    elif j==1 & k==1:\n",
    "                        tf_vectorizer = TfidfVectorizer(max_df=i/100.0, min_df=h/100.0)\n",
    "\n",
    "                    train_tf_ = tf_vectorizer.fit_transform(train['texts'].values)\n",
    "                    test_tf_  = tf_vectorizer.transform(test['texts'].values)\n",
    "\n",
    "                    if l==0:\n",
    "                        clf = MultinomialNB()\n",
    "                    elif l==1:\n",
    "                        clf = GaussianNB()\n",
    "                    elif l==2:\n",
    "                        clf = BernoulliNB()\n",
    "                    elif l==3:\n",
    "                        clf = LogisticRegression()\n",
    "                    elif l==4:\n",
    "                        clf = SGDClassifier()\n",
    "                    elif l==5:\n",
    "                        clf = Ridge()\n",
    "                    elif l==6:\n",
    "                        clf = ElasticNet()\n",
    "                    elif l==7:\n",
    "                        clf = LassoLars()\n",
    "                    elif l==8:\n",
    "                        clf = SGDRegressor()\n",
    "                    elif l==9:\n",
    "                        continue #skip - takes too long\n",
    "                        clf = ARDRegression()\n",
    "                    elif l==10:\n",
    "                        clf = GradientBoostingClassifier()\n",
    "                    elif l==11:\n",
    "                        clf = RandomForestRegressor()\n",
    "                    elif l==12:\n",
    "                        clf = SVC()\n",
    "                    elif l==13:\n",
    "                        clf = LinearSVC()\n",
    "                    elif l==14:\n",
    "                        clf = SVR()\n",
    "                    elif l==15:\n",
    "                        clf = KNeighborsClassifier()\n",
    "                    \n",
    "                    if l==1 or l==5 or l==7 or l==9 or l==10:\n",
    "                        clf.fit(train_tf_.toarray(), train['label'])\n",
    "                        print \"%d %d %d %d %d \\t %.4f\" % (h,i,j,k,l,clf.score(test_tf_.toarray(), test['label']))\n",
    "                    else:\n",
    "                        clf.fit(train_tf_, train['label'])\n",
    "                        print \"%d %d %d %d %d \\t %.4f\" % (h,i,j,k,l,clf.score(test_tf_, test['label']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 20,000+ possibilities and running 8000+ cases, <br>\n",
    "It never hit 0.7.. ->> For train:test = 1:1 size\n",
    "\n",
    "Train:test = 10:1 size. I do get up to 0.8+ <br>\n",
    "Now we also have up to (more reasonable) 14400 cases (all expected to run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning w/ TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  20%|██        | 15/75 [01:27<09:55,  9.92s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.730170602345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  39%|███▊      | 29/75 [06:32<13:10, 17.18s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.730170602345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  60%|██████    | 45/75 [13:02<10:46, 21.53s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.739191581365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  79%|███████▊  | 59/75 [19:13<07:42, 28.88s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.739191581365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: DecisionTreeClassifier(Nystroem(input_matrix, 19, 0.53000000000000003, 24))\n",
      "0.694951472663\n",
      "0 1 90 0 5 4 3 9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  20%|██        | 15/75 [02:29<17:58, 17.98s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.752644927536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  41%|████▏     | 31/75 [06:50<14:50, 20.25s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.752644927536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  59%|█████▊    | 44/75 [06:55<01:26,  2.79s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.752644927536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  81%|████████▏ | 61/75 [07:00<00:03,  4.00pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.752644927536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   0%|          | 0/100 [00:00<?, ?pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: DecisionTreeClassifier(input_matrix)\n",
      "0.672018348624\n",
      "0 1 90 0 5 4 3 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  20%|██        | 20/100 [02:51<16:01, 12.02s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.763276972625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  42%|████▏     | 42/100 [08:27<17:21, 17.96s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.763276972625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  59%|█████▉    | 59/100 [08:31<00:55,  1.35s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.763276972625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  83%|████████▎ | 83/100 [09:39<00:08,  2.00pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.763276972625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   0%|          | 0/100 [00:00<?, ?pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: DecisionTreeClassifier(input_matrix)\n",
      "0.692401960784\n",
      "0 1 90 0 5 4 4 9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  21%|██        | 21/100 [00:01<00:07, 11.09pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.72766798419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  38%|███▊      | 38/100 [00:03<00:05, 10.74pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.72766798419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  57%|█████▋    | 57/100 [00:06<00:09,  4.66pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.732213438735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  81%|████████  | 81/100 [00:54<00:10,  1.78pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.732213438735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   1%|          | 1/90 [00:00<00:09,  9.71pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: LinearSVC(Binarizer(input_matrix, 0.34000000000000002), 0.42999999999999999, 49, True)\n",
      "0.729801604052\n",
      "0 1 90 0 5 4 4 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  18%|█▊        | 16/90 [01:02<03:20,  2.71s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.69483226918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  37%|███▋      | 33/90 [03:04<04:45,  5.01s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.69483226918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  54%|█████▍    | 49/90 [03:05<00:20,  1.97pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.69483226918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  69%|██████▉   | 62/90 [03:10<00:12,  2.19pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.69483226918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  84%|████████▍ | 76/90 [03:11<00:01,  9.49pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 0.69483226918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   0%|          | 0/90 [00:00<?, ?pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: DecisionTreeClassifier(RBFSampler(input_matrix, 0.55000000000000004))\n",
      "0.730289757412\n",
      "0 1 90 0 5 5 3 9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  19%|█▉        | 17/90 [01:05<04:29,  3.69s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.728938923395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  37%|███▋      | 33/90 [03:41<08:59,  9.47s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.731247412008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  51%|█████     | 46/90 [03:42<01:12,  1.65s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.731247412008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  64%|██████▍   | 58/90 [03:43<00:08,  3.81pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.731247412008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  84%|████████▍ | 76/90 [04:33<00:17,  1.26s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 0.731247412008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   1%|          | 1/120 [00:00<00:12,  9.31pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: DecisionTreeClassifier(input_matrix)\n",
      "0.695038746631\n",
      "0 1 90 0 5 5 3 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  17%|█▋        | 20/120 [02:38<13:29,  8.10s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.732443329617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  32%|███▎      | 39/120 [04:05<09:01,  6.68s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.732443329617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  48%|████▊     | 57/120 [07:54<11:35, 11.03s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.732443329617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  64%|██████▍   | 77/120 [13:36<12:31, 17.48s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.732443329617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  82%|████████▏ | 98/120 [18:51<10:07, 27.63s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 0.732443329617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: RandomForestClassifier(input_matrix)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Optimization Progress:   0%|          | 0/120 [00:00<?, ?pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.761808367072\n",
      "0 1 90 0 5 5 4 9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  18%|█▊        | 21/120 [00:44<03:43,  2.26s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.710939676266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  32%|███▏      | 38/120 [04:22<13:57, 10.21s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.710939676266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  49%|████▉     | 59/120 [04:30<00:23,  2.62pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.710939676266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  64%|██████▍   | 77/120 [04:40<00:16,  2.56pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.710939676266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  82%|████████▏ | 98/120 [04:48<00:12,  1.82pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 0.710939676266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   0%|          | 0/75 [00:00<?, ?pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: LinearSVC(input_matrix, 9.0, 38, True)\n",
      "0.691016324381\n",
      "0 1 90 0 5 5 4 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  21%|██▏       | 16/75 [01:18<04:24,  4.49s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.712237583205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  40%|████      | 30/75 [05:27<18:44, 24.99s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.712237583205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  59%|█████▊    | 44/75 [13:02<17:29, 33.84s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.712237583205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  77%|███████▋  | 58/75 [16:20<03:48, 13.44s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.712237583205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: ExtraTreesClassifier(Binarizer(input_matrix, 0.27000000000000002), 14, 0.94000000000000006)\n",
      "0.697577194022\n",
      "0 1 90 0 6 4 3 9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  21%|██▏       | 16/75 [03:25<20:30, 20.85s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.75966951567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  39%|███▊      | 29/75 [10:00<26:20, 34.37s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.75966951567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  60%|██████    | 45/75 [10:07<00:12,  2.38pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.75966951567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  83%|████████▎ | 62/75 [10:09<00:01,  8.96pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.759811965812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   0%|          | 0/100 [00:00<?, ?pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: LogisticRegression(input_matrix, 0.98999999999999999, 5, True)\n",
      "0.71221449851\n",
      "0 1 90 0 6 4 3 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  20%|██        | 20/100 [03:30<07:44,  5.81s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.732525126622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  37%|███▋      | 37/100 [11:04<33:28, 31.88s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.732525126622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  51%|█████     | 51/100 [21:21<44:59, 55.09s/pipeline]"
     ]
    }
   ],
   "source": [
    "#for i in range(0,2): # 'Negative' from sample = -1 or 0\n",
    "#  if i==0:\n",
    "#      sample['Answer.sentiment'] = sample['Answer.sentiment'].map({'Positive':1, 'Negative':-1})\n",
    "#  elif i==1:\n",
    "#      sample['Answer.sentiment'] = sample['Answer.sentiment'].map({'Positive':1, 'Negative':0})\n",
    "\n",
    "for j in range(1,10): # min_df\n",
    "    for k in range(90,100): # max_df\n",
    "        for l in range(0,2): # l=0 CountVectorizer (count), l=1 TfidfVectorizer (weighed)\n",
    "            for m in range(5,10): # train:test = m/10 : (1-m/10) so 50:50 to 90:10\n",
    "                for n in range(4,6): # generation (# of TPOT iteration)\n",
    "                    for p in range(3,5): # pop_size 5p \n",
    "                        for q in range(9,11): # k-fold number\n",
    "                            print \"\"    \n",
    "\n",
    "                            sample = pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/sample_data_results.csv') \n",
    "\n",
    "                            # label ('positive','Negative') ->> (#,#) (e.g. (1,-1) or (1,0))\n",
    "                            sample['Answer.sentiment'] = sample['Answer.sentiment'].map({'Positive':1, 'Neutral':0, 'Negative':-1})\n",
    "\n",
    "                            if l==0:\n",
    "                                tf_vectorizer = CountVectorizer(min_df=j/100.0, max_df=k/100.0)\n",
    "                            elif l==1:\n",
    "                                tf_vectorizer = TfidfVectorizer(min_df=j/100.0, max_df=k/100.0)\n",
    "\n",
    "                            sample_input_tf  = tf_vectorizer.fit_transform(sample['Input.content'].values)\n",
    "\n",
    "                            X_train, X_test, y_train, y_test = train_test_split(sample_input_tf, sample['Answer.sentiment'].values,\n",
    "                                                                train_size=m/10.0, test_size=(1-m/10.0))#, random_state=)\n",
    "\n",
    "                            # Official website example: gen=5, pop_size=20, verbo=2\n",
    "                            tpot = TPOTClassifier(generations=n, population_size=5*p, num_cv_folds=q,\n",
    "                                                  verbosity=2)\n",
    "                            tpot.fit(X_train, y_train)\n",
    "                            print(tpot.score(X_test, y_test))\n",
    "                            print \"%d %d %d %d %d %d %d %d\" % (i,j,k,l,m,n,p,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  18%|█▊        | 21/120 [00:44<02:12,  1.34s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.549951021106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  34%|███▍      | 41/120 [01:25<01:05,  1.21pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.549951021106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  51%|█████     | 61/120 [01:39<00:09,  6.15pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.549951021106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  68%|██████▊   | 82/120 [01:50<00:06,  6.19pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.552475895017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  84%|████████▍ | 101/120 [01:52<00:02,  7.88pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 0.552475895017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   0%|          | 0/120 [00:00<?, ?pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: BernoulliNB(input_matrix, 0.13, 0.37)\n",
      "0.554229052499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  18%|█▊        | 21/120 [01:38<02:35,  1.57s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.554238670933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  35%|███▌      | 42/120 [04:11<11:44,  9.03s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.554238670933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  52%|█████▏    | 62/120 [04:39<00:37,  1.56pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.55534574241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  68%|██████▊   | 82/120 [04:40<00:03, 10.12pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.55534574241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  86%|████████▌ | 103/120 [04:42<00:00, 18.04pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 0.55534574241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: MultinomialNB(input_matrix, 0.0)\n",
      "0.519631410256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Just to run it once\n",
    "\n",
    "for i in range(0,2): # min_df\n",
    "    if i==0:\n",
    "        tf_vectorizer = CountVectorizer(max_df=0.9, min_df=0.05)\n",
    "    elif i==1:\n",
    "        tf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.05)\n",
    "\n",
    "    sample_input_tf  = tf_vectorizer.fit_transform(sample['texts'].values)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sample_input_tf, sample['label'].values,\n",
    "                                        train_size=0.92, test_size=0.08)#, random_state=)\n",
    "\n",
    "    # Official website example: gen=5, pop_size=20, verbo=2\n",
    "    tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2)\n",
    "    tpot.fit(X_train, y_train)\n",
    "    print(tpot.score(X_test, y_test))\n",
    "    #print \"%d %d %d %d %d %d %d %d\" % (i,j,k,l,m,n,p,q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer, Pos=1 Neg=-1, train:test=75:25, size=872 in TPOT = 0.78 <br>\n",
    "CountVectorizer, Pos=1 Neg=-1, train:test=92:08, size=872 in TPOT = 0.78 <br>\n",
    "CountVectorizer, Pos=1 Neg =0, train:test=92:08, size=872 in TPOT = 0.86 <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "trump donald presidency nigel farage loathsome creature calls brexit leader obama inside national guru security mind change climate experts increasingly\n",
      "Topic #1:\n",
      "trump new york protesters win yorkers database tweets unfair hours info light calling mayor nyc deleted undocumented praise calls denounce\n",
      "Topic #2:\n",
      "rt help trump author historian lady future shyness cnnnewsroom detect melania speaker ryan deportation erecting force planning paul cnnpolitics allies\n",
      "Topic #3:\n",
      "america like americans hope trump promised watching does reid white fear feel tears innocent celebrate nationalists breath says deep everybody\n",
      "Topic #4:\n",
      "trump obamacare pulling going pol rug backing repeal day interview pledge appeared country end needs replace open agreed work democrats\n",
      "Topic #5:\n",
      "trump says kelly denies advance saw megyn debate report book question children gets advising rudy cnnsotu government giuliani jobs lead\n",
      "Topic #6:\n",
      "president cast ballots million unqualified candidates voters nearly 18 lady help trump win historian cities aid rendell fearful rt future\n",
      "Topic #7:\n",
      "election packing resolve american fear presidential express muslims shock start obama calls trump nigel loathsome creature leader brexit supporter farage\n",
      "Topic #8:\n",
      "trump donald elect president life voted clinton air win asranomani won state team administration new campaign protests muslim says force\n",
      "Topic #9:\n",
      "trump donald clinton hillary white people president house paul ryan says elect hate presidential sources just reince priebus chief staff\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   #max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(train['texts'].values)\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                #max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(train['texts'].values)\n",
    "\n",
    "lda = LatentDirichletAllocation(#n_topics=n_topics, \n",
    "                                max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "lda.fit(tf)\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble & Bagging (Bootstrap AGgregating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nope... http://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So text extraction + ..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task 1: Load the texts\n",
    "import pandas as pd\n",
    "import glob, os         # for reading all .txt files\n",
    "import csv\n",
    "import numpy as np\n",
    "from textblob import TextBlob # use kernel Python[Root]\n",
    "\n",
    "cutoff = 436\n",
    "\n",
    "# Read sample texts\n",
    "sample = pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/sample_data_results.csv') \n",
    "\n",
    "# Lemmatize\n",
    "new_sample = pd.DataFrame(columns=(\"Input.content\", \"Answer.sentiment\"))\n",
    "i=0\n",
    "for text in sample['Input.content']:\n",
    "    blob = TextBlob(text)\n",
    "    newtexts = \"\"\n",
    "\n",
    "    for sentence in blob.sentences:\n",
    "        newtext = \"\"\n",
    "#        print sentence.dict\n",
    "\n",
    "        for word in sentence.words:\n",
    "            newtext += \" \" + word.lemmatize('v') # 'v' for 'verb'\n",
    "\n",
    "        newtexts += newtext\n",
    "        new_sample.loc[i] = newtexts\n",
    "        i += 1\n",
    "\n",
    "i=0\n",
    "for answer in sample['Answer.sentiment']: # updating answers\n",
    "    new_sample['Answer.sentiment'].loc[i] = answer\n",
    "    i += 1\n",
    "    \n",
    "sample = new_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.DataFrame({'label':sample['Answer.sentiment'][:cutoff], 'texts':sample['Input.content'][:cutoff]})\n",
    "test  = pd.DataFrame({'label':sample['Answer.sentiment'][cutoff:], 'texts':sample['Input.content'][cutoff:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 6 0 0 1 - 0.7778\n",
      "1 6 0 0 2 - 0.6389\n",
      "1 6 0 1 1 - 0.7778\n",
      "1 6 0 1 2 - 0.6389\n",
      "1 6 1 0 1 - 0.7778\n",
      "1 6 1 0 2 - 0.6389\n",
      "1 6 1 1 1 - 0.7222\n",
      "1 6 1 1 2 - 0.6944\n",
      "1 7 0 0 1 - 0.8056\n",
      "1 7 0 0 2 - 0.6389\n",
      "1 7 0 1 1 - 0.8056\n",
      "1 7 0 1 2 - 0.6389\n",
      "1 7 1 0 1 - 0.8056\n",
      "1 7 1 0 2 - 0.6389\n",
      "1 7 1 1 1 - 0.7222\n",
      "1 7 1 1 2 - 0.6389\n",
      "1 8 0 0 1 - 0.8056\n",
      "1 8 0 0 2 - 0.6389\n",
      "1 8 0 1 1 - 0.8056\n",
      "1 8 0 1 2 - 0.6389\n",
      "1 8 1 0 1 - 0.8056\n",
      "1 8 1 0 2 - 0.6389\n",
      "1 8 1 1 1 - 0.7222\n",
      "1 8 1 1 2 - 0.6389\n",
      "1 9 0 0 1 - 0.8056\n",
      "1 9 0 0 2 - 0.5833\n",
      "1 9 0 1 1 - 0.8056\n",
      "1 9 0 1 2 - 0.5833\n",
      "1 9 1 0 1 - 0.8056\n",
      "1 9 1 0 2 - 0.5833\n",
      "1 9 1 1 1 - 0.7222\n",
      "1 9 1 1 2 - 0.6389\n",
      "1 10 0 0 1 - 0.8056\n",
      "1 10 0 0 2 - 0.6389\n",
      "1 10 0 1 1 - 0.8056\n",
      "1 10 0 1 2 - 0.6389\n",
      "1 10 1 0 1 - 0.8056\n",
      "1 10 1 0 2 - 0.6389\n",
      "1 10 1 1 1 - 0.7222\n",
      "1 10 1 1 2 - 0.6944\n",
      "1 11 0 0 1 - 0.8056\n",
      "1 11 0 0 2 - 0.6389\n",
      "1 11 0 1 1 - 0.8056\n",
      "1 11 0 1 2 - 0.6389\n",
      "1 11 1 0 1 - 0.8056\n",
      "1 11 1 0 2 - 0.6389\n",
      "1 11 1 1 1 - 0.7222\n",
      "1 11 1 1 2 - 0.6944\n",
      "1 12 0 0 1 - 0.8056\n",
      "1 12 0 0 2 - 0.6389\n",
      "1 12 0 1 1 - 0.8056\n",
      "1 12 0 1 2 - 0.6389\n",
      "1 12 1 0 1 - 0.8056\n",
      "1 12 1 0 2 - 0.6389\n",
      "1 12 1 1 1 - 0.7222\n",
      "1 12 1 1 2 - 0.6944\n",
      "1 13 0 0 1 - 0.8056\n",
      "1 13 0 0 2 - 0.6389\n",
      "1 13 0 1 1 - 0.8056\n",
      "1 13 0 1 2 - 0.6389\n",
      "1 13 1 0 1 - 0.8056\n",
      "1 13 1 0 2 - 0.6389\n",
      "1 13 1 1 1 - 0.7222\n",
      "1 13 1 1 2 - 0.6944\n",
      "1 14 0 0 1 - 0.8056\n",
      "1 14 0 0 2 - 0.6389\n",
      "1 14 0 1 1 - 0.8056\n",
      "1 14 0 1 2 - 0.6389\n",
      "1 14 1 0 1 - 0.8056\n",
      "1 14 1 0 2 - 0.6389\n",
      "1 14 1 1 1 - 0.7222\n",
      "1 14 1 1 2 - 0.6944\n",
      "1 15 0 0 1 - 0.8056\n",
      "1 15 0 0 2 - 0.5833\n",
      "1 15 0 1 1 - 0.8056\n",
      "1 15 0 1 2 - 0.5833\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e18b73c0799f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                         \u001b[0mtf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                     \u001b[0mtrain_tf_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'texts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                     \u001b[0mtest_tf_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'texts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 839\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 241\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mtoken_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtoken_pattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Task 4: feature engineering\n",
    "for h in range(1,30): # min_df\n",
    "    for i in range(h+5,100): # max_df\n",
    "        for j in range(0,2): # stop_words: k=0 'engl' k=1 none\n",
    "            for k in range(0,2): # k=0 CountVectorizer (count), k=1 TfidfVectorizer (weighed)\n",
    "                for l in range(0,3): # l=0 MultinomialNB, l=1 GaussianNB, l=2 BernoulliNB              \n",
    "\n",
    "                    if j==0 & k==0:\n",
    "                        tf_vectorizer = CountVectorizer(max_df=i/100.0, min_df=h/100.0, stop_words='english')\n",
    "                    elif j==1 & k==0:\n",
    "                        tf_vectorizer = CountVectorizer(max_df=i/100.0, min_df=h/100.0)\n",
    "                    elif j==0 & k==1:\n",
    "                        tf_vectorizer = TfidfVectorizer(max_df=i/100.0, min_df=h/100.0, stop_words='english')\n",
    "                    elif j==1 & k==1:\n",
    "                        tf_vectorizer = TfidfVectorizer(max_df=i/100.0, min_df=h/100.0)\n",
    "\n",
    "                    train_tf_ = tf_vectorizer.fit_transform(train['texts'].values)\n",
    "                    test_tf_  = tf_vectorizer.transform(test['texts'].values)\n",
    "\n",
    "                    if l==0:\n",
    "                        clf = MultinomialNB()\n",
    "                    elif l==1:\n",
    "                        clf = GaussianNB()\n",
    "                    elif l==2:\n",
    "                        clf = BernoulliNB()\n",
    "\n",
    "                    if l==0 | l==2:\n",
    "                        clf.fit(train_tf_, train['label'])\n",
    "                        print \"%d %d %d %d %d - %.4f\" % (h,i,j,k,l,clf.score(test_tf_, test['label']))\n",
    "                    elif l==1:\n",
    "                        clf.fit(train_tf_.toarray(), train['label'])\n",
    "                        print \"%d %d %d %d %d - %.4f\" % (h,i,j,k,l,clf.score(test_tf_.toarray(), test['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
