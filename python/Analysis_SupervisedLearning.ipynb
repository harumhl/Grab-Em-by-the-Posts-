{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-cd983f85b09e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# run json-to-pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Part-of-speech Tagging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfT' is not defined"
     ]
    }
   ],
   "source": [
    "# https://textblob.readthedocs.io/en/dev/\n",
    "\n",
    "# How to install TextBlob\n",
    "#     1. pip install -U textblob\n",
    "#     2. python -m textblob.download_corpora\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(dfT['text'][0]) # run json-to-pandas\n",
    "\n",
    "# Part-of-speech Tagging\n",
    "print blob.tags\n",
    "print\n",
    "\n",
    "# Noun Phrase Extraction¶\n",
    "print blob.noun_phrases\n",
    "print \n",
    "\n",
    "# Tokenization\n",
    "print blob.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0166666666667\n",
      "\n",
      "One---One\n",
      "of---of\n",
      "China---China\n",
      "'s---'s\n",
      "first---first\n",
      "female---female\n",
      "fighter---fighter\n",
      "pilots---pilot\n",
      "was---be\n",
      "killed---kill\n",
      "in---in\n",
      "a---a\n",
      "training---train\n",
      "accident---accident\n",
      "according---accord\n",
      "to---to\n",
      "state-run---state-run\n",
      "media---media\n",
      "reports…---reports…\n",
      "https---https\n",
      "t.co/DoEZLme8Cq---t.co/DoEZLme8Cq\n"
     ]
    }
   ],
   "source": [
    "# The subjectivity is a float within the range [0.0, 1.0] \n",
    "# where 0.0 is very objective and 1.0 is very subjective\n",
    "for sentence in blob.sentences:\n",
    "    print sentence.sentiment.polarity\n",
    "print\n",
    "\n",
    "# Lemmatize each word\n",
    "for sentence in blob.sentences:    \n",
    "    for word in sentence.words:\n",
    "        print \"%s---%s\" % (word, word.lemmatize('v')) # 'v' for 'verb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.nltk.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b645383ea184>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtreebank\u001b[0m \u001b[0;31m# to draw a parse tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# run json-to-pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfT' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank # to draw a parse tree\n",
    "\n",
    "sentence = dfT['text'][0] # run json-to-pandas\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "# Identify named entities - Make parse tree?\n",
    "# You might need to call nltk.download() and down load some packages\n",
    "entities = nltk.chunk.ne_chunk(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.728\n",
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.7 : 1.0\n",
      "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
      "              astounding = True              pos : neg    =     10.3 : 1.0\n",
      "             fascination = True              pos : neg    =     10.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Example from http://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    " \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    " \n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    " \n",
    "negcutoff = len(negfeats)*3/4\n",
    "poscutoff = len(posfeats)*3/4\n",
    " \n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 665 instances, test on 56 instances\n",
      "accuracy: 0.767857142857\n",
      "Most Informative Features\n",
      "                Carolina = True              pos : neg    =     10.3 : 1.0\n",
      "                 project = True              pos : neg    =      9.1 : 1.0\n",
      "                    love = True              pos : neg    =      8.7 : 1.0\n",
      "                  attack = True              neg : pos    =      8.5 : 1.0\n",
      "                 history = True              pos : neg    =      7.2 : 1.0\n",
      "               community = True              pos : neg    =      7.2 : 1.0\n",
      "                   Kaine = True              pos : neg    =      7.2 : 1.0\n",
      "                     NBC = True              pos : neg    =      7.2 : 1.0\n",
      "         President-elect = True              pos : neg    =      7.2 : 1.0\n",
      "                     Tim = True              pos : neg    =      7.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Example from http://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    " \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "negfeats = [(word_feats(text.split()), 'neg') for text in sample.loc[sample['labels'] == -1]['texts']]\n",
    "#neufeats = [(word_feats(text.split()), 'neu') for text in sample.loc[sample['labels'] == 0]['texts']]\n",
    "posfeats = [(word_feats(text.split()), 'pos') for text in sample.loc[sample['labels'] == 1]['texts']]\n",
    "\n",
    "negcutoff = int(math.ceil(len(negfeats)*0.92)) # 92%\n",
    "#neucutoff = int(math.ceil(len(neufeats)*0.92)) # 92%\n",
    "poscutoff = int(math.ceil(len(posfeats)*0.92)) # 92%\n",
    " \n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]# + neufeats[:neucutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]# + neufeats[neucutoff:]\n",
    "print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "most_informative = {\"lead\":16.7/1, \n",
    "                    \"primary\":8.7/1, \n",
    "                    \"projects\":8.7/1, \n",
    "                    \"voters\":8.1/1, \n",
    "                    \"Ohio\":7.2/1, \n",
    "                    \"NBC\":7.2/1, \n",
    "                    \"President-elect\":7.2/1, \n",
    "                    \"CBS\":7.2/1, \n",
    "                    \"love\":7.2/1, \n",
    "                    \"News\":7.2/1}\n",
    "print sample['texts'].str.count(\"primary\").sum()\n",
    "print sample['texts'].str.count(\"lead\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Supervised Learning w/ manual scripting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task 1: Load the texts\n",
    "import pandas as pd\n",
    "import glob, os         # for reading all .txt files\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.base import ClassifierMixin\n",
    "#from sklearn.dummy import DummyClassifier\n",
    "#from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "#from sklearn.multiclass import OneVsRestClassifier\n",
    "#from sklearn.multiclass import OneVsOneClassifier\n",
    "#from sklearn.multiclass import OutputCodeClassifier\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "#from sklearn.calibration import CalibratedClassifierCV\n",
    "#from sklearn.semi_supervised import LabelPropagation\n",
    "import math\n",
    "import scipy.sparse as sp\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 5)\n",
      "(1402, 4)\n",
      "                                                  texts   sources  labels  \\\n",
      "0     It is unusual for a president's children to be...       cnn       1   \n",
      "1     Just hours after Mitt Romney blasted Donald J....       cbs      -1   \n",
      "2     Hillary Clinton wins the state of Washington, ...       cbs       1   \n",
      "3     \"We have to take the jobs away from other coun...       fox      -1   \n",
      "4     Jessica Leeds, 74, of Manhattan, was one of tw...  usatoday      -1   \n",
      "5     Ash Carter warns Russia its policy in Syria wi...       cbs      -1   \n",
      "6     \"It's Rubio!\" Watch Donald J. Trump use a wate...       fox       0   \n",
      "7     Online, some Trump supporters discuss monitori...  usatoday      -1   \n",
      "8     This mogul once said he's a member of \"the luc...       cnn       1   \n",
      "9     Live: Hillary Clinton speaks at the NAACP Conv...       fox       0   \n",
      "10    BREAKING: Hillary Clinton has chosen Virginia ...       cbs       0   \n",
      "11    The billboard translated reads: \"Donald Trump,...  usatoday      -1   \n",
      "12    How weird was the world of politics in 2015? W...       cnn      -1   \n",
      "13    NEW: A Chicago police spokesperson told CBS Ne...       cbs       0   \n",
      "14    Hillary Clinton \"did some bad things,\" Donald ...       cbs      -1   \n",
      "15    Tim Kaine defended the work of the Clinton Fou...       fox       1   \n",
      "16    Just before Clinton and Trump squared off in w...  usatoday       0   \n",
      "17    Chelsea took to Twitter to announce her second...       cbs       1   \n",
      "18    One adviser described Clinton as obsessed with...  usatoday      -1   \n",
      "19    \"If you look at it as an American, as a human,...       fox      -1   \n",
      "20    A new report shows donors to the Clinton Found...       cbs      -1   \n",
      "21    Donald J. Trump's  list includes six federal a...       wsj       0   \n",
      "22    Sen. Elizabeth Warren launched a blistering at...       cnn      -1   \n",
      "23    Former KKK grand wizard David Duke didn't win ...  usatoday       0   \n",
      "24    \"We have to work to heal the divisions of a lo...       cbs       1   \n",
      "25    For 40 years, Donald J. Trump has been part of...       fox      -1   \n",
      "26    Michelle Obama, President Obama and Joe Biden ...       cbs      -1   \n",
      "27    Paul Ryan: \"I think what Donald Trump just pul...       cbs       1   \n",
      "28    The 10-Point: Gerard Baker on the World Econom...       wsj       0   \n",
      "29    The school secretary wore an orange jumpsuit a...  usatoday       0   \n",
      "...                                                 ...       ...     ...   \n",
      "1372   Donald Trump has this one wish about his weight.  usatoday       0   \n",
      "1373  Hillary Clinton wins the South Dakota Democrat...  usatoday       1   \n",
      "1374  BREAKING: The FBI will review additional email...  usatoday      -1   \n",
      "1375  President Obama's lower court appointments cou...  usatoday       0   \n",
      "1376  When Barack Obama won Ohio in 2008 and 2012, h...  usatoday      -1   \n",
      "1377  Eight years after a hard-fought primary battle...  usatoday       1   \n",
      "1378  Hillary Clinton might rescue Republicans from ...  usatoday      -1   \n",
      "1379  Sen. Bernie Sanders is laying the groundwork f...  usatoday       1   \n",
      "1380  \"Trump may be peaking early. Or it may be the ...  usatoday       1   \n",
      "1381  During the primaries, one of the most common a...  usatoday      -1   \n",
      "1382                    \"Trumpy\" is her biggest seller.  usatoday       0   \n",
      "1383  Bernie Sanders could give Hillary Clinton that...  usatoday       1   \n",
      "1384  Trump has blasted Clinton since she made the \"...  usatoday      -1   \n",
      "1385  Police removed Rose Hamid, who was wearing a h...  usatoday       0   \n",
      "1386  \"Hillary Clinton may be the most corrupt perso...  usatoday      -1   \n",
      "1387  \"I have just one word for Donald Trump: Basta!...  usatoday      -1   \n",
      "1388  Donald J. Trump is no longer committed to suin...  usatoday       0   \n",
      "1389  Trump says \"it would be nice\" if the Republica...  usatoday       1   \n",
      "1390  Mike Pence offered a full-throated endorsement...  usatoday       1   \n",
      "1391  The remarks didn't sit well with Donald J. Trump.  usatoday      -1   \n",
      "1392  Donald J. Trump had a lot to say about Tuesday...  usatoday       0   \n",
      "1393  Do me a favor, Clinton told New York City Mayo...  usatoday       0   \n",
      "1394  \"The American people deserve to get the full a...  usatoday       0   \n",
      "1395  Donald J. Trump called Hillary Clinton a candi...  usatoday      -1   \n",
      "1396  The bill now goes to Gov. Mary Fallin, whose n...  usatoday       0   \n",
      "1397  With a smile on her face  and some help from a...  usatoday       1   \n",
      "1398  Donald J. Trump continues to lead the Republic...  usatoday       0   \n",
      "1399  Donald Trump told 'The Wall Street Journal' th...  usatoday       1   \n",
      "1400  Nothing has made me prouder than to be your ch...  usatoday       1   \n",
      "1401  The latest release includes \"around 150\" email...  usatoday       0   \n",
      "\n",
      "                            statusID  \n",
      "0       5550296508_10155606385896509  \n",
      "1     131459315949_10153427911075950  \n",
      "2     131459315949_10154056641215950  \n",
      "3      15704546335_10153385284896336  \n",
      "4      13652355666_10153933280450667  \n",
      "5     131459315949_10153397324575950  \n",
      "6      15704546335_10154088259801336  \n",
      "7      13652355666_10153959863300667  \n",
      "8       5550296508_10154648040171509  \n",
      "9      15704546335_10154454708126336  \n",
      "10    131459315949_10153757406760950  \n",
      "11     13652355666_10153948842760667  \n",
      "12      5550296508_10154315359966509  \n",
      "13    131459315949_10153447192105950  \n",
      "14    131459315949_10154071618380950  \n",
      "15     15704546335_10154668092296336  \n",
      "16     13652355666_10153932123850667  \n",
      "17    131459315949_10153292327635950  \n",
      "18     13652355666_10153924828260667  \n",
      "19     15704546335_10154713987721336  \n",
      "20    131459315949_10153837579360950  \n",
      "21      8304333127_10154314158863128  \n",
      "22      5550296508_10154898645306509  \n",
      "23     13652355666_10154026471955667  \n",
      "24    131459315949_10154058345805950  \n",
      "25     15704546335_10154103175901336  \n",
      "26    131459315949_10153976233385950  \n",
      "27    131459315949_10154058369590950  \n",
      "28      8304333127_10153976433233128  \n",
      "29     13652355666_10154007889465667  \n",
      "...                              ...  \n",
      "1372   13652355666_10153846997160667  \n",
      "1373   13652355666_10153585700530667  \n",
      "1374   13652355666_10153985134535667  \n",
      "1375   13652355666_10153844454430667  \n",
      "1376   13652355666_10154030994500667  \n",
      "1377   13652355666_10153981680050667  \n",
      "1378   13652355666_10153690408580667  \n",
      "1379   13652355666_10153987891575667  \n",
      "1380   13652355666_10152975490600667  \n",
      "1381   13652355666_10154027073720667  \n",
      "1382   13652355666_10153574967355667  \n",
      "1383   13652355666_10153653526225667  \n",
      "1384   13652355666_10153920173020667  \n",
      "1385   13652355666_10153260556620667  \n",
      "1386   13652355666_10153620528550667  \n",
      "1387   13652355666_10153584498190667  \n",
      "1388   13652355666_10153981193040667  \n",
      "1389   13652355666_10153613555685667  \n",
      "1390   13652355666_10153922104885667  \n",
      "1391   13652355666_10153831940125667  \n",
      "1392   13652355666_10153109194405667  \n",
      "1393   13652355666_10153466063975667  \n",
      "1394   13652355666_10153985983565667  \n",
      "1395   13652355666_10153781627400667  \n",
      "1396   13652355666_10153544883210667  \n",
      "1397   13652355666_10153967236665667  \n",
      "1398   13652355666_10152991654495667  \n",
      "1399    13652355666_1742117792779408  \n",
      "1400    13652355666_1740277859630068  \n",
      "1401   13652355666_10153033465715667  \n",
      "\n",
      "[1402 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# THREE ANSWERS TO ONE\n",
    "sample = pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/Turk/6source_results_filtered.csv') \n",
    "#sample['Answer.sentiment'] = sample['Answer.sentiment'].map({'Positive':1, 'Neutral':0, 'Negative':-1})\n",
    "\n",
    "new_index = range(0, len(sample.index)/3)\n",
    "new_sample = pd.DataFrame(index=new_index, columns=['texts', 'sources', 'labels', 'statusID'])\n",
    "\n",
    "text = \"\"\n",
    "source = \"\"\n",
    "statusID = \"\"\n",
    "#acc_value = 0 # accumulated score per post\n",
    "#num_posts = 0 # number of same text posts\n",
    "pos = 0 # number of 'positive' per post\n",
    "neu = 0 # number of 'neutral' per post\n",
    "neg = 0 # number of 'negative' per post\n",
    "pd_index = 0\n",
    "\n",
    "for index, row in sample.iterrows():\n",
    "    # If this new post is different post, then append what we worked so far\n",
    "    if text != row['Input.content']:\n",
    "        if text != \"\":\n",
    "            if pos >= 2:\n",
    "                new_sample.loc[[pd_index], ['labels']] = 1\n",
    "            elif neu >= 2: # could be commented out for pos/neg\n",
    "                new_sample.loc[[pd_index], ['labels']] = 0\n",
    "            elif neg >= 2:\n",
    "                new_sample.loc[[pd_index], ['labels']] = -1\n",
    "            else: # 1:1:1\n",
    "                #continue # discard\n",
    "                pd_index -= 1 # to keep the same index\n",
    "\n",
    "            #new_sample.loc[[pd_index], ['score']] = acc_value*1.0/num_posts\n",
    "            new_sample.loc[[pd_index], ['sources']] = source\n",
    "            new_sample.loc[[pd_index], ['texts']] = text\n",
    "            new_sample.loc[[pd_index], ['statusID']] = statusID\n",
    "            pd_index += 1\n",
    "            \n",
    "        # Assign a new post\n",
    "        text = row['Input.content']\n",
    "        source = row['Input.source']\n",
    "        statusID = row['Input.statis_id']\n",
    "        #acc_value = 0\n",
    "        #num_posts = 0\n",
    "        pos = 0\n",
    "        neu = 0\n",
    "        neg = 0\n",
    "        \n",
    "    if row['Answer.sentiment'] == \"Positive\":\n",
    "        #acc_value += 1\n",
    "        #num_posts += 1\n",
    "        pos += 1\n",
    "    elif row['Answer.sentiment'] == \"Neutral\":\n",
    "        #num_posts += 1\n",
    "        neu += 1\n",
    "    elif row['Answer.sentiment'] == \"Negative\":\n",
    "        #acc_value -= 1\n",
    "        #num_posts += 1\n",
    "        neg += 1\n",
    "\n",
    "# not perfect, so we have some extra NaN rows (all should be filled, technically)\n",
    "new_sample = new_sample.dropna()\n",
    "        \n",
    "# change 0.333, 0.666, ... to whole number by x 3\n",
    "#new_sample.loc[:, 'score'] *= 3\n",
    "new_sample['labels'] = new_sample['labels'].astype('int')\n",
    "    \n",
    "#new_sample['sources'] = new_sample['sources'].map({'msnbc':4, \n",
    "#                                                   'cnn':3, \n",
    "#                                                   'cbs':2,\n",
    "#                                                   'usatoday':1,\n",
    "#                                                   'wsj':-1,\n",
    "#                                                   'fox':-2})\n",
    "#new_sample['sources'] = new_sample['sources'].astype('int')\n",
    "print sample.shape\n",
    "sample = new_sample\n",
    "\n",
    "#cutoff = int(math.ceil(len(sample.index)*0.9)) # 90%\n",
    "#train = pd.DataFrame({'labels':sample['labels'][:cutoff], 'texts':sample['texts'][:cutoff], 'sources':sample['sources'][:cutoff]})\n",
    "#test  = pd.DataFrame({'labels':sample['labels'][cutoff:], 'texts':sample['texts'][cutoff:], 'sources':sample['sources'][cutoff:]})\n",
    "#print train.shape\n",
    "#print test.shape\n",
    "print sample.shape\n",
    "print sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               texts   sources  labels  \\\n",
      "0  It is unusual for a president's children to be...       cnn       1   \n",
      "1  Just hours after Mitt Romney blasted Donald J....       cbs      -1   \n",
      "2  Hillary Clinton wins the state of Washington, ...       cbs       1   \n",
      "3  \"We have to take the jobs away from other coun...       fox      -1   \n",
      "4  Jessica Leeds, 74, of Manhattan, was one of tw...  usatoday      -1   \n",
      "\n",
      "                         statusID candidates  \n",
      "0    5550296508_10155606385896509      trump  \n",
      "1  131459315949_10153427911075950      trump  \n",
      "2  131459315949_10154056641215950    clinton  \n",
      "3   15704546335_10153385284896336      trump  \n",
      "4   13652355666_10153933280450667      trump  \n"
     ]
    }
   ],
   "source": [
    "# classifying candidates\n",
    "def classify_post (row):\n",
    "   status = row['texts'].lower()\n",
    "   if (('donald' in status) or ('trump' in status)) and (('hillary' in status) or ('clinton' in status)) :\n",
    "      return 'other'\n",
    "   elif ('donald' in status) or ('trump' in status) :\n",
    "      return 'trump'\n",
    "   elif ('hillary' in status) or ('clinton' in status) :\n",
    "      return 'clinton'\n",
    "   return 'other'\n",
    "\n",
    "sample['candidates'] = sample.apply (lambda row: classify_post (row),axis=1)\n",
    "\n",
    "print sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Haru/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1402, 6)\n",
      "(1402, 6)\n",
      "                                               texts   sources  labels  \\\n",
      "0  It is unusual for a president's children to be...       cnn       1   \n",
      "1  Just hours after Mitt Romney blasted Donald J....       cbs      -1   \n",
      "2  Hillary Clinton wins the state of Washington, ...       cbs       1   \n",
      "3  \"We have to take the jobs away from other coun...       fox      -1   \n",
      "4  Jessica Leeds, 74, of Manhattan, was one of tw...  usatoday      -1   \n",
      "\n",
      "                         statusID candidates                dates  \n",
      "0    5550296508_10155606385896509      trump  2016-11-15 00:00:02  \n",
      "1  131459315949_10153427911075950      trump  2016-03-03 14:44:04  \n",
      "2  131459315949_10154056641215950    clinton  2016-11-09 00:03:03  \n",
      "3   15704546335_10153385284896336      trump  2015-06-23 02:00:01  \n",
      "4   13652355666_10153933280450667      trump  2016-10-13 21:42:41  \n"
     ]
    }
   ],
   "source": [
    "raw = {}\n",
    "raw['cbs'] = pd.read_csv('/Users/Haru/Documents/! college/4. Fall 2016/489Project/Facebook_RAW/CBSNews_facebook_statuses.csv')\n",
    "raw['cnn'] = pd.read_csv('/Users/Haru/Documents/! college/4. Fall 2016/489Project/Facebook_RAW/cnn_facebook_statuses.csv', encoding='utf-8')\n",
    "raw['fox'] = pd.read_csv('/Users/Haru/Documents/! college/4. Fall 2016/489Project/Facebook_RAW/FoxNews_facebook_statuses.csv', encoding='utf-8')\n",
    "raw['msnbc'] = pd.read_csv('/Users/Haru/Documents/! college/4. Fall 2016/489Project/Facebook_RAW/msnbc_facebook_statuses.csv', encoding='utf-8') \n",
    "raw['nyt'] = pd.read_csv('/Users/Haru/Documents/! college/4. Fall 2016/489Project/Facebook_RAW/nytimes_facebook_statuses.csv', encoding='utf-8') \n",
    "raw['usatoday'] = pd.read_csv('/Users/Haru/Documents/! college/4. Fall 2016/489Project/Facebook_RAW/usatoday_facebook_statuses.csv', encoding='utf-8') \n",
    "raw['wsj'] = pd.read_csv('/Users/Haru/Documents/! college/4. Fall 2016/489Project/Facebook_RAW/wsj_facebook_statuses.csv', encoding='utf-8') \n",
    "\n",
    "sample['dates'] = \"\"\n",
    "\n",
    "for index, row in sample.iterrows():\n",
    "    the_source = row['sources']\n",
    "    \n",
    "    raw_index = list(raw[the_source]['status_id']).index(row['statusID'])\n",
    "    sample['dates'][index] = raw[the_source]['status_published'][raw_index]\n",
    "\n",
    "print sample.dropna().shape\n",
    "print sample.shape\n",
    "print sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Haru/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Haru/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>sources</th>\n",
       "      <th>labels</th>\n",
       "      <th>statusID</th>\n",
       "      <th>candidates</th>\n",
       "      <th>dates</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It is unusual for a president's children to be...</td>\n",
       "      <td>cnn</td>\n",
       "      <td>1</td>\n",
       "      <td>5550296508_10155606385896509</td>\n",
       "      <td>trump</td>\n",
       "      <td>2016-11-15 00:00:02</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just hours after Mitt Romney blasted Donald J....</td>\n",
       "      <td>cbs</td>\n",
       "      <td>-1</td>\n",
       "      <td>131459315949_10153427911075950</td>\n",
       "      <td>trump</td>\n",
       "      <td>2016-03-03 14:44:04</td>\n",
       "      <td>2016-03</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hillary Clinton wins the state of Washington, ...</td>\n",
       "      <td>cbs</td>\n",
       "      <td>1</td>\n",
       "      <td>131459315949_10154056641215950</td>\n",
       "      <td>clinton</td>\n",
       "      <td>2016-11-09 00:03:03</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"We have to take the jobs away from other coun...</td>\n",
       "      <td>fox</td>\n",
       "      <td>-1</td>\n",
       "      <td>15704546335_10153385284896336</td>\n",
       "      <td>trump</td>\n",
       "      <td>2015-06-23 02:00:01</td>\n",
       "      <td>2015-06</td>\n",
       "      <td>02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jessica Leeds, 74, of Manhattan, was one of tw...</td>\n",
       "      <td>usatoday</td>\n",
       "      <td>-1</td>\n",
       "      <td>13652355666_10153933280450667</td>\n",
       "      <td>trump</td>\n",
       "      <td>2016-10-13 21:42:41</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts   sources  labels  \\\n",
       "0  It is unusual for a president's children to be...       cnn       1   \n",
       "1  Just hours after Mitt Romney blasted Donald J....       cbs      -1   \n",
       "2  Hillary Clinton wins the state of Washington, ...       cbs       1   \n",
       "3  \"We have to take the jobs away from other coun...       fox      -1   \n",
       "4  Jessica Leeds, 74, of Manhattan, was one of tw...  usatoday      -1   \n",
       "\n",
       "                         statusID candidates                dates     date  \\\n",
       "0    5550296508_10155606385896509      trump  2016-11-15 00:00:02  2016-11   \n",
       "1  131459315949_10153427911075950      trump  2016-03-03 14:44:04  2016-03   \n",
       "2  131459315949_10154056641215950    clinton  2016-11-09 00:03:03  2016-11   \n",
       "3   15704546335_10153385284896336      trump  2015-06-23 02:00:01  2015-06   \n",
       "4   13652355666_10153933280450667      trump  2016-10-13 21:42:41  2016-10   \n",
       "\n",
       "  hour  \n",
       "0   00  \n",
       "1   14  \n",
       "2   00  \n",
       "3   02  \n",
       "4   21  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['date'] = \"\"\n",
    "sample['hour'] = \"\"\n",
    "\n",
    "for index, row in sample.iterrows():\n",
    "    date_obj = datetime.strptime(row['dates'], \"%Y-%m-%d %H:%M:%S\")\n",
    "    sample['date'][index] = date_obj.strftime(\"%Y-%m\")\n",
    "    sample['hour'][index] = date_obj.strftime(\"%H\")\n",
    "\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  texts   sources  labels  \\\n",
      "0      It be unusual for a president 's children to ...       cnn       1   \n",
      "1      Just hours after Mitt Romney blast Donald J T...       cbs      -1   \n",
      "2      Hillary Clinton win the state of Washington C...       cbs       1   \n",
      "3      We have to take the job away from other count...       fox      -1   \n",
      "4      Jessica Leeds 74 of Manhattan be one of two w...  usatoday      -1   \n",
      "5      Ash Carter warn Russia its policy in Syria wi...       cbs      -1   \n",
      "6      It 's Rubio Watch Donald J Trump use a water ...       fox       0   \n",
      "7      Online some Trump supporters discuss monitor ...  usatoday      -1   \n",
      "8      This mogul once say he 's a member of the luc...       cnn       1   \n",
      "9      Live Hillary Clinton speak at the NAACP Conve...       fox       0   \n",
      "10     BREAKING Hillary Clinton have choose Virginia...       cbs       0   \n",
      "11     The billboard translate read Donald Trump he ...  usatoday      -1   \n",
      "12     How weird be the world of politics in 2015 We...       cnn      -1   \n",
      "13     NEW A Chicago police spokesperson tell CBS Ne...       cbs       0   \n",
      "14     Hillary Clinton do some bad things Donald J T...       cbs      -1   \n",
      "15     Tim Kaine defend the work of the Clinton Foun...       fox       1   \n",
      "16     Just before Clinton and Trump square off in w...  usatoday       0   \n",
      "17     Chelsea take to Twitter to announce her secon...       cbs       1   \n",
      "18     One adviser describe Clinton as obsess with J...  usatoday      -1   \n",
      "19     If you look at it as an American as a human a...       fox      -1   \n",
      "20     A new report show donors to the Clinton Found...       cbs      -1   \n",
      "21     Donald J Trump 's list include six federal ap...       wsj       0   \n",
      "22     Sen Elizabeth Warren launch a blister attack ...       cnn      -1   \n",
      "23     Former KKK grand wizard David Duke do n't win...  usatoday       0   \n",
      "24     We have to work to heal the divisions of a lo...       cbs       1   \n",
      "25     For 40 years Donald J Trump have be part of t...       fox      -1   \n",
      "26     Michelle Obama President Obama and Joe Biden ...       cbs      -1   \n",
      "27     Paul Ryan I think what Donald Trump just pull...       cbs       1   \n",
      "28     The 10-Point Gerard Baker on the World Econom...       wsj       0   \n",
      "29     The school secretary wear an orange jumpsuit ...  usatoday       0   \n",
      "...                                                 ...       ...     ...   \n",
      "1372   Donald Trump have this one wish about his weight  usatoday       0   \n",
      "1373   Hillary Clinton win the South Dakota Democrat...  usatoday       1   \n",
      "1374   BREAKING The FBI will review additional email...  usatoday      -1   \n",
      "1375   President Obama 's lower court appointments c...  usatoday       0   \n",
      "1376   When Barack Obama win Ohio in 2008 and 2012 h...  usatoday      -1   \n",
      "1377   Eight years after a hard-fought primary battl...  usatoday       1   \n",
      "1378   Hillary Clinton might rescue Republicans from...  usatoday      -1   \n",
      "1379   Sen Bernie Sanders be lay the groundwork for ...  usatoday       1   \n",
      "1380   Trump may be peak early Or it may be the begi...  usatoday       1   \n",
      "1381   During the primaries one of the most common a...  usatoday      -1   \n",
      "1382                       Trumpy be her biggest seller  usatoday       0   \n",
      "1383   Bernie Sanders could give Hillary Clinton tha...  usatoday       1   \n",
      "1384   Trump have blast Clinton since she make the d...  usatoday      -1   \n",
      "1385   Police remove Rose Hamid who be wear a hijab ...  usatoday       0   \n",
      "1386   Hillary Clinton may be the most corrupt perso...  usatoday      -1   \n",
      "1387   I have just one word for Donald Trump Basta E...  usatoday      -1   \n",
      "1388   Donald J Trump be no longer commit to sue the...  usatoday       0   \n",
      "1389   Trump say it would be nice if the Republican ...  usatoday       1   \n",
      "1390   Mike Pence offer a full-throated endorsement ...  usatoday       1   \n",
      "1391     The remark do n't sit well with Donald J Trump  usatoday      -1   \n",
      "1392   Donald J Trump have a lot to say about Tuesda...  usatoday       0   \n",
      "1393   Do me a favor Clinton tell New York City Mayo...  usatoday       0   \n",
      "1394   The American people deserve to get the full a...  usatoday       0   \n",
      "1395   Donald J Trump call Hillary Clinton a candida...  usatoday      -1   \n",
      "1396   The bill now go to Gov Mary Fallin whose name...  usatoday       0   \n",
      "1397   With a smile on her face and some help from a...  usatoday       1   \n",
      "1398   Donald J Trump continue to lead the Republica...  usatoday       0   \n",
      "1399   Donald Trump tell 'The Wall Street Journal th...  usatoday       1   \n",
      "1400   Nothing have make me prouder than to be your ...  usatoday       1   \n",
      "1401   The latest release include around 150 email t...  usatoday       0   \n",
      "\n",
      "                            statusID candidates                dates     date  \\\n",
      "0       5550296508_10155606385896509      trump  2016-11-15 00:00:02  2016-11   \n",
      "1     131459315949_10153427911075950      trump  2016-03-03 14:44:04  2016-03   \n",
      "2     131459315949_10154056641215950    clinton  2016-11-09 00:03:03  2016-11   \n",
      "3      15704546335_10153385284896336      trump  2015-06-23 02:00:01  2015-06   \n",
      "4      13652355666_10153933280450667      trump  2016-10-13 21:42:41  2016-10   \n",
      "5     131459315949_10153397324575950    clinton  2016-02-17 07:39:27  2016-02   \n",
      "6      15704546335_10154088259801336      trump  2016-02-26 14:00:14  2016-02   \n",
      "7      13652355666_10153959863300667      trump  2016-10-20 22:41:48  2016-10   \n",
      "8       5550296508_10154648040171509      trump  2016-04-01 16:28:02  2016-04   \n",
      "9      15704546335_10154454708126336    clinton  2016-07-18 11:42:36  2016-07   \n",
      "10    131459315949_10153757406760950    clinton  2016-07-22 19:17:14  2016-07   \n",
      "11     13652355666_10153948842760667      trump  2016-10-17 22:00:00  2016-10   \n",
      "12      5550296508_10154315359966509      trump  2015-12-22 07:22:01  2015-12   \n",
      "13    131459315949_10153447192105950      trump  2016-03-11 22:40:47  2016-03   \n",
      "14    131459315949_10154071618380950      other  2016-11-13 19:15:54  2016-11   \n",
      "15     15704546335_10154668092296336    clinton  2016-10-04 21:56:11  2016-10   \n",
      "16     13652355666_10153932123850667      other  2016-10-13 16:00:00  2016-10   \n",
      "17    131459315949_10153292327635950    clinton  2015-12-21 16:28:32  2015-12   \n",
      "18     13652355666_10153924828260667    clinton  2016-10-11 11:43:00  2016-10   \n",
      "19     15704546335_10154713987721336      trump  2016-10-20 00:06:02  2016-10   \n",
      "20    131459315949_10153837579360950    clinton  2016-08-24 08:30:05  2016-08   \n",
      "21      8304333127_10154314158863128      trump  2016-05-18 16:40:02  2016-05   \n",
      "22      5550296508_10154898645306509      trump  2016-06-09 19:31:23  2016-06   \n",
      "23     13652355666_10154026471955667      other  2016-11-09 01:40:00  2016-11   \n",
      "24    131459315949_10154058345805950      trump  2016-11-09 11:28:20  2016-11   \n",
      "25     15704546335_10154103175901336      trump  2016-03-03 23:55:33  2016-03   \n",
      "26    131459315949_10153976233385950    clinton  2016-10-14 07:24:03  2016-10   \n",
      "27    131459315949_10154058369590950      trump  2016-11-09 11:35:20  2016-11   \n",
      "28      8304333127_10153976433233128      trump  2016-01-20 08:30:00  2016-01   \n",
      "29     13652355666_10154007889465667    clinton  2016-11-03 20:13:57  2016-11   \n",
      "...                              ...        ...                  ...      ...   \n",
      "1372   13652355666_10153846997160667      trump  2016-09-15 16:30:00  2016-09   \n",
      "1373   13652355666_10153585700530667    clinton  2016-06-07 22:58:39  2016-06   \n",
      "1374   13652355666_10153985134535667    clinton  2016-10-28 12:23:55  2016-10   \n",
      "1375   13652355666_10153844454430667      other  2016-09-14 17:30:00  2016-09   \n",
      "1376   13652355666_10154030994500667      trump  2016-11-09 23:24:43  2016-11   \n",
      "1377   13652355666_10153981680050667    clinton  2016-10-27 12:54:48  2016-10   \n",
      "1378   13652355666_10153690408580667      other  2016-07-23 13:10:00  2016-07   \n",
      "1379   13652355666_10153987891575667      other  2016-10-29 09:50:01  2016-10   \n",
      "1380   13652355666_10152975490600667      trump  2015-08-04 12:03:00  2015-08   \n",
      "1381   13652355666_10154027073720667    clinton  2016-11-09 03:20:00  2016-11   \n",
      "1382   13652355666_10153574967355667      trump  2016-06-05 13:29:00  2016-06   \n",
      "1383   13652355666_10153653526225667    clinton  2016-07-07 17:25:13  2016-07   \n",
      "1384   13652355666_10153920173020667      other  2016-10-09 21:33:45  2016-10   \n",
      "1385   13652355666_10153260556620667      trump  2016-01-10 07:33:00  2016-01   \n",
      "1386   13652355666_10153620528550667      other  2016-06-22 10:21:25  2016-06   \n",
      "1387   13652355666_10153584498190667      trump  2016-06-07 13:40:57  2016-06   \n",
      "1388   13652355666_10153981193040667      trump  2016-10-27 09:23:42  2016-10   \n",
      "1389   13652355666_10153613555685667      trump  2016-06-19 11:07:00  2016-06   \n",
      "1390   13652355666_10153922104885667      trump  2016-10-10 13:21:24  2016-10   \n",
      "1391   13652355666_10153831940125667      trump  2016-09-10 09:48:53  2016-09   \n",
      "1392   13652355666_10153109194405667      trump  2015-10-14 02:33:00  2015-10   \n",
      "1393   13652355666_10153466063975667    clinton  2016-04-10 15:15:00  2016-04   \n",
      "1394   13652355666_10153985983565667    clinton  2016-10-28 18:43:09  2016-10   \n",
      "1395   13652355666_10153781627400667      other  2016-08-24 22:40:00  2016-08   \n",
      "1396   13652355666_10153544883210667      trump  2016-05-19 23:11:00  2016-05   \n",
      "1397   13652355666_10153967236665667    clinton  2016-10-23 00:45:10  2016-10   \n",
      "1398   13652355666_10152991654495667      trump  2015-08-11 14:27:01  2015-08   \n",
      "1399    13652355666_1742117792779408      trump  2016-11-11 20:53:04  2016-11   \n",
      "1400    13652355666_1740277859630068    clinton  2016-11-09 12:57:37  2016-11   \n",
      "1401   13652355666_10153033465715667    clinton  2015-09-01 03:00:01  2015-09   \n",
      "\n",
      "     hour  \n",
      "0      00  \n",
      "1      14  \n",
      "2      00  \n",
      "3      02  \n",
      "4      21  \n",
      "5      07  \n",
      "6      14  \n",
      "7      22  \n",
      "8      16  \n",
      "9      11  \n",
      "10     19  \n",
      "11     22  \n",
      "12     07  \n",
      "13     22  \n",
      "14     19  \n",
      "15     21  \n",
      "16     16  \n",
      "17     16  \n",
      "18     11  \n",
      "19     00  \n",
      "20     08  \n",
      "21     16  \n",
      "22     19  \n",
      "23     01  \n",
      "24     11  \n",
      "25     23  \n",
      "26     07  \n",
      "27     11  \n",
      "28     08  \n",
      "29     20  \n",
      "...   ...  \n",
      "1372   16  \n",
      "1373   22  \n",
      "1374   12  \n",
      "1375   17  \n",
      "1376   23  \n",
      "1377   12  \n",
      "1378   13  \n",
      "1379   09  \n",
      "1380   12  \n",
      "1381   03  \n",
      "1382   13  \n",
      "1383   17  \n",
      "1384   21  \n",
      "1385   07  \n",
      "1386   10  \n",
      "1387   13  \n",
      "1388   09  \n",
      "1389   11  \n",
      "1390   13  \n",
      "1391   09  \n",
      "1392   02  \n",
      "1393   15  \n",
      "1394   18  \n",
      "1395   22  \n",
      "1396   23  \n",
      "1397   00  \n",
      "1398   14  \n",
      "1399   20  \n",
      "1400   12  \n",
      "1401   03  \n",
      "\n",
      "[1402 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Lemmanize the sentences (e.g. is, are, am ->> be)\n",
    "# Might be better to skip it\n",
    "\n",
    "new_index = range(0, len(sample.index))\n",
    "new_sample = pd.DataFrame(index=new_index, columns=(\"texts\", \"sources\", \"labels\"))\n",
    "i=0\n",
    "for text in sample['texts']:\n",
    "    blob = TextBlob(text)\n",
    "    newtexts = \"\"\n",
    "\n",
    "    for sentence in blob.sentences:\n",
    "        newtext = \"\"\n",
    "\n",
    "        for word in sentence.words:\n",
    "            newtext += \" \" + word.lemmatize('v') # 'v' for 'verb'\n",
    "\n",
    "        newtexts += newtext\n",
    "    new_sample['texts'].loc[i] = newtexts\n",
    "    i += 1\n",
    "    \n",
    "sample['texts'] = new_sample['texts']\n",
    "print sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  texts   sources  labels  \\\n",
      "0     It unusual president 's children clue White Ho...       cnn       1   \n",
      "1     Just hours Mitt Romney blast Donald J Trump sp...       cbs      -1   \n",
      "2     Hillary Clinton win state Washington CBS News ...       cbs       1   \n",
      "3     We take job away countries make product 're ta...       fox      -1   \n",
      "4     Jessica Leeds 74 Manhattan one two women quote...  usatoday      -1   \n",
      "5     Ash Carter warn Russia policy Syria consequenc...       cbs      -1   \n",
      "6     It 's Rubio Watch Donald J Trump use water bot...       fox       0   \n",
      "7     Online Trump supporters discuss monitor poll p...  usatoday      -1   \n",
      "8     This mogul say 's member lucky sperm club How ...       cnn       1   \n",
      "9     Live Hillary Clinton speak NAACP Convention Ci...       fox       0   \n",
      "10    BREAKING Hillary Clinton choose Virginia Senat...       cbs       0   \n",
      "11    The billboard translate read Donald Trump ca n...  usatoday      -1   \n",
      "12    How weird world politics 2015 We n't even need...       cnn      -1   \n",
      "13    NEW A Chicago police spokesperson tell CBS New...       cbs       0   \n",
      "14    Hillary Clinton bad things Donald J Trump tell...       cbs      -1   \n",
      "15    Tim Kaine defend work Clinton Foundation say C...       fox       1   \n",
      "16    Just Clinton Trump square would become one nas...  usatoday       0   \n",
      "17    Chelsea take Twitter announce second child mot...       cbs       1   \n",
      "18         One adviser describe Clinton obsess Jeb Bush  usatoday      -1   \n",
      "19    If look American human 're totally disgust 's ...       fox      -1   \n",
      "20    A new report show donors Clinton Foundation sp...       cbs      -1   \n",
      "21    Donald J Trump 's list include six federal app...       wsj       0   \n",
      "22    Sen Elizabeth Warren launch blister attack Don...       cnn      -1   \n",
      "23    Former KKK grand wizard David Duke n't win sen...  usatoday       0   \n",
      "24    We work heal divisions long campaign Paul Ryan...       cbs       1   \n",
      "25    For 40 years Donald J Trump part corruption Wa...       fox      -1   \n",
      "26    Michelle Obama President Obama Joe Biden campa...       cbs      -1   \n",
      "27    Paul Ryan I think Donald Trump pull enormous p...       cbs       1   \n",
      "28    The 10-Point Gerard Baker World Economic Forum...       wsj       0   \n",
      "29    The school secretary wear orange jumpsuit Depa...  usatoday       0   \n",
      "...                                                 ...       ...     ...   \n",
      "1372                       Donald Trump one wish weight  usatoday       0   \n",
      "1373  Hillary Clinton win South Dakota Democratic pr...  usatoday       1   \n",
      "1374  BREAKING The FBI review additional email Hilla...  usatoday      -1   \n",
      "1375  President Obama 's lower court appointments co...  usatoday       0   \n",
      "1376  When Barack Obama win Ohio 2008 2012 support w...  usatoday      -1   \n",
      "1377  Eight years hard-fought primary battle Michell...  usatoday       1   \n",
      "1378  Hillary Clinton might rescue Republicans inevi...  usatoday      -1   \n",
      "1379  Sen Bernie Sanders lay groundwork post-electio...  usatoday       1   \n",
      "1380  Trump may peak early Or may begin something un...  usatoday       1   \n",
      "1381  During primaries one common attack Bernie Sand...  usatoday      -1   \n",
      "1382                              Trumpy biggest seller  usatoday       0   \n",
      "1383  Bernie Sanders could give Hillary Clinton long...  usatoday       1   \n",
      "1384  Trump blast Clinton since make deplorables com...  usatoday      -1   \n",
      "1385  Police remove Rose Hamid wear hijab stand sile...  usatoday       0   \n",
      "1386  Hillary Clinton may corrupt person ever seek p...  usatoday      -1   \n",
      "1387               I one word Donald Trump Basta Enough  usatoday      -1   \n",
      "1388  Donald J Trump longer commit sue women accuse ...  usatoday       0   \n",
      "1389  Trump say would nice Republican Party get behi...  usatoday       1   \n",
      "1390  Mike Pence offer full-throated endorsement Don...  usatoday       1   \n",
      "1391             The remark n't sit well Donald J Trump  usatoday      -1   \n",
      "1392        Donald J Trump lot say Tuesday 's DemDebate  usatoday       0   \n",
      "1393  Do favor Clinton tell New York City Mayor Bill...  usatoday       0   \n",
      "1394  The American people deserve get full complete ...  usatoday       0   \n",
      "1395  Donald J Trump call Hillary Clinton candidate ...  usatoday      -1   \n",
      "1396  The bill go Gov Mary Fallin whose name float p...  usatoday       0   \n",
      "1397  With smile face help poll worker far-off fanta...  usatoday       1   \n",
      "1398  Donald J Trump continue lead Republican field ...  usatoday       0   \n",
      "1399  Donald Trump tell 'The Wall Street Journal may...  usatoday       1   \n",
      "1400  Nothing make prouder champion say Hillary Clin...  usatoday       1   \n",
      "1401  The latest release include around 150 email cl...  usatoday       0   \n",
      "\n",
      "                            statusID candidates                dates     date  \\\n",
      "0       5550296508_10155606385896509      trump  2016-11-15 00:00:02  2016-11   \n",
      "1     131459315949_10153427911075950      trump  2016-03-03 14:44:04  2016-03   \n",
      "2     131459315949_10154056641215950    clinton  2016-11-09 00:03:03  2016-11   \n",
      "3      15704546335_10153385284896336      trump  2015-06-23 02:00:01  2015-06   \n",
      "4      13652355666_10153933280450667      trump  2016-10-13 21:42:41  2016-10   \n",
      "5     131459315949_10153397324575950    clinton  2016-02-17 07:39:27  2016-02   \n",
      "6      15704546335_10154088259801336      trump  2016-02-26 14:00:14  2016-02   \n",
      "7      13652355666_10153959863300667      trump  2016-10-20 22:41:48  2016-10   \n",
      "8       5550296508_10154648040171509      trump  2016-04-01 16:28:02  2016-04   \n",
      "9      15704546335_10154454708126336    clinton  2016-07-18 11:42:36  2016-07   \n",
      "10    131459315949_10153757406760950    clinton  2016-07-22 19:17:14  2016-07   \n",
      "11     13652355666_10153948842760667      trump  2016-10-17 22:00:00  2016-10   \n",
      "12      5550296508_10154315359966509      trump  2015-12-22 07:22:01  2015-12   \n",
      "13    131459315949_10153447192105950      trump  2016-03-11 22:40:47  2016-03   \n",
      "14    131459315949_10154071618380950      other  2016-11-13 19:15:54  2016-11   \n",
      "15     15704546335_10154668092296336    clinton  2016-10-04 21:56:11  2016-10   \n",
      "16     13652355666_10153932123850667      other  2016-10-13 16:00:00  2016-10   \n",
      "17    131459315949_10153292327635950    clinton  2015-12-21 16:28:32  2015-12   \n",
      "18     13652355666_10153924828260667    clinton  2016-10-11 11:43:00  2016-10   \n",
      "19     15704546335_10154713987721336      trump  2016-10-20 00:06:02  2016-10   \n",
      "20    131459315949_10153837579360950    clinton  2016-08-24 08:30:05  2016-08   \n",
      "21      8304333127_10154314158863128      trump  2016-05-18 16:40:02  2016-05   \n",
      "22      5550296508_10154898645306509      trump  2016-06-09 19:31:23  2016-06   \n",
      "23     13652355666_10154026471955667      other  2016-11-09 01:40:00  2016-11   \n",
      "24    131459315949_10154058345805950      trump  2016-11-09 11:28:20  2016-11   \n",
      "25     15704546335_10154103175901336      trump  2016-03-03 23:55:33  2016-03   \n",
      "26    131459315949_10153976233385950    clinton  2016-10-14 07:24:03  2016-10   \n",
      "27    131459315949_10154058369590950      trump  2016-11-09 11:35:20  2016-11   \n",
      "28      8304333127_10153976433233128      trump  2016-01-20 08:30:00  2016-01   \n",
      "29     13652355666_10154007889465667    clinton  2016-11-03 20:13:57  2016-11   \n",
      "...                              ...        ...                  ...      ...   \n",
      "1372   13652355666_10153846997160667      trump  2016-09-15 16:30:00  2016-09   \n",
      "1373   13652355666_10153585700530667    clinton  2016-06-07 22:58:39  2016-06   \n",
      "1374   13652355666_10153985134535667    clinton  2016-10-28 12:23:55  2016-10   \n",
      "1375   13652355666_10153844454430667      other  2016-09-14 17:30:00  2016-09   \n",
      "1376   13652355666_10154030994500667      trump  2016-11-09 23:24:43  2016-11   \n",
      "1377   13652355666_10153981680050667    clinton  2016-10-27 12:54:48  2016-10   \n",
      "1378   13652355666_10153690408580667      other  2016-07-23 13:10:00  2016-07   \n",
      "1379   13652355666_10153987891575667      other  2016-10-29 09:50:01  2016-10   \n",
      "1380   13652355666_10152975490600667      trump  2015-08-04 12:03:00  2015-08   \n",
      "1381   13652355666_10154027073720667    clinton  2016-11-09 03:20:00  2016-11   \n",
      "1382   13652355666_10153574967355667      trump  2016-06-05 13:29:00  2016-06   \n",
      "1383   13652355666_10153653526225667    clinton  2016-07-07 17:25:13  2016-07   \n",
      "1384   13652355666_10153920173020667      other  2016-10-09 21:33:45  2016-10   \n",
      "1385   13652355666_10153260556620667      trump  2016-01-10 07:33:00  2016-01   \n",
      "1386   13652355666_10153620528550667      other  2016-06-22 10:21:25  2016-06   \n",
      "1387   13652355666_10153584498190667      trump  2016-06-07 13:40:57  2016-06   \n",
      "1388   13652355666_10153981193040667      trump  2016-10-27 09:23:42  2016-10   \n",
      "1389   13652355666_10153613555685667      trump  2016-06-19 11:07:00  2016-06   \n",
      "1390   13652355666_10153922104885667      trump  2016-10-10 13:21:24  2016-10   \n",
      "1391   13652355666_10153831940125667      trump  2016-09-10 09:48:53  2016-09   \n",
      "1392   13652355666_10153109194405667      trump  2015-10-14 02:33:00  2015-10   \n",
      "1393   13652355666_10153466063975667    clinton  2016-04-10 15:15:00  2016-04   \n",
      "1394   13652355666_10153985983565667    clinton  2016-10-28 18:43:09  2016-10   \n",
      "1395   13652355666_10153781627400667      other  2016-08-24 22:40:00  2016-08   \n",
      "1396   13652355666_10153544883210667      trump  2016-05-19 23:11:00  2016-05   \n",
      "1397   13652355666_10153967236665667    clinton  2016-10-23 00:45:10  2016-10   \n",
      "1398   13652355666_10152991654495667      trump  2015-08-11 14:27:01  2015-08   \n",
      "1399    13652355666_1742117792779408      trump  2016-11-11 20:53:04  2016-11   \n",
      "1400    13652355666_1740277859630068    clinton  2016-11-09 12:57:37  2016-11   \n",
      "1401   13652355666_10153033465715667    clinton  2015-09-01 03:00:01  2015-09   \n",
      "\n",
      "     hour  \n",
      "0      00  \n",
      "1      14  \n",
      "2      00  \n",
      "3      02  \n",
      "4      21  \n",
      "5      07  \n",
      "6      14  \n",
      "7      22  \n",
      "8      16  \n",
      "9      11  \n",
      "10     19  \n",
      "11     22  \n",
      "12     07  \n",
      "13     22  \n",
      "14     19  \n",
      "15     21  \n",
      "16     16  \n",
      "17     16  \n",
      "18     11  \n",
      "19     00  \n",
      "20     08  \n",
      "21     16  \n",
      "22     19  \n",
      "23     01  \n",
      "24     11  \n",
      "25     23  \n",
      "26     07  \n",
      "27     11  \n",
      "28     08  \n",
      "29     20  \n",
      "...   ...  \n",
      "1372   16  \n",
      "1373   22  \n",
      "1374   12  \n",
      "1375   17  \n",
      "1376   23  \n",
      "1377   12  \n",
      "1378   13  \n",
      "1379   09  \n",
      "1380   12  \n",
      "1381   03  \n",
      "1382   13  \n",
      "1383   17  \n",
      "1384   21  \n",
      "1385   07  \n",
      "1386   10  \n",
      "1387   13  \n",
      "1388   09  \n",
      "1389   11  \n",
      "1390   13  \n",
      "1391   09  \n",
      "1392   02  \n",
      "1393   15  \n",
      "1394   18  \n",
      "1395   22  \n",
      "1396   23  \n",
      "1397   00  \n",
      "1398   14  \n",
      "1399   20  \n",
      "1400   12  \n",
      "1401   03  \n",
      "\n",
      "[1402 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Getting rid of too common words (e.g. the, an)\n",
    "\n",
    "#print stopwords.words(\"english\") \n",
    "\n",
    "new_index = range(0, len(sample.index))\n",
    "new_sample = pd.DataFrame(index=new_index, columns=(\"texts\", \"sources\", \"labels\"))\n",
    "\n",
    "i=0\n",
    "for index, row in sample.iterrows():\n",
    "    # If this new post is different post, then append what we worked so far\n",
    "    words = [w for w in row['texts'].split() if not w in stopwords.words(\"english\")]\n",
    "    new_sample['texts'].loc[i] = ' '.join(words)\n",
    "    i += 1\n",
    "\n",
    "sample['texts'] = new_sample['texts']\n",
    "print sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 95 0 0 300\t 0.2013\n",
      "5 95 0 4 300\t 0.2416\n",
      "5 95 0 5 12300\t 0.2685\n",
      "5 95 1 24 12300\n",
      "5 93 0 11 9300\t 0.2752\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-05030010c3d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m                         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_combined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_combined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[0;32m-> 1028\u001b[0;31m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1029\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1081\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[1;32m   1082\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m                                      X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m             residual = loss.negative_gradient(y, y_pred, k=k,\n\u001b[0;32m--> 763\u001b[0;31m                                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0;31m# induce regression tree on residuals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mnegative_gradient\u001b[0;34m(self, y, pred, k, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;34m\"\"\"Compute negative gradient for the ``k``-th class. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         return y - np.nan_to_num(np.exp(pred[:, k] -\n\u001b[0;32m--> 564\u001b[0;31m                                         logsumexp(pred, axis=1)))\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/utils/extmath.pyc\u001b[0m in \u001b[0;36mlogsumexp\u001b[0;34m(arr, axis)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mvmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for j in range(0,2): # stop_words: k=0 'engl' k=1 none\n",
    "\n",
    "cutoff = int(math.ceil(len(sample.index)*0.9)) # 90%\n",
    "train = pd.DataFrame({'labels':sample['labels'][:cutoff], \n",
    "                      'texts':sample['texts'][:cutoff], \n",
    "                      'sources':sample['sources'][:cutoff], \n",
    "                      'candidates':sample['candidates'][:cutoff], \n",
    "                      'date':sample['date'][:cutoff], \n",
    "                      'hour':sample['hour'][:cutoff]})\n",
    "test  = pd.DataFrame({'labels':sample['labels'][cutoff:], \n",
    "                      'texts':sample['texts'][cutoff:], \n",
    "                      'sources':sample['sources'][cutoff:], \n",
    "                      'candidates':sample['candidates'][cutoff:], \n",
    "                      'date':sample['date'][cutoff:], \n",
    "                      'hour':sample['hour'][cutoff:]})\n",
    "\n",
    "best_score = 0\n",
    "\n",
    "for h in range(5,18,2): # min_df from 0.05 to 0.30, adding 0.02 every time\n",
    "    for i in range(95,72,-2): # max_df from 0.95 to 0.70, -0.02 every time\n",
    "        for k in range(0,2): # k=0 CountVectorizer (count), k=1 TfidfVectorizer (weighed)\n",
    "            for l in range(0,25): \n",
    "                for m in range(300,13001,1000): # max_features (how many most frequent words should we consider)\n",
    "\n",
    "                    if k==0:\n",
    "                        tf_vectorizer = CountVectorizer(max_df=i/100.0, \n",
    "                                                        min_df=h/100.0, \n",
    "                                                        max_features = m)\n",
    "                    elif k==1:\n",
    "                        tf_vectorizer = TfidfVectorizer(max_df=i/100.0, \n",
    "                                                        min_df=h/100.0, \n",
    "                                                        max_features = m)\n",
    "\n",
    "                    train_text_tf_  = tf_vectorizer.fit_transform(train['texts'].values)\n",
    "                    test_text_tf_  = tf_vectorizer.transform(test['texts'].values)\n",
    "\n",
    "                    train_source_tf_ = tf_vectorizer.fit_transform(train['sources'].values)\n",
    "                    test_source_tf_ = tf_vectorizer.transform(test['sources'].values)\n",
    "\n",
    "                    train_candidate = tf_vectorizer.fit_transform(train['candidates'].values)\n",
    "                    test_candidate = tf_vectorizer.transform(test['candidates'].values)\n",
    "\n",
    "                    train_date = tf_vectorizer.fit_transform(train['date'].values)\n",
    "                    test_date = tf_vectorizer.transform(test['date'].values)\n",
    "                    \n",
    "                    train_hour = tf_vectorizer.fit_transform(train['hour'].values)\n",
    "                    test_hour = tf_vectorizer.transform(test['hour'].values)\n",
    "                    ####\n",
    "                    train_combined = sp.hstack([train_text_tf_, train_source_tf_], format='csr')\n",
    "                    test_combined = sp.hstack([test_text_tf_, test_source_tf_], format='csr')\n",
    "\n",
    "                    train_combined = sp.hstack([train_combined, train_candidate], format='csr')\n",
    "                    test_combined = sp.hstack([test_combined, test_candidate], format='csr')\n",
    "\n",
    "                    train_combined = sp.hstack([train_combined, train_date], format='csr')\n",
    "                    test_combined = sp.hstack([test_combined, test_date], format='csr')\n",
    "\n",
    "                    train_combined = sp.hstack([train_combined, train_hour], format='csr')\n",
    "                    test_combined = sp.hstack([test_combined, test_hour], format='csr')\n",
    "\n",
    "                    # CHANGE ARGS TOO\n",
    "                    if l==0:\n",
    "                        clf = MultinomialNB()\n",
    "                    elif l==1:\n",
    "                        clf = GaussianNB()\n",
    "                    elif l==2:\n",
    "                        clf = BernoulliNB()\n",
    "                    elif l==3:\n",
    "                        clf = LogisticRegression(random_state=1)\n",
    "                    elif l==4:\n",
    "                        clf = LogisticRegressionCV()\n",
    "                    elif l==5:\n",
    "                        clf = SGDClassifier()\n",
    "                    elif l==6:\n",
    "                        clf = Ridge()\n",
    "                    elif l==7:\n",
    "                        clf = RidgeClassifier()\n",
    "                    elif l==8:\n",
    "                        clf = RidgeClassifierCV()\n",
    "                    elif l==9:\n",
    "                        clf = ElasticNet()\n",
    "                    elif l==10:\n",
    "                        clf = LassoLars()\n",
    "                    elif l==11:\n",
    "                        clf = PassiveAggressiveClassifier()\n",
    "                    elif l==12:\n",
    "                        clf = SVC(random_state=1)\n",
    "                    elif l==13:\n",
    "                        clf = LinearSVC()\n",
    "                    elif l==14:\n",
    "                        clf = KNeighborsClassifier(n_neighbors=9)\n",
    "                    elif l==15:\n",
    "                        clf = NearestCentroid()\n",
    "                    elif l==16:\n",
    "                        clf = GradientBoostingClassifier()\n",
    "                    elif l==17:\n",
    "                        clf = BaggingClassifier(random_state=1)\n",
    "                    elif l==18:\n",
    "                        clf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "                    elif l==19:\n",
    "                        clf = AdaBoostClassifier()\n",
    "                    elif l==20:\n",
    "                        clf = ExtraTreesClassifier()\n",
    "                    elif l==21:\n",
    "                        clf = GradientBoostingClassifier()\n",
    "                    elif l==22:\n",
    "                        clf = VotingClassifier(estimators=\n",
    "                                [('l01', MultinomialNB()),\n",
    "                                ('l02', GaussianNB()),\n",
    "                                ('l03', BernoulliNB()),\n",
    "                                ('l04', LogisticRegression(random_state=1)),\n",
    "                                ('l13', SVC(random_state=1)),\n",
    "                                ('l15', KNeighborsClassifier(n_neighbors=9)),\n",
    "                                ('l18', BaggingClassifier(random_state=1)),\n",
    "                                ('l19', RandomForestClassifier(n_estimators=100, random_state=1)),\n",
    "                                ('l24', DecisionTreeClassifier(random_state=1))])\n",
    "                    elif l==23:\n",
    "                        clf = ExtraTreeClassifier()\n",
    "                    elif l==24:\n",
    "                        clf = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "                    clf.fit(train_combined.toarray(), train['labels'])\n",
    "                    score = clf.score(test_combined.toarray(), test['labels'])\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        print \"%d %d %d %d %d\\t %.4f\" % (h,i,k,l,m,score)\n",
    "        print \"%d %d %d %d %d\" % (h,i,k,l,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 20,000+ possibilities and running 8000+ cases, <br>\n",
    "It never hit 0.7.. ->> For train:test = 1:1 size\n",
    "\n",
    "Train:test = 10:1 size. I do get up to 0.8+ <br>\n",
    "Now we also have up to (more reasonable) 14400 cases (all expected to run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning w/ TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  18%|█▊        | 21/120 [00:15<01:01,  1.60pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.567128522453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  35%|███▌      | 42/120 [00:21<00:11,  6.74pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.567128522453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  52%|█████▏    | 62/120 [01:07<03:11,  3.30s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.567128522453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  68%|██████▊   | 82/120 [01:49<00:22,  1.72pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.571469654091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  84%|████████▍ | 101/120 [01:53<00:02,  8.83pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 0.571469654091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: BernoulliNB(Normalizer(input_matrix, 26), 0.040000000000000001, 0.33000000000000002)\n",
      "0.549321012559\n",
      "1402 5 80 0 50 5 20 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  18%|█▊        | 21/120 [01:02<00:59,  1.68pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.567894748329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  34%|███▍      | 41/120 [01:15<01:16,  1.04pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.567894748329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  49%|████▉     | 59/120 [01:30<00:48,  1.25pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.567894748329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  68%|██████▊   | 81/120 [01:40<00:06,  6.04pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.567894748329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  84%|████████▍ | 101/120 [01:50<00:08,  2.28pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 0.567894748329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: DecisionTreeClassifier(input_matrix)\n",
      "0.505894778536\n",
      "1402 5 80 0 50 5 20 11\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GP closed prematurely - will use current best pipeline\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A pipeline has not yet been optimized. Please call fit() first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-0a2a4b563941>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m                                                   verbosity=2)\n\u001b[1;32m     44\u001b[0m                             \u001b[0mtpot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"%d %d %d %d %d %d %d %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/tpot/base.pyc\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, testing_features, testing_classes)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \"\"\"\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fitted_pipeline\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             raise ValueError('A pipeline has not yet been optimized. '\n\u001b[0m\u001b[1;32m    442\u001b[0m                              'Please call fit() first.')\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: A pipeline has not yet been optimized. Please call fit() first."
     ]
    }
   ],
   "source": [
    "#for i in range(0,2): # 'Negative' from sample = -1 or 0\n",
    "#  if i==0:\n",
    "#      sample['Answer.sentiment'] = sample['Answer.sentiment'].map({'Positive':1, 'Negative':-1})\n",
    "#  elif i==1:\n",
    "#      sample['Answer.sentiment'] = sample['Answer.sentiment'].map({'Positive':1, 'Negative':0})\n",
    "\n",
    "for j in range(5,21,2): # min_df\n",
    "    for k in range(80,101,2): # max_df\n",
    "        for l in range(0,2): # l=0 CountVectorizer (count), l=1 TfidfVectorizer (weighed)\n",
    "            for m in range(50,101,10): # train:test = m/100 : (1-m/100) so 50:50 to 90:10\n",
    "                for n in range(5,7): # generation (# of TPOT iteration)\n",
    "                    for p in range(20,26,5): # pop_size p \n",
    "                        for q in range(10,12): # k-fold number\n",
    "                            print \"\"    \n",
    "\n",
    "                            #sample = pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/sample_data_results.csv') \n",
    "\n",
    "                            # label ('positive','Negative') ->> (#,#) (e.g. (1,-1) or (1,0))\n",
    "                            #sample['Answer.sentiment'] = sample['Answer.sentiment'].map({'Positive':1, 'Neutral':0, 'Negative':-1})\n",
    "\n",
    "                            if l==0:\n",
    "                                tf_vectorizer = CountVectorizer(min_df=j/100.0, max_df=k/100.0, max_features=5000)\n",
    "                            elif l==1:\n",
    "                                tf_vectorizer = TfidfVectorizer(min_df=j/100.0, max_df=k/100.0, max_features=5000)\n",
    "\n",
    "                            #sample_input_tf  = tf_vectorizer.fit_transform(sample['Input.content'].values)\n",
    "                            text_tf_  = tf_vectorizer.fit_transform(sample['texts'].values)\n",
    "                            source_tf_ = tf_vectorizer.fit_transform(sample['sources'].values)\n",
    "                            candidates_tf_ = tf_vectorizer.fit_transform(sample['candidates'].values)\n",
    "                            date_tf_ = tf_vectorizer.fit_transform(sample['date'].values)\n",
    "                            hour_tf_ = tf_vectorizer.fit_transform(sample['hour'].values)\n",
    "\n",
    "                            combined = sp.hstack([text_tf_, source_tf_], format='csr')\n",
    "                            combined = sp.hstack([combined, candidates_tf_], format='csr')\n",
    "                            combined = sp.hstack([combined, date_tf_], format='csr')\n",
    "                            combined = sp.hstack([combined, hour_tf_], format='csr')\n",
    "\n",
    "                            X_train, X_test, y_train, y_test = train_test_split(combined, sample['labels'].values,\n",
    "                                                                train_size=m/100.0, test_size=(1-m/100.0))#, random_state=)\n",
    "\n",
    "                            # Official website example: gen=5, pop_size=20, verbo=2\n",
    "                            tpot = TPOTClassifier(generations=n, population_size=p, num_cv_folds=q,\n",
    "                                                  verbosity=2)\n",
    "                            tpot.fit(X_train, y_train)\n",
    "                            print(tpot.score(X_test, y_test))\n",
    "                            print \"%d %d %d %d %d %d %d %d\" % (i,j,k,l,m,n,p,q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer, Pos=1 Neg=-1, train:test=75:25, size=872 in TPOT = 0.78 <br>\n",
    "CountVectorizer, Pos=1 Neg=-1, train:test=92:08, size=872 in TPOT = 0.78 <br>\n",
    "CountVectorizer, Pos=1 Neg =0, train:test=92:08, size=872 in TPOT = 0.86 <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "matter lives series black super city moon mckinnon photo parks landing photoshopped tweeted kate kill coalition spray concluded blasts camera\n",
      "Topic #1:\n",
      "carson ben suggesting damage rise dr faced backlash caused later khan health trump care hero views polls insults young rhetoric\n",
      "Topic #2:\n",
      "clinton hillary trump donald today eye opener matters said campaign world 90 seconds benghazi face emails presidential debate gave lawmakers\n",
      "Topic #3:\n",
      "trump donald cbsn clinton ws http hillary said president news cbs new says watch republican just gop campaign obama election\n",
      "Topic #4:\n",
      "percent cruz ted quickly twitter compared pope internet election wife users support francis black voting actor ceo economic 42 bowl\n",
      "Topic #5:\n",
      "sanders bernie demdebate mitt romney caucuses malley heated president conway kellyanne cause iowa debate used called michigan asked wage strong\n",
      "Topic #6:\n",
      "ready rights human play tear tactics apart false chief comeback columbia civil communities way harlem loser fully passionate enthusiastic attempts\n",
      "Topic #7:\n",
      "mission muslims election officials mcdonald trump national entering ban shut thought ivanka coming said benghazi police man presidential season donald\n",
      "Topic #8:\n",
      "team sarah shooting palin transition mark insists warning carrying streets trump cuban boycott mass rahm laquanmcdonald christmas stop asking worse\n",
      "Topic #9:\n",
      "tax returns assault warned audit sexual planned parenthood releasing note tack following involve irs rare cycle led turned flooded hundreds\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df1 = pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/Facebook_RAW/CBSNews_facebook_statuses.csv') \n",
    "df2 = pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/Facebook_RAW/cnn_facebook_statuses.csv') \n",
    "df3 = pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/Facebook_RAW/facebook_sample_data.txt') \n",
    "df4 = pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/Facebook_RAW/FoxNews_facebook_statuses.csv') \n",
    "df5 = pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/Facebook_RAW/msnbc_facebook_statuses.csv') \n",
    "df6 = pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/Facebook_RAW/nytimes_facebook_statuses.csv') \n",
    "df7 = pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/Facebook_RAW/usatoday_facebook_statuses.csv') \n",
    "df8 = pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/Facebook_RAW/wsj_facebook_statuses.csv') \n",
    "\n",
    "df1.append(df2)\n",
    "df3.append(df4)\n",
    "df5.append(df6)\n",
    "df7.append(df8)\n",
    "\n",
    "df1.append(df3)\n",
    "df5.append(df7)\n",
    "\n",
    "df1.append(df5)\n",
    "\n",
    "sample = df1\n",
    "\n",
    "#print sample.head()\n",
    "\n",
    "cutoff = int(math.ceil(len(sample.index)*0.92)) # 92%\n",
    "train = pd.DataFrame({'texts':sample['status_message'][:cutoff]})\n",
    "test  = pd.DataFrame({'texts':sample['status_message'][cutoff:]})\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, analyzer = \"word\",\n",
    "                                   #max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(train['texts'].values)\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, analyzer = \"word\",\n",
    "                                #max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(train['texts'].values)\n",
    "\n",
    "lda = LatentDirichletAllocation(#n_topics=n_topics, \n",
    "                                max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "lda.fit(tf)\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble & Bagging (Bootstrap AGgregating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nope... http://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So text extraction + ..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task 1: Load the texts\n",
    "import pandas as pd\n",
    "import glob, os         # for reading all .txt files\n",
    "import csv\n",
    "import numpy as np\n",
    "from textblob import TextBlob # use kernel Python[Root]\n",
    "\n",
    "cutoff = 436\n",
    "\n",
    "# Read sample texts\n",
    "sample = pd.read_csv('/Users/Haru/Documents/! College/4. Fall 2016/489Project/sample_data_results.csv') \n",
    "\n",
    "# Lemmatize\n",
    "new_sample = pd.DataFrame(columns=(\"Input.content\", \"Answer.sentiment\"))\n",
    "i=0\n",
    "for text in sample['Input.content']:\n",
    "    blob = TextBlob(text)\n",
    "    newtexts = \"\"\n",
    "\n",
    "    for sentence in blob.sentences:\n",
    "        newtext = \"\"\n",
    "#        print sentence.dict\n",
    "\n",
    "        for word in sentence.words:\n",
    "            newtext += \" \" + word.lemmatize('v') # 'v' for 'verb'\n",
    "\n",
    "        newtexts += newtext\n",
    "        new_sample['texts'].loc[i] = newtexts\n",
    "        i += 1\n",
    "\n",
    "i=0\n",
    "for answer in sample['Answer.sentiment']: # updating answers\n",
    "    new_sample['Answer.sentiment'].loc[i] = answer\n",
    "    i += 1\n",
    "    \n",
    "sample = new_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.DataFrame({'label':sample['Answer.sentiment'][:cutoff], 'texts':sample['Input.content'][:cutoff]})\n",
    "test  = pd.DataFrame({'label':sample['Answer.sentiment'][cutoff:], 'texts':sample['Input.content'][cutoff:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 6 0 0 1 - 0.7778\n",
      "1 6 0 0 2 - 0.6389\n",
      "1 6 0 1 1 - 0.7778\n",
      "1 6 0 1 2 - 0.6389\n",
      "1 6 1 0 1 - 0.7778\n",
      "1 6 1 0 2 - 0.6389\n",
      "1 6 1 1 1 - 0.7222\n",
      "1 6 1 1 2 - 0.6944\n",
      "1 7 0 0 1 - 0.8056\n",
      "1 7 0 0 2 - 0.6389\n",
      "1 7 0 1 1 - 0.8056\n",
      "1 7 0 1 2 - 0.6389\n",
      "1 7 1 0 1 - 0.8056\n",
      "1 7 1 0 2 - 0.6389\n",
      "1 7 1 1 1 - 0.7222\n",
      "1 7 1 1 2 - 0.6389\n",
      "1 8 0 0 1 - 0.8056\n",
      "1 8 0 0 2 - 0.6389\n",
      "1 8 0 1 1 - 0.8056\n",
      "1 8 0 1 2 - 0.6389\n",
      "1 8 1 0 1 - 0.8056\n",
      "1 8 1 0 2 - 0.6389\n",
      "1 8 1 1 1 - 0.7222\n",
      "1 8 1 1 2 - 0.6389\n",
      "1 9 0 0 1 - 0.8056\n",
      "1 9 0 0 2 - 0.5833\n",
      "1 9 0 1 1 - 0.8056\n",
      "1 9 0 1 2 - 0.5833\n",
      "1 9 1 0 1 - 0.8056\n",
      "1 9 1 0 2 - 0.5833\n",
      "1 9 1 1 1 - 0.7222\n",
      "1 9 1 1 2 - 0.6389\n",
      "1 10 0 0 1 - 0.8056\n",
      "1 10 0 0 2 - 0.6389\n",
      "1 10 0 1 1 - 0.8056\n",
      "1 10 0 1 2 - 0.6389\n",
      "1 10 1 0 1 - 0.8056\n",
      "1 10 1 0 2 - 0.6389\n",
      "1 10 1 1 1 - 0.7222\n",
      "1 10 1 1 2 - 0.6944\n",
      "1 11 0 0 1 - 0.8056\n",
      "1 11 0 0 2 - 0.6389\n",
      "1 11 0 1 1 - 0.8056\n",
      "1 11 0 1 2 - 0.6389\n",
      "1 11 1 0 1 - 0.8056\n",
      "1 11 1 0 2 - 0.6389\n",
      "1 11 1 1 1 - 0.7222\n",
      "1 11 1 1 2 - 0.6944\n",
      "1 12 0 0 1 - 0.8056\n",
      "1 12 0 0 2 - 0.6389\n",
      "1 12 0 1 1 - 0.8056\n",
      "1 12 0 1 2 - 0.6389\n",
      "1 12 1 0 1 - 0.8056\n",
      "1 12 1 0 2 - 0.6389\n",
      "1 12 1 1 1 - 0.7222\n",
      "1 12 1 1 2 - 0.6944\n",
      "1 13 0 0 1 - 0.8056\n",
      "1 13 0 0 2 - 0.6389\n",
      "1 13 0 1 1 - 0.8056\n",
      "1 13 0 1 2 - 0.6389\n",
      "1 13 1 0 1 - 0.8056\n",
      "1 13 1 0 2 - 0.6389\n",
      "1 13 1 1 1 - 0.7222\n",
      "1 13 1 1 2 - 0.6944\n",
      "1 14 0 0 1 - 0.8056\n",
      "1 14 0 0 2 - 0.6389\n",
      "1 14 0 1 1 - 0.8056\n",
      "1 14 0 1 2 - 0.6389\n",
      "1 14 1 0 1 - 0.8056\n",
      "1 14 1 0 2 - 0.6389\n",
      "1 14 1 1 1 - 0.7222\n",
      "1 14 1 1 2 - 0.6944\n",
      "1 15 0 0 1 - 0.8056\n",
      "1 15 0 0 2 - 0.5833\n",
      "1 15 0 1 1 - 0.8056\n",
      "1 15 0 1 2 - 0.5833\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e18b73c0799f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                         \u001b[0mtf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                     \u001b[0mtrain_tf_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'texts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                     \u001b[0mtest_tf_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'texts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 839\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 241\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Haru/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mtoken_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtoken_pattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Task 4: feature engineering\n",
    "for h in range(1,30): # min_df\n",
    "    for i in range(h+5,100): # max_df\n",
    "        for j in range(0,2): # stop_words: k=0 'engl' k=1 none\n",
    "            for k in range(0,2): # k=0 CountVectorizer (count), k=1 TfidfVectorizer (weighed)\n",
    "                for l in range(0,3): # l=0 MultinomialNB, l=1 GaussianNB, l=2 BernoulliNB              \n",
    "\n",
    "                    if j==0 & k==0:\n",
    "                        tf_vectorizer = CountVectorizer(max_df=i/100.0, min_df=h/100.0, stop_words='english', analyzer = \"word\")\n",
    "                    elif j==1 & k==0:\n",
    "                        tf_vectorizer = CountVectorizer(max_df=i/100.0, min_df=h/100.0, analyzer = \"word\")\n",
    "                    elif j==0 & k==1:\n",
    "                        tf_vectorizer = TfidfVectorizer(max_df=i/100.0, min_df=h/100.0, stop_words='english', analyzer = \"word\")\n",
    "                    elif j==1 & k==1:\n",
    "                        tf_vectorizer = TfidfVectorizer(max_df=i/100.0, min_df=h/100.0, analyzer = \"word\")\n",
    "\n",
    "                    train_tf_ = tf_vectorizer.fit_transform(train['texts'].values)\n",
    "                    test_tf_  = tf_vectorizer.transform(test['texts'].values)\n",
    "\n",
    "                    if l==0:\n",
    "                        clf = MultinomialNB()\n",
    "                    elif l==1:\n",
    "                        clf = GaussianNB()\n",
    "                    elif l==2:\n",
    "                        clf = BernoulliNB()\n",
    "\n",
    "                    if l==0 | l==2:\n",
    "                        clf.fit(train_tf_, train['label'])\n",
    "                        print \"%d %d %d %d %d - %.4f\" % (h,i,j,k,l,clf.score(test_tf_, test['label']))\n",
    "                    elif l==1:\n",
    "                        clf.fit(train_tf_.toarray(), train['label'])\n",
    "                        print \"%d %d %d %d %d - %.4f\" % (h,i,j,k,l,clf.score(test_tf_.toarray(), test['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
